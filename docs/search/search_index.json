{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Monstache \u00b6 Sync MongoDB to Elasticsearch in realtime Monstache is a sync daemon written in Go that continously indexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use Elasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime Kibana visualizations and dashboards. Latest news For a showcase of what monstache can do with open data see the monstache-showcase If you find monstache useful check out some other projects related to MongoDB Sync MongoDB to InfluxDB mongofluxd Sync MongoDB to RediSearch redisetgo Sync MongoDB to Kafka route81 Features \u00b6 Supports up to and including the latest versions of Elasticsearch and MongoDB Single binary with a light footprint Support for MongoDB change streams and aggregation pipelines Pre built Docker containers Optionally filter the set of collections to sync Advanced support for sharded MongoDB clusters including auto-detection of new shards Direct read mode to do a full sync of collections in addition to tailing the oplog Transform and filter documents before indexing using Golang plugins or JavaScript Index the content of GridFS files Support for propogating hard/soft document deletes Support for propogating database and collection drops as index deletes Optional custom document routing in Elasticsearch Stateful resume feature Time machine feature to track document changes over time Worker and Clustering modes for High Availability Support for rfc7396 JSON merge patches Systemd support Optional http server to get access to liveness, stats, profiling, etc Next Steps \u00b6 See Getting Started for instructions how to get it up and running. See Release Notes for updates.","title":"Home"},{"location":"#monstache","text":"Sync MongoDB to Elasticsearch in realtime Monstache is a sync daemon written in Go that continously indexes your MongoDB collections into Elasticsearch. Monstache gives you the ability to use Elasticsearch to do complex searches and aggregations of your MongoDB data and easily build realtime Kibana visualizations and dashboards. Latest news For a showcase of what monstache can do with open data see the monstache-showcase If you find monstache useful check out some other projects related to MongoDB Sync MongoDB to InfluxDB mongofluxd Sync MongoDB to RediSearch redisetgo Sync MongoDB to Kafka route81","title":"Monstache"},{"location":"#features","text":"Supports up to and including the latest versions of Elasticsearch and MongoDB Single binary with a light footprint Support for MongoDB change streams and aggregation pipelines Pre built Docker containers Optionally filter the set of collections to sync Advanced support for sharded MongoDB clusters including auto-detection of new shards Direct read mode to do a full sync of collections in addition to tailing the oplog Transform and filter documents before indexing using Golang plugins or JavaScript Index the content of GridFS files Support for propogating hard/soft document deletes Support for propogating database and collection drops as index deletes Optional custom document routing in Elasticsearch Stateful resume feature Time machine feature to track document changes over time Worker and Clustering modes for High Availability Support for rfc7396 JSON merge patches Systemd support Optional http server to get access to liveness, stats, profiling, etc","title":"Features"},{"location":"#next-steps","text":"See Getting Started for instructions how to get it up and running. See Release Notes for updates.","title":"Next Steps"},{"location":"about/","text":"About \u00b6 License \u00b6 The MIT License (MIT) Copyright (c) 2016 Ryan Wynn Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Contributing \u00b6 The Monstache project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are: Code patches via pull requests Documentation improvements Bug reports and patch reviews Reporting an Issue \u00b6 Please include as much detail as you can. Let us know your platform, Monstache version, MongoDB version, and Elasticsearch version. Testing the Development Version \u00b6 If you want to just install and try out the latest development version of Monstache you can do so with the following commands. This can be useful if you want to provide feedback for a new feature or want to confirm if a bug you have encountered is fixed in the git master. You will need at least golang 1.11 which includes native go modules support. cd ~/build # somewhere outside your $GOPATH git clone https://github.com/rwynn/monstache.git cd monstache git checkout <branch-to-build> go install # the resulting monstache binary will be in $GOPATH/bin Running the tests \u00b6 To run the tests without Docker, you will need to have local mongod and elasticsearch servers running. Then you will need to start a monstache process in one terminal or in the background. monstache -verbose Finally in another terminal you can run the tests by issuing the following commands cd monstache go test -v Warning Running the Monstache tests will perform modifications to the test.test namespace in MongoDB and will index documents in the test.test index in Elasticsearch. If you have data that you need to keep on your local servers, make a back up before running the tests. Note If you don't want to setup MongoDB and Elasticsearch on your machine another option for running the tests is via Docker. After cloning the monstache repo you can cd into the docker/test directory and run run-tests.sh . You will need recent versions of docker and docker-compose to run the tests this way. Services for MongoDB and Elasticsearch will be started and the tests run on any changes you have made to the source code. Submitting Pull Requests \u00b6 Once you are happy with your changes or you are ready for some feedback, push it to your fork and send a pull request. For a change to be accepted it will most likely need to have tests and documentation if it is a new feature. Release Notes \u00b6 See github.com for the most recent release notes. monstache v4.16.1 \u00b6 Fix bug in mongoX509Settings validate (issue #198) Fix issue stopping monstache when using legacy oplog tailing (non change-stream) of a sharded cluster Upgrade golang on release builds to 1.12.1 Upgrade docker images to use Alpine 3.9.2 monstache v3.23.1 \u00b6 Fix bug in mongoX509Settings validate (issue #198) Fix issue stopping monstache when using legacy oplog tailing (non change-stream) of a sharded cluster Upgrade golang on release builds to 1.12.1 Upgrade docker images to use Alpine 3.9.2 monstache v4.16.0 (4tsb) \u00b6 Add a new integer setting direct-read-concur which, when set, limits the number of concurrent direct reads that will be performed. E.g. if you have direct-read-namespaces set to 23 namespaces and direct-read-concur set to 2, then monstache will read and sync namespace 1 and 2 concurrently and wait for both to finish before starting 3 and 4 - and so on. Add the ability to disable direct read collection splitting by setting direct-read-split-max to -1. By default, monstache will split each direct read collection up to 9 times and read each segment in a separate go routine. If you don't want to split collections at all then set direct-read-split-max to -1. monstache v3.23.0 (4tsb) \u00b6 Add a new integer setting direct-read-concur which, when set, limits the number of concurrent direct reads that will be performed. E.g. if you have direct-read-namespaces set to 23 namespaces and direct-read-concur set to 2, then monstache will read and sync namespace 1 and 2 concurrently and wait for both to finish before starting 3 and 4 - and so on. Add the ability to disable direct read collection splitting by setting direct-read-split-max to -1. By default, monstache will split each direct read collection up to 9 times and read each segment in a separate go routine. If you don't want to split collections at all then set direct-read-split-max to -1. monstache v4.15.2 \u00b6 Fix issue using monstache with GoCenter monstache v3.22.2 \u00b6 Fix issue using monstache with GoCenter monstache v4.15.1 \u00b6 Upgrade golang to 1.12 Fix a panic under some conditions when processing the result of a golang plugin Map function monstache v3.22.1 \u00b6 Upgrade golang to 1.12 Fix a panic under some conditions when processing the result of a golang plugin Map function monstache v4.15.0 \u00b6 This release adds the ability for MongoDB 4+ users to open change streams against entire databases or entire deployments. See the documentation for the change-stream-namespaces option for details. The resume , resume-from-timestamp , replay , and cluster-name options can now be used in conjunction with change-stream-namespaces if you have MongoDB 4 or greater. Improved support for connecting directly to shards when authorization is required on the connection. This is only applicable if you are not using change streams and you are connecting to a sharded MongoDB deployment. In that case Monstache needs to discover and connect directly to shards. In this case it will reuse the login info from the initial connection to the mongos server when connecting to the shards. monstache v3.22.0 \u00b6 This release adds the ability for MongoDB 4+ users to open change streams against entire databases or entire deployments. See the documentation for the change-stream-namespaces option for details. The resume , resume-from-timestamp , replay , and cluster-name options can now be used in conjunction with change-stream-namespaces if you have MongoDB 4 or greater. Improved support for connecting directly to shards when authorization is required on the connection. This is only applicable if you are not using change streams and you are connecting to a sharded MongoDB deployment. In that case Monstache needs to discover and connect directly to shards. In this case it will reuse the login info from the initial connection to the mongos server when connecting to the shards. monstache v4.14.2 \u00b6 Change stream performance improvements monstache v3.21.2 \u00b6 Change stream performance improvements monstache v4.14.1 \u00b6 Fix for regression in previous release where the MongoDB initial connection timeout was not being honored Added a new setting relate-buffer which is the maximum number of relate events to allow in queue before skipping the relate and printing an error. This was added to prevent the scenario where a large number of relate events stall the pipeline. The default number of relates to allow in queue is 1000. monstache v3.21.1 \u00b6 Fix for regression in previous release where the MongoDB initial connection timeout was not being honored Added a new setting relate-buffer which is the maximum number of relate events to allow in queue before skipping the relate and printing an error. This was added to prevent the scenario where a large number of relate events stall the pipeline. The default number of relates to allow in queue is 1000. monstache v4.14.0 \u00b6 Performance and reliability improvements Fix for issue #168 by adding new TOML only configs elasticsearch-healthcheck-timeout-startup and elasticsearch-healthcheck-timeout. These are in seconds. Fix a panic occurring when an empty array was returned from a pipeline javascript function. Default read/write timeouts changed to 30s, up from 7s. monstache v3.21.0 \u00b6 Performance and reliability improvements Fix for issue #168 by adding new TOML only configs elasticsearch-healthcheck-timeout-startup and elasticsearch-healthcheck-timeout. These are in seconds. Fix a panic occurring when an empty array was returned from a pipeline javascript function. Default read/write timeouts changed to 30s, up from 7s. monstache v4.13.4 \u00b6 Improvements to change-stream-namespaces monstache v3.20.4 \u00b6 Improvements to change-stream-namespaces monstache v4.13.3 \u00b6 Improvements in recovery from failed connections and errors monstache v3.20.3 \u00b6 Improvements in recovery from failed connections and errors monstache v4.13.2 \u00b6 Fix namespace parsing for collections with dots in the name in find calls in Javascript Add additional validation when certs are appended to the root store to make sure it was successful Ensure the mechanism is set to MONGODB-X509 when logging in with an X509 certificate monstache v3.20.2 \u00b6 Fix namespace parsing for collections with dots in the name in find calls in Javascript Add additional validation when certs are appended to the root store to make sure it was successful Ensure the mechanism is set to MONGODB-X509 when logging in with an X509 certificate monstache v4.13.1 \u00b6 Fix issue #157 related to the relate config Improve reliability for issue #153 by ensuring direct reads are more resilient in cluster mode Breaking: removes the dynamic nature of finding the oplog collection. Now defaults to oplog.rs . If you are still using MongoDB with master mode instead of replica sets then you now need to explicitly set mongo-oplog-collection-name to oplog.$main . monstache v3.20.1 \u00b6 Fix issue #157 related to the relate config Improve reliability for issue #153 by ensuring direct reads are more resilient in cluster mode Breaking: removes the dynamic nature of finding the oplog collection. Now defaults to oplog.rs . If you are still using MongoDB with master mode instead of replica sets then you now need to explicitly set mongo-oplog-collection-name to oplog.$main . monstache v4.13.0 \u00b6 Fixed issue where keep-src was not being honored for relate configs Added mongo-x509-settings config option to allow x509 auth when connecting Added ability to see field level changes (updateDescription) if available in javascript and golang Handle delete events if possible such that they trigger resync of related docs when a relationship exists Remove the requirement that golang plugins must implement a Map function Added the ability to override the ID sent to Elasticsearch in the mapping phase. monstache v3.20.0 \u00b6 Fixed issue where keep-src was not being honored for relate configs Added mongo-x509-settings config option to allow x509 auth when connecting Added ability to see field level changes (updateDescription) if available in javascript and golang Handle delete events if possible such that they trigger resync of related docs when a relationship exists Remove the requirement that golang plugins must implement a Map function Added the ability to override the ID sent to Elasticsearch in the mapping phase. monstache v4.12.5 \u00b6 Fix regression preventing resume-name from being set correctly Add configuration option config-database-name to configure the MongoDB database under which monstache stores metadata. Previously hard coded to monstache , the default value. monstache v3.19.5 \u00b6 Fix regression preventing resume-name from being set correctly Add configuration option config-database-name to configure the MongoDB database under which monstache stores metadata. Previously hard coded to monstache , the default value. monstache v4.12.4 \u00b6 Repeatable builds with go modules Fixes for deadlock and race conditions monstache v3.19.4 \u00b6 Repeatable builds with go modules Fixes for deadlock and race conditions monstache v4.12.3 \u00b6 Upgrade elastic client to pickup fix for AWS request signing monstache v3.19.3 \u00b6 Upgrade elastic client to pickup fix for AWS request signing monstache v4.12.2 \u00b6 Fix parse of env value with = character monstache v3.19.2 \u00b6 Fix parse of env value with = character monstache v4.12.1 \u00b6 Fix read of MONSTACHE-MONGO-CONFIG-URL environment variable monstache v3.19.1 \u00b6 Fix read of MONSTACHE-MONGO-CONFIG-URL environment variable monstache v4.12.0 \u00b6 Added the ability to configure monstache with environment variables. See issue #133 for details Added a -tpl flag to preprocess TOML config files as golang templates with access to env vars Added a disable-change-events option to turn off listening to the oplog Upgraded the monstache docker image from Alpine 3.7 to 3.8 monstache v3.19.0 \u00b6 Added the ability to configure monstache with environment variables. See issue #133 for details. Added a -tpl flag to preprocess TOML config files as golang templates with access to env vars Added a disable-change-events option to turn off listening to the oplog Upgraded the monstache docker image from Alpine 3.7 to 3.8 monstache v4.11.9 \u00b6 Fix for lock ups by removing default of no timeout Default timeouts for MongoDB set to 7s Better recovery for change-streams Breaking changes - timeouts set in config file must be greater than 0 (no timeout) monstache v3.18.9 \u00b6 Fix for lock ups by removing default of no timeout Default timeouts for MongoDB set to 7s Better recovery for change-streams Breaking changes - timeouts set in config file must be greater than 0 (no timeout) monstache v4.11.8 \u00b6 Fix panic on nil access for issue #129 monstache v3.18.8 \u00b6 Fix panic on nil access for issue #129 monstache v4.11.7 \u00b6 Ensure input.Document map contains an _id field on deletes when sent to the Process function in a plugin Ensure failed bulk response items are logged as errors monstache v3.18.7 \u00b6 Ensure input.Document map contains an _id field on deletes when sent to the Process function in a plugin Ensure failed bulk response items are logged as errors monstache v4.11.6 \u00b6 Fix issue where the file-namespaces config option was not being processed correctly monstache v3.18.6 \u00b6 Fix issue where the file-namespaces config option was not being processed correctly monstache v4.11.5 \u00b6 Fix a race condition when a related config is used and a golang plugin implements Process . monstache v3.18.5 \u00b6 Fix a race condition when a related config is used and a golang plugin implements Process . monstache v4.11.4 \u00b6 Use less CPU resources Fix for race conditions monstache v3.18.4 \u00b6 Use less CPU resources Fix for race conditions monstache v4.11.3 \u00b6 Fix an issue where a paused monstache process would not resume correctly in cluster mode monstache v3.18.3 \u00b6 Fix an issue where a paused monstache process would not resume correctly in cluster mode monstache v4.11.2 \u00b6 Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures monstache v3.18.2 \u00b6 Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures monstache v4.11.1 \u00b6 Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors monstache v3.18.1 \u00b6 Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors monstache v4.11.0 \u00b6 Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4 monstache v3.18.0 \u00b6 Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4 monstache v4.10.2 \u00b6 Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0 monstache v3.17.2 \u00b6 Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0 monstache v4.10.1 \u00b6 Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600 monstache v3.17.1 \u00b6 Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600 monstache v4.10.0 \u00b6 Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline added to the existing Map and Filter functions. The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. direct-read-namespaces = [test.test] change-stream-namespaces = [test.test] [[pipeline]] script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: {\"fullDocument.foo\": 1} } ]; } else { return [ { $match: {\"foo\": 1} } ]; } } \"\"\" [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ]); return doc; } \"\"\" monstache v3.17.0 \u00b6 Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline . The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. monstache v4.9.0 \u00b6 Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable monstache v3.16.0 \u00b6 Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable monstache v4.8.0 \u00b6 Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts monstache v3.15.0 \u00b6 Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts monstache v4.7.0 \u00b6 add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm monstache v3.14.0 \u00b6 add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm monstache v4.6.5 \u00b6 decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm monstache v3.13.5 \u00b6 decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm monstache v4.6.4 \u00b6 Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy. monstache v3.13.4 \u00b6 Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy. monstache v4.6.3 \u00b6 Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time. monstache v3.13.3 \u00b6 Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time. monstache v4.6.2 \u00b6 Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 -> 4 elasticsearch-max-docs went from 1000 -> do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB -> 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB monstache v3.13.2 \u00b6 Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 -> 4 elasticsearch-max-docs went from 1000 -> do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB -> 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB monstache v4.6.1 \u00b6 Performance and bug fixes in the gtm library monstache v3.13.1 \u00b6 Performance and bug fixes in the gtm library monstache v4.6.0 \u00b6 Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations. monstache v3.13.0 \u00b6 Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations. monstache v4.5.0 \u00b6 Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors. monstache v3.12.0 \u00b6 Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors. monstache v4.4.0 \u00b6 Updated the default delete strategy Breaking change: check delete-strategy monstache v3.11.0 \u00b6 Updated the default delete strategy Breaking change: check delete-strategy monstache v4.3.2 \u00b6 Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1 monstache v3.10.2 \u00b6 Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1 monstache v4.3.1 \u00b6 Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection. monstache v3.10.1 \u00b6 Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection. monstache v4.3.0 \u00b6 Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates monstache v3.10.0 \u00b6 Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates monstache v4.2.1 \u00b6 Ensure index names are lowercase monstache v3.9.1 \u00b6 Ensure index names are lowercase monstache v4.2.0 \u00b6 Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request. monstache v3.9.0 \u00b6 Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request. monstache v4.1.2 \u00b6 Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts monstache v3.8.2 \u00b6 Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts monstache v4.1.1 \u00b6 Route time machine docs by MongoDB source id monstache v3.8.1 \u00b6 Route time machine docs by MongoDB source id monstache v4.1.0 \u00b6 Add a nifty time machine feature monstache v3.8.0 \u00b6 Add a nifty time machine feature monstache v4.0.1 \u00b6 Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections monstache v3.7.0 \u00b6 Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections monstache v4.0.0 \u00b6 Monstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3 Fixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type Monstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42. monstache v3.6.5 \u00b6 Remove brittle normalization of index names, type names, and ids Start differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch Soon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3 Technically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward. monstache v3.6.4 \u00b6 Trying to set the record for github releases in one night Fix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog monstache v3.6.3 \u00b6 Fix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic monstache v3.6.2 \u00b6 Resume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged When Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline. monstache v3.6.1 \u00b6 Added more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production. monstache v3.6.0 \u00b6 This release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online monstache v3.5.2 \u00b6 The previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes. This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to elasticsearch-max-conns > 1) cannot actually perform a [delete, insert] instead. In this case the insert would now carry a version number < the delete version number and be rejected. monstache v3.5.1 \u00b6 Fix for issue #37 - out of order indexing due to concurrent bulk indexing requests. With elasticsearch-max-conns set to greater than 1 you may get out of order index requests; however after this fix each document is versioned such that Elasticsearch will not replace a newer version with an older one. The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred. Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time. In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order. monstache v3.5.0 \u00b6 Support for sharded MongoDB cluster. See docs for details Performance optimizations Turn off bulk retries if configured to do so monstache v3.4.2 \u00b6 Allow the stats index name format to be configurable. Continues to default to index per day. monstache v3.4.1 \u00b6 Fix for the javascript mapping functions. An Otto Export does not appear to recurse into arrays. Need to do a recursive Export for this scenario. monstache v3.4.0 \u00b6 Add ability to embed documents during the mapping phase. Javascript plugins get 3 new global functions: findId, findOne, and find. Golang plugins get access to the mgo.Session. See the docs for details. monstache v3.3.1 \u00b6 Improve support for additional indexing metadata. Fix issue where indexing metadata was not honored monstache v3.3.0 \u00b6 Added optional http server. Enable with --enable-http-server flag. Listens on :8080 by default. Configure address with --http-server-addr :8000. The server responds to the following endpoints (/started, /healthz, /config, and /stats). The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness. Upgraded the gtm library with performance improvements monstache v3.2.0 \u00b6 Add systemd support monstache v3.1.2 \u00b6 Built with go1.9 Fix golint warnings monstache v3.1.1 \u00b6 timestamp stats indexes by day for easier cleanup using e.g. curator monstache v3.1.0 \u00b6 add print-config argument to display the configuration and exit add index-stats option to write indexing statistics into Elasticsearch for analysis monstache v3.0.7 \u00b6 fix elasticsearch client http scheme for secure connections monstache v3.0.6 \u00b6 fix invalid struct field tag monstache v3.0.5 \u00b6 add direct-read-batch-size option upgrade gtm to accept batch size and to ensure all direct read errors are logged monstache v3.0.4 \u00b6 fix slowdown on direct reads for large mongodb collections monstache v3.0.3 \u00b6 small changes to the settings for the exponential back off on retry. see the docs for details. only record timestamps originating from the oplog and not from direct reads apply the worker routing filter to direct reads in worker mode monstache v3.0.2 \u00b6 add option to configure elasticsearch client http timout. up the default timeout to 60 seconds monstache v3.0.1 \u00b6 upgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed) monstache v3.0.0 \u00b6 new major release configuration changes with regards to Elasticsearch. see docs for details adds ability to write rolling logs to files adds ability to log indexing statistics changed go Elasticsearch client from elastigo to elastic which provides more API coverage upgrade gtm monstache v2.14.0 \u00b6 add support for golang plugins. you can now do in golang what you previously could do in javascript add more detail to bulk indexing errors upgrade gtm monstache v2.13.0 \u00b6 add direct-read-ns option. allows one to sync documents directly from a set of collections in addition to going through the oplog add exit-after-direct-reads option. tells monstache to exit after performing direct reads. useful for running monstache as a cron job. fix issue around custom routing where db name was being stored as an array upgrade gtm monstache v2.12.0 \u00b6 Fix order of operations surrounding db or collection drops in the oplog. Required the removal of some gtm-options introduced in 2.11. Built with latest version of gtm which includes some performance gains Add ssl option under mongo-dial-settings. Previously, in order to enable connections with TLS one had to provide a PEM file. Now, one can enable TLS without a PEM file by setting this new option to true. This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file monstache v2.11.2 \u00b6 Built with Go 1.8 Added option fail-fast Added option index-oplog-time monstache v2.11.1 \u00b6 Built with Go 1.8 Performance improvements Support for rfc7386 JSON merge patches Support for overriding Elasticsearch index and type in JavaScript More configuration options surfaced monstache v2.10.0 \u00b6 add shard routing capability add Makefile monstache v2.9.3 \u00b6 extend ttl for active in cluster to reduce process switching monstache v2.9.2 \u00b6 fix potential collision on floating point _id closes #16 monstache v2.9.1 \u00b6 fix an edge case #18 where a process resuming for the cluster would remain paused monstache v2.9 \u00b6 fix an issue with formatting of integer ids enable option for new clustering feature for high availability add TLS skip verify options for mongodb and elasticsearch add an option to specify a specific timestamp to start syncing from monstache v2.8.1 \u00b6 fix an index out of bounds panic during error reporting surface gtm options for setting the oplog database and collection name as well as the cursor timeout report an error if unable to unzip a response when verbose is true monstache v2.8 \u00b6 add a version flag -v document the elasticsearch-pem-file option add the elasticsearch-hosts option to configure pool of available nodes within a cluster monstache v2.7 \u00b6 add a gzip configuration option to increase performance default resume-name to the worker name if defined decrease binary size by building with -ldflags \"-w\" monstache v2.6 \u00b6 reuse allocations made for gridfs files add workers feature to distribute synching between multiple processes monstache v2.5 \u00b6 add option to speed up writes when saving resume state remove extra buffering when adding file content monstache v2.4 \u00b6 Fixed issue #10 Fixed issue #11 monstache v2.3 \u00b6 Added configuration option for max file size Added code to normalize index and type names based on restrictions in Elasticsearch Performance improvements for GridFs files monstache v2.2 \u00b6 Added configuration option for dropped databases and dropped collections. See the README for more information. monstache v2.1 \u00b6 Added support for dropped databases and collections. Now when you drop a database or collection from mongodb the corresponding indexes are deleted in elasticsearch. monstache v2.0 \u00b6 Fixes an issue with the default mapping between mongodb and elasticsearch. Previously, each database in mongodb was mapped to an index of the same name in elasticsearch. This creates a problem because mongodb document ids are only guaranteed unique at the collection level. If there are 2 or more documents in a mongodb database with the same id those documents were previously written to the same elasticsearch index. This fix changes the default mapping such that the entire mongodb document namespace (database + collection) is mapped to the destination index in elasticsearch. This prevents the possibility of collisions within an index. Since this change requires reindexing of previously indexed data using monstache, the version number of monstache was bumped to 2. This change also means that by default you will have an index in elasticsearch for each mongodb collection instead of each mongod database. So more indexes by default. You still have control to override the default mapping. See the docs for how to explicitly control the index and type used for a particular mongodb namespace. Bumps the go version to 1.7.3 monstache v1.3.1 \u00b6 Version 1.3 rebuilt with go1.7.1 monstache v1.3 \u00b6 Improve log messages Add support for the ingest-attachment plugin in elasticsearch 5 monstache v1.2 \u00b6 Improve Error Reporting and Add Config Options monstache v1.1 \u00b6 Fixes crash during replay (issue #2) Adds supports for indexing GridFS content (issue #3) monstache v1.0 \u00b6 64-bit Linux binary built with go1.6.2 monstache v0.8-beta.2 \u00b6 64-bit Linux binary built with go1.6.2 monstache v0.8-beta.1 \u00b6 64-bit Linux binary built with go1.6.2 monstache v0.8-beta \u00b6 64-bit Linux binary built with go1.6.2 monstache v0.8-alpha \u00b6 64-bit Linux binary built with go1.6.2","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#license","text":"The MIT License (MIT) Copyright (c) 2016 Ryan Wynn Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/#contributing","text":"The Monstache project welcomes, and depends, on contributions from developers and users in the open source community. Contributions can be made in a number of ways, a few examples are: Code patches via pull requests Documentation improvements Bug reports and patch reviews","title":"Contributing"},{"location":"about/#reporting-an-issue","text":"Please include as much detail as you can. Let us know your platform, Monstache version, MongoDB version, and Elasticsearch version.","title":"Reporting an Issue"},{"location":"about/#testing-the-development-version","text":"If you want to just install and try out the latest development version of Monstache you can do so with the following commands. This can be useful if you want to provide feedback for a new feature or want to confirm if a bug you have encountered is fixed in the git master. You will need at least golang 1.11 which includes native go modules support. cd ~/build # somewhere outside your $GOPATH git clone https://github.com/rwynn/monstache.git cd monstache git checkout <branch-to-build> go install # the resulting monstache binary will be in $GOPATH/bin","title":"Testing the Development Version"},{"location":"about/#running-the-tests","text":"To run the tests without Docker, you will need to have local mongod and elasticsearch servers running. Then you will need to start a monstache process in one terminal or in the background. monstache -verbose Finally in another terminal you can run the tests by issuing the following commands cd monstache go test -v Warning Running the Monstache tests will perform modifications to the test.test namespace in MongoDB and will index documents in the test.test index in Elasticsearch. If you have data that you need to keep on your local servers, make a back up before running the tests. Note If you don't want to setup MongoDB and Elasticsearch on your machine another option for running the tests is via Docker. After cloning the monstache repo you can cd into the docker/test directory and run run-tests.sh . You will need recent versions of docker and docker-compose to run the tests this way. Services for MongoDB and Elasticsearch will be started and the tests run on any changes you have made to the source code.","title":"Running the tests"},{"location":"about/#submitting-pull-requests","text":"Once you are happy with your changes or you are ready for some feedback, push it to your fork and send a pull request. For a change to be accepted it will most likely need to have tests and documentation if it is a new feature.","title":"Submitting Pull Requests"},{"location":"about/#release-notes","text":"See github.com for the most recent release notes.","title":"Release Notes"},{"location":"about/#monstache-v4161","text":"Fix bug in mongoX509Settings validate (issue #198) Fix issue stopping monstache when using legacy oplog tailing (non change-stream) of a sharded cluster Upgrade golang on release builds to 1.12.1 Upgrade docker images to use Alpine 3.9.2","title":"monstache v4.16.1"},{"location":"about/#monstache-v3231","text":"Fix bug in mongoX509Settings validate (issue #198) Fix issue stopping monstache when using legacy oplog tailing (non change-stream) of a sharded cluster Upgrade golang on release builds to 1.12.1 Upgrade docker images to use Alpine 3.9.2","title":"monstache v3.23.1"},{"location":"about/#monstache-v4160-4tsb","text":"Add a new integer setting direct-read-concur which, when set, limits the number of concurrent direct reads that will be performed. E.g. if you have direct-read-namespaces set to 23 namespaces and direct-read-concur set to 2, then monstache will read and sync namespace 1 and 2 concurrently and wait for both to finish before starting 3 and 4 - and so on. Add the ability to disable direct read collection splitting by setting direct-read-split-max to -1. By default, monstache will split each direct read collection up to 9 times and read each segment in a separate go routine. If you don't want to split collections at all then set direct-read-split-max to -1.","title":"monstache v4.16.0 (4tsb)"},{"location":"about/#monstache-v3230-4tsb","text":"Add a new integer setting direct-read-concur which, when set, limits the number of concurrent direct reads that will be performed. E.g. if you have direct-read-namespaces set to 23 namespaces and direct-read-concur set to 2, then monstache will read and sync namespace 1 and 2 concurrently and wait for both to finish before starting 3 and 4 - and so on. Add the ability to disable direct read collection splitting by setting direct-read-split-max to -1. By default, monstache will split each direct read collection up to 9 times and read each segment in a separate go routine. If you don't want to split collections at all then set direct-read-split-max to -1.","title":"monstache v3.23.0 (4tsb)"},{"location":"about/#monstache-v4152","text":"Fix issue using monstache with GoCenter","title":"monstache v4.15.2"},{"location":"about/#monstache-v3222","text":"Fix issue using monstache with GoCenter","title":"monstache v3.22.2"},{"location":"about/#monstache-v4151","text":"Upgrade golang to 1.12 Fix a panic under some conditions when processing the result of a golang plugin Map function","title":"monstache v4.15.1"},{"location":"about/#monstache-v3221","text":"Upgrade golang to 1.12 Fix a panic under some conditions when processing the result of a golang plugin Map function","title":"monstache v3.22.1"},{"location":"about/#monstache-v4150","text":"This release adds the ability for MongoDB 4+ users to open change streams against entire databases or entire deployments. See the documentation for the change-stream-namespaces option for details. The resume , resume-from-timestamp , replay , and cluster-name options can now be used in conjunction with change-stream-namespaces if you have MongoDB 4 or greater. Improved support for connecting directly to shards when authorization is required on the connection. This is only applicable if you are not using change streams and you are connecting to a sharded MongoDB deployment. In that case Monstache needs to discover and connect directly to shards. In this case it will reuse the login info from the initial connection to the mongos server when connecting to the shards.","title":"monstache v4.15.0"},{"location":"about/#monstache-v3220","text":"This release adds the ability for MongoDB 4+ users to open change streams against entire databases or entire deployments. See the documentation for the change-stream-namespaces option for details. The resume , resume-from-timestamp , replay , and cluster-name options can now be used in conjunction with change-stream-namespaces if you have MongoDB 4 or greater. Improved support for connecting directly to shards when authorization is required on the connection. This is only applicable if you are not using change streams and you are connecting to a sharded MongoDB deployment. In that case Monstache needs to discover and connect directly to shards. In this case it will reuse the login info from the initial connection to the mongos server when connecting to the shards.","title":"monstache v3.22.0"},{"location":"about/#monstache-v4142","text":"Change stream performance improvements","title":"monstache v4.14.2"},{"location":"about/#monstache-v3212","text":"Change stream performance improvements","title":"monstache v3.21.2"},{"location":"about/#monstache-v4141","text":"Fix for regression in previous release where the MongoDB initial connection timeout was not being honored Added a new setting relate-buffer which is the maximum number of relate events to allow in queue before skipping the relate and printing an error. This was added to prevent the scenario where a large number of relate events stall the pipeline. The default number of relates to allow in queue is 1000.","title":"monstache v4.14.1"},{"location":"about/#monstache-v3211","text":"Fix for regression in previous release where the MongoDB initial connection timeout was not being honored Added a new setting relate-buffer which is the maximum number of relate events to allow in queue before skipping the relate and printing an error. This was added to prevent the scenario where a large number of relate events stall the pipeline. The default number of relates to allow in queue is 1000.","title":"monstache v3.21.1"},{"location":"about/#monstache-v4140","text":"Performance and reliability improvements Fix for issue #168 by adding new TOML only configs elasticsearch-healthcheck-timeout-startup and elasticsearch-healthcheck-timeout. These are in seconds. Fix a panic occurring when an empty array was returned from a pipeline javascript function. Default read/write timeouts changed to 30s, up from 7s.","title":"monstache v4.14.0"},{"location":"about/#monstache-v3210","text":"Performance and reliability improvements Fix for issue #168 by adding new TOML only configs elasticsearch-healthcheck-timeout-startup and elasticsearch-healthcheck-timeout. These are in seconds. Fix a panic occurring when an empty array was returned from a pipeline javascript function. Default read/write timeouts changed to 30s, up from 7s.","title":"monstache v3.21.0"},{"location":"about/#monstache-v4134","text":"Improvements to change-stream-namespaces","title":"monstache v4.13.4"},{"location":"about/#monstache-v3204","text":"Improvements to change-stream-namespaces","title":"monstache v3.20.4"},{"location":"about/#monstache-v4133","text":"Improvements in recovery from failed connections and errors","title":"monstache v4.13.3"},{"location":"about/#monstache-v3203","text":"Improvements in recovery from failed connections and errors","title":"monstache v3.20.3"},{"location":"about/#monstache-v4132","text":"Fix namespace parsing for collections with dots in the name in find calls in Javascript Add additional validation when certs are appended to the root store to make sure it was successful Ensure the mechanism is set to MONGODB-X509 when logging in with an X509 certificate","title":"monstache v4.13.2"},{"location":"about/#monstache-v3202","text":"Fix namespace parsing for collections with dots in the name in find calls in Javascript Add additional validation when certs are appended to the root store to make sure it was successful Ensure the mechanism is set to MONGODB-X509 when logging in with an X509 certificate","title":"monstache v3.20.2"},{"location":"about/#monstache-v4131","text":"Fix issue #157 related to the relate config Improve reliability for issue #153 by ensuring direct reads are more resilient in cluster mode Breaking: removes the dynamic nature of finding the oplog collection. Now defaults to oplog.rs . If you are still using MongoDB with master mode instead of replica sets then you now need to explicitly set mongo-oplog-collection-name to oplog.$main .","title":"monstache v4.13.1"},{"location":"about/#monstache-v3201","text":"Fix issue #157 related to the relate config Improve reliability for issue #153 by ensuring direct reads are more resilient in cluster mode Breaking: removes the dynamic nature of finding the oplog collection. Now defaults to oplog.rs . If you are still using MongoDB with master mode instead of replica sets then you now need to explicitly set mongo-oplog-collection-name to oplog.$main .","title":"monstache v3.20.1"},{"location":"about/#monstache-v4130","text":"Fixed issue where keep-src was not being honored for relate configs Added mongo-x509-settings config option to allow x509 auth when connecting Added ability to see field level changes (updateDescription) if available in javascript and golang Handle delete events if possible such that they trigger resync of related docs when a relationship exists Remove the requirement that golang plugins must implement a Map function Added the ability to override the ID sent to Elasticsearch in the mapping phase.","title":"monstache v4.13.0"},{"location":"about/#monstache-v3200","text":"Fixed issue where keep-src was not being honored for relate configs Added mongo-x509-settings config option to allow x509 auth when connecting Added ability to see field level changes (updateDescription) if available in javascript and golang Handle delete events if possible such that they trigger resync of related docs when a relationship exists Remove the requirement that golang plugins must implement a Map function Added the ability to override the ID sent to Elasticsearch in the mapping phase.","title":"monstache v3.20.0"},{"location":"about/#monstache-v4125","text":"Fix regression preventing resume-name from being set correctly Add configuration option config-database-name to configure the MongoDB database under which monstache stores metadata. Previously hard coded to monstache , the default value.","title":"monstache v4.12.5"},{"location":"about/#monstache-v3195","text":"Fix regression preventing resume-name from being set correctly Add configuration option config-database-name to configure the MongoDB database under which monstache stores metadata. Previously hard coded to monstache , the default value.","title":"monstache v3.19.5"},{"location":"about/#monstache-v4124","text":"Repeatable builds with go modules Fixes for deadlock and race conditions","title":"monstache v4.12.4"},{"location":"about/#monstache-v3194","text":"Repeatable builds with go modules Fixes for deadlock and race conditions","title":"monstache v3.19.4"},{"location":"about/#monstache-v4123","text":"Upgrade elastic client to pickup fix for AWS request signing","title":"monstache v4.12.3"},{"location":"about/#monstache-v3193","text":"Upgrade elastic client to pickup fix for AWS request signing","title":"monstache v3.19.3"},{"location":"about/#monstache-v4122","text":"Fix parse of env value with = character","title":"monstache v4.12.2"},{"location":"about/#monstache-v3192","text":"Fix parse of env value with = character","title":"monstache v3.19.2"},{"location":"about/#monstache-v4121","text":"Fix read of MONSTACHE-MONGO-CONFIG-URL environment variable","title":"monstache v4.12.1"},{"location":"about/#monstache-v3191","text":"Fix read of MONSTACHE-MONGO-CONFIG-URL environment variable","title":"monstache v3.19.1"},{"location":"about/#monstache-v4120","text":"Added the ability to configure monstache with environment variables. See issue #133 for details Added a -tpl flag to preprocess TOML config files as golang templates with access to env vars Added a disable-change-events option to turn off listening to the oplog Upgraded the monstache docker image from Alpine 3.7 to 3.8","title":"monstache v4.12.0"},{"location":"about/#monstache-v3190","text":"Added the ability to configure monstache with environment variables. See issue #133 for details. Added a -tpl flag to preprocess TOML config files as golang templates with access to env vars Added a disable-change-events option to turn off listening to the oplog Upgraded the monstache docker image from Alpine 3.7 to 3.8","title":"monstache v3.19.0"},{"location":"about/#monstache-v4119","text":"Fix for lock ups by removing default of no timeout Default timeouts for MongoDB set to 7s Better recovery for change-streams Breaking changes - timeouts set in config file must be greater than 0 (no timeout)","title":"monstache v4.11.9"},{"location":"about/#monstache-v3189","text":"Fix for lock ups by removing default of no timeout Default timeouts for MongoDB set to 7s Better recovery for change-streams Breaking changes - timeouts set in config file must be greater than 0 (no timeout)","title":"monstache v3.18.9"},{"location":"about/#monstache-v4118","text":"Fix panic on nil access for issue #129","title":"monstache v4.11.8"},{"location":"about/#monstache-v3188","text":"Fix panic on nil access for issue #129","title":"monstache v3.18.8"},{"location":"about/#monstache-v4117","text":"Ensure input.Document map contains an _id field on deletes when sent to the Process function in a plugin Ensure failed bulk response items are logged as errors","title":"monstache v4.11.7"},{"location":"about/#monstache-v3187","text":"Ensure input.Document map contains an _id field on deletes when sent to the Process function in a plugin Ensure failed bulk response items are logged as errors","title":"monstache v3.18.7"},{"location":"about/#monstache-v4116","text":"Fix issue where the file-namespaces config option was not being processed correctly","title":"monstache v4.11.6"},{"location":"about/#monstache-v3186","text":"Fix issue where the file-namespaces config option was not being processed correctly","title":"monstache v3.18.6"},{"location":"about/#monstache-v4115","text":"Fix a race condition when a related config is used and a golang plugin implements Process .","title":"monstache v4.11.5"},{"location":"about/#monstache-v3185","text":"Fix a race condition when a related config is used and a golang plugin implements Process .","title":"monstache v3.18.5"},{"location":"about/#monstache-v4114","text":"Use less CPU resources Fix for race conditions","title":"monstache v4.11.4"},{"location":"about/#monstache-v3184","text":"Use less CPU resources Fix for race conditions","title":"monstache v3.18.4"},{"location":"about/#monstache-v4113","text":"Fix an issue where a paused monstache process would not resume correctly in cluster mode","title":"monstache v4.11.3"},{"location":"about/#monstache-v3183","text":"Fix an issue where a paused monstache process would not resume correctly in cluster mode","title":"monstache v3.18.3"},{"location":"about/#monstache-v4112","text":"Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures","title":"monstache v4.11.2"},{"location":"about/#monstache-v3182","text":"Fix an issue with workers where only one worker would be used for change documents Redact sensitive connection information when logging connection failures","title":"monstache v3.18.2"},{"location":"about/#monstache-v4111","text":"Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors","title":"monstache v4.11.1"},{"location":"about/#monstache-v3181","text":"Fix for connection checker thread exiting early in cluster mode Better handling of JSON serialization errors","title":"monstache v3.18.1"},{"location":"about/#monstache-v4110","text":"Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4","title":"monstache v4.11.0"},{"location":"about/#monstache-v3180","text":"Reliability improvements Fix allowing one to use a MongoDB view as a direct-read-namespace Addition of the relate config to declare dependencies between collections Experimental support for AWS Signing Version 4","title":"monstache v3.18.0"},{"location":"about/#monstache-v4102","text":"Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0","title":"monstache v4.10.2"},{"location":"about/#monstache-v3172","text":"Fixes related to i/o timeout errors Default timeout configurations changed to no timeout (0) with the exception of the initial connection to MongoDB which times out after 15 seconds. Values of 0 disable timeouts. All other positive values are in seconds. New defaults shown below. You probably do not need to specify any of these values unless you encounter problems. [mongo-dial-settings] timeout=15 read-timeout=0 write-timeout=0 [mongo-session-settings] socket-timeout=0 sync-timeout=0","title":"monstache v3.17.2"},{"location":"about/#monstache-v4101","text":"Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600","title":"monstache v4.10.1"},{"location":"about/#monstache-v3171","text":"Clean up timeout configurations and increase default timeout values New timeout configurations surfaced - read and write timeout. Configure as follows (default values shown): [mongo-dial-settings] timeout=10 read-timeout=600 write-timeout=30 [mongo-session-settings] socket-timeout=600 sync-timeout=600","title":"monstache v3.17.1"},{"location":"about/#monstache-v4100","text":"Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline added to the existing Map and Filter functions. The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument. direct-read-namespaces = [test.test] change-stream-namespaces = [test.test] [[pipeline]] script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: {\"fullDocument.foo\": 1} } ]; } else { return [ { $match: {\"foo\": 1} } ]; } } \"\"\" [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ]); return doc; } \"\"\"","title":"monstache v4.10.0"},{"location":"about/#monstache-v3170","text":"Fix for issue #97 where monstache would exit before syncing all documents with -exit-after-direct-reads enabled Support added for MongoDB change streams via the change-stream-namespaces option New golang plugin functions Process and Pipeline . The Process function allows one to code complex processing after an event. The Process function has access to the MongoDB session, the Elasticsearch client, the Elasticsearch bulk processor, and information about the change that occurred (insert, update, delete). The Pipeline function allows one to assign MongoDB pipeline stages to both direct reads and change streams. Since the pipeline stages may differ between direct reads and change streams the function is passed a boolean indicating the source of the data. For example, a $match clause on the change stream may need to reference the fullDocument field since the root will be the change event. For direct reads the root will simply be the full document. New config option pipeline allows one to create aggregation pipelines in javascript for direct reads and change streams. This can be used instead of the Pipeline function in a golang plugin. The exported function in javascript takes a namespace and a boolean indicating whether or not the source was a change stream. The function should return an array of pipeline stages to apply. New config option pipe-allow-disk which when enabled allows large pipelines to use the disk to save intermediate results. New global function available in javascript script functions named pipe . The pipe function is simliar to existing find function but takes an array of aggregation pipeline stages as the first argument.","title":"monstache v3.17.0"},{"location":"about/#monstache-v490","text":"Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable","title":"monstache v4.9.0"},{"location":"about/#monstache-v3160","text":"Fix to omit version information on deletes when the index-as-update setting is ON (to match the omitted version information at indexing time) Fix issue #89 by making the indexed oplog field names and date format configurable","title":"monstache v3.16.0"},{"location":"about/#monstache-v480","text":"Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts","title":"monstache v4.8.0"},{"location":"about/#monstache-v3150","text":"Seperate namespace regexes for drop operations vs create/update/delete operations Better handling of panics A new index-as-update boolean config option that allow merge instead of replace Fixes to the find and findOne functions available in scripts","title":"monstache v3.15.0"},{"location":"about/#monstache-v470","text":"add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm","title":"monstache v4.7.0"},{"location":"about/#monstache-v3140","text":"add -pprof setting. When enabled and combined with -enable-http-server you can read profiling information. See Profiling for Go for more information. add -enable-easy-json setting. When enabled easy-json will be used for serialization to Elasticsearch. tweaks to the tailing code in gtm","title":"monstache v3.14.0"},{"location":"about/#monstache-v465","text":"decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm","title":"monstache v4.6.5"},{"location":"about/#monstache-v3135","text":"decrease the fetch channel flush timeout clarify version conflicts and invalid json messages as warnings remove the /config endpoint for better security (use -print-config instead) fix LoadPlugins method (contributed by @YouthLab) performance tweaks in gtm","title":"monstache v3.13.5"},{"location":"about/#monstache-v464","text":"Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy.","title":"monstache v4.6.4"},{"location":"about/#monstache-v3134","text":"Expose new setting direct-read-split-max which limits the number of times a collection is split for reading during direct-reads and thus the number of go routines and MongoDB connections spawned. The default is 9. Tune this setting to increase/decrease the amount of memory the monstache process will consume. Better Docker support due to the contributions of @a-magdy.","title":"monstache v3.13.4"},{"location":"about/#monstache-v463","text":"Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time.","title":"monstache v4.6.3"},{"location":"about/#monstache-v3133","text":"Fix for issue #65, year outside of [0,9999]. Invalid time will be removed now with prune-invalid-json turned on Fix for issue #62, the number of connections to MongoDB is now limited to a max of 32 per namespace Fix for issue #59, unsupported values of +/- Infinity and NaN. These values can now be removed with the prune-invalid-json setting Fix for issue $46 and #66, having to do with filtering. Filters now use locks to ensure the javascript environment is used by one at a time.","title":"monstache v3.13.3"},{"location":"about/#monstache-v462","text":"Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 -> 4 elasticsearch-max-docs went from 1000 -> do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB -> 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB","title":"monstache v4.6.2"},{"location":"about/#monstache-v3132","text":"Fix regression in 3.13 series where collections under 50K documents were not synching Performing Tuning. The following defaults have changed so please update your config files accordingly. elasticsearch-max-conns went from 10 -> 4 elasticsearch-max-docs went from 1000 -> do not flush based on count (I suggest not overriding this since document sizes can vary greatly - instead use max-bytes) elasticsearch-max-bytes went from 5MB -> 8MB Note when you specify elasticseach-max-bytes the value must be in bytes not MB","title":"monstache v3.13.2"},{"location":"about/#monstache-v461","text":"Performance and bug fixes in the gtm library","title":"monstache v4.6.1"},{"location":"about/#monstache-v3131","text":"Performance and bug fixes in the gtm library","title":"monstache v3.13.1"},{"location":"about/#monstache-v460","text":"Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.","title":"monstache v4.6.0"},{"location":"about/#monstache-v3130","text":"Performance improvements. Much of the performance gains come from an upgrade of the gtm library. This library now uses split vector failing back to a paginated range queries. Also, some buffering has been removed at the gtm level for certain operations.","title":"monstache v3.13.0"},{"location":"about/#monstache-v450","text":"Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors.","title":"monstache v4.5.0"},{"location":"about/#monstache-v3120","text":"Adds an option delete-index-pattern to specify an Elasticsearch index pattern to scope stateless deletes. Indexes outside of this pattern will not be considered when propogating deletes from MongoDB to Elasticsearch. By default all Elasticsearch indexes are queried. Adds the ability to specify a global filter function in Javascript. Previously, a filter function needed to be tied to a MongoDB namespace. Now you can leave off the namespace and the filter function will be applied to all namespaces. The filter function will receive the document as the first argument and the MongoDB namespace as the second argument. Breaking change: direct-read-cursors and direct-read-batch-size have been removed as options. The underlying gtm library of monstache has been upgraded and no longer supports parallelCollectionScan which is being removed in future versions of MongoDB. Now gtm will use splitVector to divy up collections to read documents concurrently. Also, the batch size will be managed by MongoDB and not set explicitly. See the gtm library docs for more information. Adds a boolean configuration option, prune-invalid-json , which defaults to false. Set this to true if your MongoDB data has values such as +Inf, -Inf, or NaN which are not supported by the golang JSON parser and cause infinite error loops to occur. With prune-invalid-json set to true Monstache will remove these values before indexing into Elasticsearch to avoid these errors.","title":"monstache v3.12.0"},{"location":"about/#monstache-v440","text":"Updated the default delete strategy Breaking change: check delete-strategy","title":"monstache v4.4.0"},{"location":"about/#monstache-v3110","text":"Updated the default delete strategy Breaking change: check delete-strategy","title":"monstache v3.11.0"},{"location":"about/#monstache-v432","text":"Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1","title":"monstache v4.3.2"},{"location":"about/#monstache-v3102","text":"Allow specifying a script without a namespace. In this case documents from all collections will be run through the script. The document object will continue to be the 1st argument to the function and a new 2nd argument will be the namespace of the source document. Fixes #55. Fix an issue where a Date object created in Javascript would not be formatted correctly for indexing. Build with go 1.10.1","title":"monstache v3.10.2"},{"location":"about/#monstache-v431","text":"Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.","title":"monstache v4.3.1"},{"location":"about/#monstache-v3101","text":"Upgrade gtm to pick up fix for parallel collection scans on direct reads. Each cursor now gets its own connection.","title":"monstache v3.10.1"},{"location":"about/#monstache-v430","text":"Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates","title":"monstache v4.3.0"},{"location":"about/#monstache-v3100","text":"Upgrade gtm to pick up support for parallel collection scan on direct reads if your mongodb storage engine supports it Add config option to specify the number of cursors to request for parallel collection scans Allow mappings to specify overrides for 1 of index and type instead of requiring both Fix an issue where filters were not being applied to document updates","title":"monstache v3.10.0"},{"location":"about/#monstache-v421","text":"Ensure index names are lowercase","title":"monstache v4.2.1"},{"location":"about/#monstache-v391","text":"Ensure index names are lowercase","title":"monstache v3.9.1"},{"location":"about/#monstache-v420","text":"Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request.","title":"monstache v4.2.0"},{"location":"about/#monstache-v390","text":"Add filtering to Javascript and Golang plugins. Filtered documents are completely ignored while dropped documents result in a delete request.","title":"monstache v3.9.0"},{"location":"about/#monstache-v412","text":"Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts","title":"monstache v4.1.2"},{"location":"about/#monstache-v382","text":"Fix custom routing for golang plugins Configuration now supports paths to Javascript files in addition to inline scripts","title":"monstache v3.8.2"},{"location":"about/#monstache-v411","text":"Route time machine docs by MongoDB source id","title":"monstache v4.1.1"},{"location":"about/#monstache-v381","text":"Route time machine docs by MongoDB source id","title":"monstache v3.8.1"},{"location":"about/#monstache-v410","text":"Add a nifty time machine feature","title":"monstache v4.1.0"},{"location":"about/#monstache-v380","text":"Add a nifty time machine feature","title":"monstache v3.8.0"},{"location":"about/#monstache-v401","text":"Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections","title":"monstache v4.0.1"},{"location":"about/#monstache-v370","text":"Fixed a bug where monstache would think direct reads were done when they had not even started Performance improvements for direct reads on large collections","title":"monstache v3.7.0"},{"location":"about/#monstache-v400","text":"Monstache v4+ should be used for ES6+. There will still be bug fixes and maintenance done to the Monstache v3 releases to support ES2-5. You can still download v3.x releases from the downloads page or by directing go get to gopkg.in/rwynn/monstache.v3 Fixes deprecation warnings during bulk indexing against ES6 because of renamed fields version and version_type Monstache will now default to using the ES type _doc (as opposed to the MongoDB collection name) when it detects ES 6.2+. This is the new recommended type name going forward. See issue #42.","title":"monstache v4.0.0"},{"location":"about/#monstache-v365","text":"Remove brittle normalization of index names, type names, and ids Start differentiating between releases supporting ES6+ and pre-ES6 by releasing from rel3 branch Soon a 4.0.0 release will be cut from master that will be ES6 forward. pre-ES6 will still be supported by downloading 3.x releases from the releases page or directing go get to gopkg.in/rwynn/monstache.v3 Technically this release will still work with ES+ but that won't last forever. There are some deprecation warnings. In summary, if you need pre-ES6 use v3.x releases of monstache and v4.x releases of monstache for ES6+ going forward.","title":"monstache v3.6.5"},{"location":"about/#monstache-v364","text":"Trying to set the record for github releases in one night Fix a regression whereby monstache would exit after direct reads were complete when it should have kept tailing the oplog","title":"monstache v3.6.4"},{"location":"about/#monstache-v363","text":"Fix for a benign race condition in shutdown, introduced in 3.6.2, that caused a panic","title":"monstache v3.6.3"},{"location":"about/#monstache-v362","text":"Resume usage of upstream elastic client library now that fix for Elasticsearch going down has been merged When Elasticsearch goes down the elastic client will now put back pressure on Add and Flush calls. When Elasticsearch comes back up it will resume Adding and Flushing were it left off. Do to the blocking nature of Add and Flush the shutdown function of monstache has been refactored to take this into account. Shutdown will not hang if Elasticsearch is down. It will try to Flush pending documents but if this blocks due to a down server it will still exit after a 5 second deadline.","title":"monstache v3.6.2"},{"location":"about/#monstache-v361","text":"Added more detailed error logging. Each bulk request line that failed will be logged separately with details. This is much more lightweight than having to turn on verbose to get error details. Verbose is not a recommended setting for production.","title":"monstache v3.6.1"},{"location":"about/#monstache-v360","text":"This release focuses on improvements with regards to handling dropped connections to either Elasticsearch or MongoDB and resuming gracefully when they come back online","title":"monstache v3.6.0"},{"location":"about/#monstache-v352","text":"The previous release safeguards the integrity of inserts and updates with a version number, but neglected deletes. This release adds versions to deletes such that an [insert, delete] sequence that gets sent to Elasticsearch in 2 different requests (due to elasticsearch-max-conns > 1) cannot actually perform a [delete, insert] instead. In this case the insert would now carry a version number < the delete version number and be rejected.","title":"monstache v3.5.2"},{"location":"about/#monstache-v351","text":"Fix for issue #37 - out of order indexing due to concurrent bulk indexing requests. With elasticsearch-max-conns set to greater than 1 you may get out of order index requests; however after this fix each document is versioned such that Elasticsearch will not replace a newer version with an older one. The version of the document is the timestamp from the MongoDB oplog of when the change (insert, update) occurred. Out of order indexing typically happens when both an insert and an update are queued for a bulk request at around the same time. In this case, do to the way the bulk processor multiplexes requests onto multiple connections, the document may be received out of order.","title":"monstache v3.5.1"},{"location":"about/#monstache-v350","text":"Support for sharded MongoDB cluster. See docs for details Performance optimizations Turn off bulk retries if configured to do so","title":"monstache v3.5.0"},{"location":"about/#monstache-v342","text":"Allow the stats index name format to be configurable. Continues to default to index per day.","title":"monstache v3.4.2"},{"location":"about/#monstache-v341","text":"Fix for the javascript mapping functions. An Otto Export does not appear to recurse into arrays. Need to do a recursive Export for this scenario.","title":"monstache v3.4.1"},{"location":"about/#monstache-v340","text":"Add ability to embed documents during the mapping phase. Javascript plugins get 3 new global functions: findId, findOne, and find. Golang plugins get access to the mgo.Session. See the docs for details.","title":"monstache v3.4.0"},{"location":"about/#monstache-v331","text":"Improve support for additional indexing metadata. Fix issue where indexing metadata was not honored","title":"monstache v3.3.1"},{"location":"about/#monstache-v330","text":"Added optional http server. Enable with --enable-http-server flag. Listens on :8080 by default. Configure address with --http-server-addr :8000. The server responds to the following endpoints (/started, /healthz, /config, and /stats). The stats endpoint is only enabled if stats are enabled. The /started and /healthz endpoints can be used to check for liveness. Upgraded the gtm library with performance improvements","title":"monstache v3.3.0"},{"location":"about/#monstache-v320","text":"Add systemd support","title":"monstache v3.2.0"},{"location":"about/#monstache-v312","text":"Built with go1.9 Fix golint warnings","title":"monstache v3.1.2"},{"location":"about/#monstache-v311","text":"timestamp stats indexes by day for easier cleanup using e.g. curator","title":"monstache v3.1.1"},{"location":"about/#monstache-v310","text":"add print-config argument to display the configuration and exit add index-stats option to write indexing statistics into Elasticsearch for analysis","title":"monstache v3.1.0"},{"location":"about/#monstache-v307","text":"fix elasticsearch client http scheme for secure connections","title":"monstache v3.0.7"},{"location":"about/#monstache-v306","text":"fix invalid struct field tag","title":"monstache v3.0.6"},{"location":"about/#monstache-v305","text":"add direct-read-batch-size option upgrade gtm to accept batch size and to ensure all direct read errors are logged","title":"monstache v3.0.5"},{"location":"about/#monstache-v304","text":"fix slowdown on direct reads for large mongodb collections","title":"monstache v3.0.4"},{"location":"about/#monstache-v303","text":"small changes to the settings for the exponential back off on retry. see the docs for details. only record timestamps originating from the oplog and not from direct reads apply the worker routing filter to direct reads in worker mode","title":"monstache v3.0.3"},{"location":"about/#monstache-v302","text":"add option to configure elasticsearch client http timout. up the default timeout to 60 seconds","title":"monstache v3.0.2"},{"location":"about/#monstache-v301","text":"upgrade gtm to fix an issue where a mongodb query error (such as CappedPositionLost) causes the tail go routine to exit (after which no more events will be processed)","title":"monstache v3.0.1"},{"location":"about/#monstache-v300","text":"new major release configuration changes with regards to Elasticsearch. see docs for details adds ability to write rolling logs to files adds ability to log indexing statistics changed go Elasticsearch client from elastigo to elastic which provides more API coverage upgrade gtm","title":"monstache v3.0.0"},{"location":"about/#monstache-v2140","text":"add support for golang plugins. you can now do in golang what you previously could do in javascript add more detail to bulk indexing errors upgrade gtm","title":"monstache v2.14.0"},{"location":"about/#monstache-v2130","text":"add direct-read-ns option. allows one to sync documents directly from a set of collections in addition to going through the oplog add exit-after-direct-reads option. tells monstache to exit after performing direct reads. useful for running monstache as a cron job. fix issue around custom routing where db name was being stored as an array upgrade gtm","title":"monstache v2.13.0"},{"location":"about/#monstache-v2120","text":"Fix order of operations surrounding db or collection drops in the oplog. Required the removal of some gtm-options introduced in 2.11. Built with latest version of gtm which includes some performance gains Add ssl option under mongo-dial-settings. Previously, in order to enable connections with TLS one had to provide a PEM file. Now, one can enable TLS without a PEM file by setting this new option to true. This was tested with MongoDB Atlas which requires SSL but does not provide a PEM file","title":"monstache v2.12.0"},{"location":"about/#monstache-v2112","text":"Built with Go 1.8 Added option fail-fast Added option index-oplog-time","title":"monstache v2.11.2"},{"location":"about/#monstache-v2111","text":"Built with Go 1.8 Performance improvements Support for rfc7386 JSON merge patches Support for overriding Elasticsearch index and type in JavaScript More configuration options surfaced","title":"monstache v2.11.1"},{"location":"about/#monstache-v2100","text":"add shard routing capability add Makefile","title":"monstache v2.10.0"},{"location":"about/#monstache-v293","text":"extend ttl for active in cluster to reduce process switching","title":"monstache v2.9.3"},{"location":"about/#monstache-v292","text":"fix potential collision on floating point _id closes #16","title":"monstache v2.9.2"},{"location":"about/#monstache-v291","text":"fix an edge case #18 where a process resuming for the cluster would remain paused","title":"monstache v2.9.1"},{"location":"about/#monstache-v29","text":"fix an issue with formatting of integer ids enable option for new clustering feature for high availability add TLS skip verify options for mongodb and elasticsearch add an option to specify a specific timestamp to start syncing from","title":"monstache v2.9"},{"location":"about/#monstache-v281","text":"fix an index out of bounds panic during error reporting surface gtm options for setting the oplog database and collection name as well as the cursor timeout report an error if unable to unzip a response when verbose is true","title":"monstache v2.8.1"},{"location":"about/#monstache-v28","text":"add a version flag -v document the elasticsearch-pem-file option add the elasticsearch-hosts option to configure pool of available nodes within a cluster","title":"monstache v2.8"},{"location":"about/#monstache-v27","text":"add a gzip configuration option to increase performance default resume-name to the worker name if defined decrease binary size by building with -ldflags \"-w\"","title":"monstache v2.7"},{"location":"about/#monstache-v26","text":"reuse allocations made for gridfs files add workers feature to distribute synching between multiple processes","title":"monstache v2.6"},{"location":"about/#monstache-v25","text":"add option to speed up writes when saving resume state remove extra buffering when adding file content","title":"monstache v2.5"},{"location":"about/#monstache-v24","text":"Fixed issue #10 Fixed issue #11","title":"monstache v2.4"},{"location":"about/#monstache-v23","text":"Added configuration option for max file size Added code to normalize index and type names based on restrictions in Elasticsearch Performance improvements for GridFs files","title":"monstache v2.3"},{"location":"about/#monstache-v22","text":"Added configuration option for dropped databases and dropped collections. See the README for more information.","title":"monstache v2.2"},{"location":"about/#monstache-v21","text":"Added support for dropped databases and collections. Now when you drop a database or collection from mongodb the corresponding indexes are deleted in elasticsearch.","title":"monstache v2.1"},{"location":"about/#monstache-v20","text":"Fixes an issue with the default mapping between mongodb and elasticsearch. Previously, each database in mongodb was mapped to an index of the same name in elasticsearch. This creates a problem because mongodb document ids are only guaranteed unique at the collection level. If there are 2 or more documents in a mongodb database with the same id those documents were previously written to the same elasticsearch index. This fix changes the default mapping such that the entire mongodb document namespace (database + collection) is mapped to the destination index in elasticsearch. This prevents the possibility of collisions within an index. Since this change requires reindexing of previously indexed data using monstache, the version number of monstache was bumped to 2. This change also means that by default you will have an index in elasticsearch for each mongodb collection instead of each mongod database. So more indexes by default. You still have control to override the default mapping. See the docs for how to explicitly control the index and type used for a particular mongodb namespace. Bumps the go version to 1.7.3","title":"monstache v2.0"},{"location":"about/#monstache-v131","text":"Version 1.3 rebuilt with go1.7.1","title":"monstache v1.3.1"},{"location":"about/#monstache-v13","text":"Improve log messages Add support for the ingest-attachment plugin in elasticsearch 5","title":"monstache v1.3"},{"location":"about/#monstache-v12","text":"Improve Error Reporting and Add Config Options","title":"monstache v1.2"},{"location":"about/#monstache-v11","text":"Fixes crash during replay (issue #2) Adds supports for indexing GridFS content (issue #3)","title":"monstache v1.1"},{"location":"about/#monstache-v10","text":"64-bit Linux binary built with go1.6.2","title":"monstache v1.0"},{"location":"about/#monstache-v08-beta2","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta.2"},{"location":"about/#monstache-v08-beta1","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta.1"},{"location":"about/#monstache-v08-beta","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-beta"},{"location":"about/#monstache-v08-alpha","text":"64-bit Linux binary built with go1.6.2","title":"monstache v0.8-alpha"},{"location":"advanced/","text":"Advanced \u00b6 Versions \u00b6 Monstache version Git branch (used to build plugin) Docker tag Description Elasticsearch MongoDB Status 6 rel6 rel6, latest MongoDB, Inc. go driver Version 7+ Version 2.6+ Supported 5 rel5 rel5 MongoDB, Inc. go driver Version 6 Version 2.6+ Supported 4 master rel4 mgo community go driver Version 6 Version 3 Deprecated 3 rel3 rel3 mgo community go driver Versions 2 and 5 Version 3 Deprecated Note You can use monstache rel5 and rel6 with MongoDB versions back to 2.6 with the following caveats. If you have MongoDB 3.6 then you must explicitly enumerate collections in your change-stream-namespaces setting because change streams against databases and entire deployments was not introduced until MongoDB version 4.0. Alternatively, you can disable change events entirely with disable-change-events . You must also set resume-strategy to 1 to use a token-based resume strategy compatibile with MongoDB API 3.6. If you have MongoDB 2.6 - 3.5 then you must omit any mention of change-stream-namespaces in your config file because change streams were first introduced in 3.6. To emulate change events you must turn on the option enable-oplog . Alternatively, you can disable change events entirely with disable-change-events . Warning Your MongoDB binary version does not always mean that the feature compatibility is at that same level. Check your feature compatibility version from the MongoDB console to ensure that MongoDB is not operating in a lesser capability mode. This sometimes happens when MongoDB is upgraded in place or MongoDB is started with a data directory of a previous installation. Sometimes there are reasons to stay at a lower feature compatibility so check before you upgrade it. GridFS Support \u00b6 Monstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full text search. This feature requires that you install an Elasticsearch plugin which enables the field type attachment . For versions of Elasticsearch prior to version 5 you should install the mapper-attachments plugin. For version 5 or later of Elasticsearch you should instead install the ingest-attachment plugin. Once you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is as simple as configuring monstache. You will want to enable the index-files option and also tell monstache the namespace of all collections which will hold GridFS files. For example in your TOML config file, index-files = true direct-read-namespaces = [\"users.fs.files\", \"posts.fs.files\"] file-namespaces = [\"users.fs.files\", \"posts.fs.files\"] file-highlighting = true The above configuration tells monstache that you wish to index the raw content of GridFS files in the users and posts MongoDB databases. By default, MongoDB uses a bucket named fs , so if you just use the defaults your collection name will be fs.files . However, if you have customized the bucket name, then your file collection would be something like mybucket.files and the entire namespace would be users.mybucket.files . When you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in Elasticsearch have a field named file with a type mapping of attachment . For the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into Elasticsearch by issuing the following REST commands: For Elasticsearch versions prior to version 5... POST /users.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} POST /posts.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} For Elasticsearch version 5 and above... PUT /_ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\" } } ] } When a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw content, and index a document into Elasticsearch with the raw content stored in a file field as a base64 encoded string. The Elasticsearch plugin will then extract text content from the raw content using Apache Tika , tokenize the text content, and allow you to query on the content of the file. To test this feature of monstache you can simply use the mongofiles command to quickly add a file to MongoDB via GridFS. Continuing the example above one could issue the following command to put a file named resume.docx into GridFS and after a short time this file should be searchable in Elasticsearch in the index users.fs.files . mongofiles -d users put resume.docx After a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch curl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\" If you would like to see the text extracted by Apache Tika you can project the appropriate sub-field For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [ \"file.content\" ], \"query\": { \"match\": { \"file.content\": \"golang\" } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [ \"attachment.content\" ], \"query\": { \"match\": { \"attachment.content\": \"golang\" } } }' When file-highlighting is enabled you can add a highlight clause to your query For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [\"file.content\"], \"query\": { \"match\": { \"file.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"file.content\": { } } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [\"attachment.content\"], \"query\": { \"match\": { \"attachment.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"attachment.content\": { } } } }' The highlight response will contain emphasis on the matching terms For Elasticsearch versions prior to version 5... \"hits\" : [ { \"highlight\" : { \"file.content\" : [ \"I like to program in <em>golang</em>.\\n\\n\" ] } } ] For Elasticsearch version 5 and above... \"hits\" : [{ \"highlight\" : { \"attachment.content\" : [ \"I like to program in <em>golang</em>.\" ] } }] Workers \u00b6 You can run multiple monstache processes and distribute the work between them. First configure the names of all the workers in a shared config.toml file. workers = [\"Tom\", \"Dick\", \"Harry\"] In this case we have 3 workers. Now we can start 3 monstache processes and give each one of the worker names. monstache -f config.toml -worker Tom monstache -f config.toml -worker Dick monstache -f config.toml -worker Harry monstache will hash the id of each document using consistent hashing so that each id is handled by only one of the available workers. High Availability \u00b6 You can run monstache in high availability mode by starting multiple processes with the same value for cluster-name . Each process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch. High availability works by ensuring one active process in the monstache.cluster collection in MongoDB at any given time. Only the process in this collection will be syncing for the cluster. Processes not present in this collection will be paused. Documents in the monstache.cluster collection have a TTL assigned to them. When a document in this collection times out it will be removed from the collection by MongoDB and another process in the monstache cluster will have a chance to write to the collection and become the new active process. When cluster-name is supplied the resume feature is automatically turned on and the resume-name becomes the name of the cluster. This is to ensure that each of the processes is able to pick up syncing where the last one left off. You can combine the HA feature with the workers feature. For 3 cluster nodes with 3 workers per node you would have something like the following: // config.toml workers = [\"Tom\", \"Dick\", \"Harry\"] // on host A monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host B monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host C monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml When the clustering feature is combined with workers then the resume-name becomes the cluster name concatenated with the worker name. Index Mapping \u00b6 When indexing documents from MongoDB into Elasticsearch the default mapping is as follows: For Elasticsearch prior to 6.2 Elasticsearch index name <= MongoDB database name . MongoDB collection name Elasticsearch type <= MongoDB collection name Elasticsearch document _id <= MongoDB document _id For Elasticsearch 6.2+ Elasticsearch index name <= MongoDB database name . MongoDB collection name Elasticsearch type <= _doc Elasticsearch document _id <= MongoDB document _id If these default won't work for some reason you can override the index and type mapping on a per collection basis by adding the following to your TOML config file: [[mapping]] namespace = \"test.test\" index = \"index1\" type = \"type1\" [[mapping]] namespace = \"test.test2\" index = \"index2\" type = \"type2\" With the configuration above documents in the test.test namespace in MongoDB are indexed into the index1 index in Elasticsearch with the type1 type. If you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then see the sections Middleware and Routing . Warning It is not recommended to override the default type of _doc if using Elasticsearch 6.2+ since this will be the supported path going forward. Also, using _doc as the type will not work with Elasticsearch prior to 6.2. Make sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache. If automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create. Namespaces \u00b6 When a document is inserted, updated, or deleted in MongoDB a document is appended to the oplog representing the event. This document has a field ns which is the namespace. For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for use test; db.foo.insert({hello: \"world\"}); the namespace for the event in the oplog would be test.foo . In addition to inserts, updates, and deletes monstache also supports database and collection drops. When a database or collection is dropped in MongoDB an event is appended to the oplog. Like the other types of changes this event has a field ns representing the namespace. However, for drops the namespace is the database name and the string $cmd joined by a dot. E.g. for use test; db.foo.drop() the namespace for the event in the oplog would be test.$cmd . Middleware \u00b6 monstache supports embedding user defined middleware between MongoDB and Elasticsearch. Middleware is able to transform documents, drop documents, or define indexing metadata. Middleware may be written in either Javascript or in Golang as a plugin. Warning It is HIGHLY recommended to use a golang plugin in production over a javascript plugin due to performance differences. Currently, golang plugins are orders of magnitude faster than javascript plugins. This is due to concurrency and the need to perform locking on the javascript environment. Javascript plugins are very useful for quickly prototyping a solution, however at some point it is recommended to convert them to golang plugins. If you enable a Golang plugin then monstache will ignore an javascript middleware in your configuration. This may change in the future but for now the choice of middleware language is mutually exclusive. Golang \u00b6 monstache supports golang plugins. You should have golang version 1.11 or greater installed and will need to perform the build on the Linux or OSX platform. Golang plugins are not currently supported on the Windows platform due to limits in golang. To implement a plugin for monstache you need to implement specific function signatures, use the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache. See this wiki page for an example using Docker. Warning Golang plugins must be built with the exact same source code (including dependencies) of the loading program. If you don't build your plugin this way then monstache may fail to load it at runtime due to source code mismatches. To create a golang plugin for monstache git clone monstache somewhere outside your $GOPATH git checkout a specific monstache version tag (e.g. v6.7.4 ). See Versions above. in the monstache root directory run go install to build the monstache binary. It should now be in $GOPATH/bin create a .go source file for your plugin in the monstache root directory with the package name main implement one or more of the following functions: Map , Filter , Pipeline , Process func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) func Filter(input *monstachemap.MapperPluginInput) (keep bool, err error) func Pipeline(ns string, changeStream bool) (stages []interface, err error) func Process(input*monstachemap.ProcessPluginInput) error Compile your plugin to a .so with go build -buildmode=plugin -o myplugin.so myplugin.go Run the binary, the one you built above with go install (not a release binary), with the following arguments $GOPATH/bin/monstache -mapper-plugin-path /path/to/myplugin.so The following example plugin simply converts top-level string values to uppercase package main import ( \"github.com/rwynn/monstache/monstachemap\" \"strings\" ) // a plugin to convert document values to uppercase func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) { doc := input.Document for k, v := range doc { switch v.(type) { case string: doc[k] = strings.ToUpper(v.(string)) } } output = &monstachemap.MapperPluginOutput{Document: doc} return } The input parameter will contain information about the document's origin database and collection: field meaning Document MongoDB document updated or inserted UpdateDescription If available, the update description Namespace Operation namespace as described above Database MongoDB database from where the event came Collection MongoDB collection where the document was inserted, deleted or updated Operation Which kind of operation triggered this event, see gtm.mapOperation() . \"i\" for insert, \"u\" for update, \"d\" for delete and \"c\" for invalidate. The Map function will only receive inserts and updates. To handle deletes or invalidates implement the Process function described below. Session *mgo.Session . You need not Close the session as monstache will do this automatically when the function exits The output parameter will contain information about how the document should be treated by monstache: field meaning Document an updated document to index into Elasticsearch Index the name of the index to use Type the document type ID override the document ID Routing the routing value to use Drop set to true to indicate that the document should not be indexed but removed Passthrough set to true to indicate the original document should be indexed unchanged Parent the parent id to use Version the version of the document VersionType the version type of the document (internal, external, external_gte) Pipeline the pipeline to index with RetryOnConflict how many times to retry updates before failing Skip set to true to indicate the the document should be ignored For detailed information see monstachemap/plugin.go Few examples are: To skip the document (direct monstache to ignore it) set output.Skip = true . To drop the document (direct monstache not to index it but remove it) set output.Drop = true . To simply pass the original document through to Elasticsearch, set output.Passthrough = true To set custom indexing metadata on the document use output.Index , output.Type , output.Parent and output.Routing . Note If you override output.Index , output.Type , output.Parent or output.Routing for any MongoDB namespaces in a golang plugin you should also add those namespaces to the routing-namespaces array in your config file. This instructs Monstache to query the document metadata so that deletes of the document work correctly. If would like to embed other MongoDB documents (possibly from a different collection) within the current document before indexing, you can access the *mgo.Session pointer as input.Session . With the mgo session you can use the mgo API to find documents in MongoDB and embed them in the Document set on output. When you implement a Filter function the function is called immediately after reading inserts and updates from the oplog. You can return false from this function to completely ignore a document. This is different than setting output.Drop from the mapping function because when you set output.Drop to true, a delete request is issued to Elasticsearch in case the document had previously been indexed. By contrast, returning false from the Filter function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch. When you implement a Pipeline function the function will be called to setup an aggregation pipeline for both direct reads and any change streams that you have configured. The aggregation pipeline stages that you return may be different depending if applied to a direct read or to a change stream. For direct reads the root document will be the document in the collection. For change streams the root document will be a change event with a fullDocument field inside it. Use the boolean parameter changeStream to alter the stages that you return from this function accordingly. When you implement a Process function the function will be called after monstache processes each event. This function has full access to the MongoDB and Elasticsearch clients (including the Elasticsearch bulk processor) in the input and allows you to handle complex event processing scenarios. The input parameter for the Process function will have all the same fields as the input to a Map function described above plus the following: field meaning Document MongoDB document updated, inserted, or deleted ElasticClient A full featured Elasticsearch client ElasticBulkProcessor The same bulk processor monstache uses to index documents. You need only Add requests to the processor and they will be flushed in bulk automatically. Note you must delete the _id field from any argument to the bulk processor Add function Timestamp The MongoDB timestamp of the change event from the oplog. In the case of direct reads the timestamp is the time at which the document was read from MongoDB. Note Under the docker/plugin folder there is a build.sh script to help you build a plugin. There is a README file in that directory with instructions. Javascript \u00b6 Monstache supports plugins written in Javascript. You may find that Javascript plugins give you much less performance than golang plugins. You also may reach some limits of what can be done in the Javascript. This is due to the implementation of the Javascript environment and the locking required under high load. Javascript plugins are still very useful for quick prototypes and small data sets. Transformation \u00b6 Monstache uses the amazing otto library to provide transformation at the document field level in Javascript. You can associate one javascript mapping function per MongoDB collection. You can also associate a function at the global level by not specifying a namespace. These javascript functions are added to your TOML config file, for example: [[script]] namespace = \"mydb.mycollection\" script = \"\"\" var counter = 1; module.exports = function(doc) { doc.foo += \"test\" + counter; counter++; return doc; } \"\"\" [[script]] namespace = \"anotherdb.anothercollection\" path = \"path/to/transform.js\" routing = true [[script]] # this script does not declare a namespace # it is global to all collections script = \"\"\" module.exports = function(doc, ns, updateDesc) { // the doc namespace e.g. test.test is passed as the 2nd arg // if available, an object containing the update description is passed as the 3rd arg return _.omit(doc, \"password\", \"secret\"); } \"\"\" The example TOML above configures 3 scripts. The first is applied to mycollection in mydb while the second is applied to anothercollection in anotherdb . The first script is inlined while the second is loaded from a file path. The path can be absolute or relative to the directory monstache is executed from. The last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are linked to a specific namespace. You will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named script . The javascript assigned to script must assign a function to the exports property of the module object. This function will be passed the document from MongoDB just before it is indexed in Elasticsearch. Inside the function you can manipulate the document to drop fields, add fields, or augment the existing fields. The this reference in the mapping function is assigned to the document from MongoDB. When the return value from the mapping function is an object then that mapped object is what actually gets indexed in Elasticsearch. For these purposes an object is a javascript non-primitive, excluding Function , Array , String , Number , Boolean , Date , Error and RegExp . Filtering \u00b6 You can completely ignore documents by adding filter configurations to your TOML config file. The filter functions are executing immediately after inserts or updates are read from the oplog. The correspding document is passed into the function and you can return true or false to include or ignore the document. [[filter]] namespace = \"db.collection\" script = \"\"\" module.exports = function(doc, ns, updateDesc) { return !!doc.interesting; } \"\"\" [[filter]] namespace = \"db2.collection2\" path = \"path/to/script.js\" Aggregation Pipelines \u00b6 You can alter or filter direct reads and change streams by using a pipeline definition. Note, when building a pipeline for a change stream the root of the document will be the change event and the associated document will be under a field named fullDocument . For more information on the properties of the root document for change streams see Change Events . You can scope a pipeline to a particular namespace using the namespace attribute or leave it off to have the pipeline applied to all namespaces. [[pipeline]] script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: {\"fullDocument.foo\": 1} } ]; } else { return [ { $match: {\"foo\": 1} } ]; } } \"\"\" Warning You should not replace the root using $replaceRoot for a change stream since monstache needs this information. You should only make modifications to the fullDocument field in a pipeline. Dropping \u00b6 If the return value from the mapping function is not an object per the definition above then the result is converted into a boolean and if the boolean value is false then that indicates to monstache that you would not like to index the document. If the boolean value is true then the original document from MongoDB gets indexed in Elasticsearch. This allows you to return false or null if you have implemented soft deletes in MongoDB. [[script]] namespace = \"db.collection\" script = \"\"\" module.exports = function(doc) { if (!!doc.deletedAt) { return false; } return true; } \"\"\" In the above example monstache will index any document except the ones with a deletedAt property. If the document is first inserted without a deletedAt property, but later updated to include the deletedAt property then monstache will remove, or drop, the previously indexed document from the Elasticsearch index. Note Dropping a document is different that filtering a document. A filtered document is completely ignored. A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed. Scripting Features \u00b6 You may have noticed that in the first example above the exported mapping function closes over a var named counter . You can use closures to maintain state between invocations of your mapping function. Finally, since Otto makes it so easy, the venerable Underscore library is included for you at no extra charge. Feel free to abuse the power of the _ . Embedding Documents \u00b6 In your javascript function you have access to the following global functions to retreive documents from MongoDB for embedding in the current document before indexing. Using this approach you can pull in related data. function findId(documentId, [options]) { // convenience method for findOne({_id: documentId}) // returns 1 document or null } function findOne(query, [options]) { // returns 1 document or null } function find(query, [options]) { // returns an array of documents or null } function pipe(stages, [options]) { // returns an array of documents or null } Each function takes a query type object parameter and an optional options object parameter. The options object takes the following keys and values: var options = { database: \"test\", collection: \"test\", // to omit _id set the _id key to 0 in select select: { age: 1 }, // only applicable to find... sort: [\"name\"], limit: 2 } If the database or collection keys are omitted from the options object, the values for database and/or collection are set to the database and collection of the document being processed. Here are some examples: This example sorts the documents in the same collection as the document being processed by name and returns the first 2 documents projecting only the age field. The result is set on the current document before being indexed. [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc) { doc.twoAgesSortedByName = find({}, { sort: [\"name\"], limit: 2, select: { age: 1 } }); return doc; } \"\"\" This example grabs a reference id from a document and replaces it with the corresponding document with that id. [[script]] namespace = \"test.posts\" script = \"\"\" module.exports = function(post) { if (post.author) { // author is a an object id reference post.author = findId(post.author, { database: \"test\", collection: \"users\" }); } return post; } \"\"\" This example runs an aggregation pipeline and stores the results in an extra field in the document [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ] // optional , { database: \"foo\", collection: \"bar\"} // defaults to same namespace ); return doc; } \"\"\" Indexing Metadata \u00b6 You can override the indexing metadata for an individual document by setting a special field named _meta_monstache on the document you return from your Javascript function. The _meta_monstache object supports the following properties. prop meaning routing the routing value index the name of the index to use type the document type parent the document parent version the document version versionType the document version type pipeline the name of a pipeline to apply to the document retryOnConflict control how retry works on conflicts skip set this boolean to true to skip indexing id override the ID used to index the document Assume there is a collection in MongoDB named company in the test database. The documents in this collection look like either { \"_id\": \"london\", \"type\": \"branch\", \"name\": \"London Westminster\", \"city\": \"London\", \"country\": \"UK\" } or { \"_id\": \"alice\", \"type\": \"employee\", \"name\": \"Alice Smith\", \"branch\": \"london\" } Given the above the following snippet sets up a parent-child relationship in Elasticsearch based on the incoming documents from MongoDB and updates the ns (namespace) from test.company to company in Elasticsearch [[script]] namespace = \"test.company\" routing = true script = \"\"\" module.exports = function(doc, ns) { // var meta = { type: doc.type, index: 'company' }; var meta = { type: doc.type, index: ns.split(\".\")[1] }; if (doc.type === \"employee\") { meta.parent = doc.branch; } doc._meta_monstache = meta; return _.omit(doc, \"branch\", \"type\"); } \"\"\" The snippet above will route these documents to the company index in Elasticsearch instead of the default of test.company , if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database db . collection (MongoDB) => collection (Elasticsearch). Also, instead of using company as the Elasticsearch type, the type attribute from the document will be used as the Elasticsearch type. Finally, if the type is employee then the document will be indexed as a child of the branch the person belongs to. We can throw away the type and branch information by deleting it from the document before returning since the type information will be stored in Elasticsearch under _type and the branch information will be stored under _parent . The example is based on the Elasticsearch docs for parent-child For more on updating the namespace name, check the Delete Strategy Routing \u00b6 Routing is the process by which Elasticsearch determines which shard a document will reside in. Monstache supports user defined, or custom, routing of your MongoDB documents into Elasticsearch. Consider an example where you have a comments collection in MongoDB which stores a comment and its associated post identifier. use blog; db.comments.insert({title: \"Did you read this?\", post_id: \"123\"}); db.comments.insert({title: \"Yeah, it's good\", post_id: \"123\"}); In this case monstache will index those 2 documents in an index named blog.comments under the id created by MongoDB. When Elasticsearch routes a document to a shard, by default, it does so by hashing the id of the document. This means that as the number of comments on post 123 grows, each of the comments will be distributed somewhat evenly between the available shards in the cluster. Thus, when a query is performed searching among the comments for post 123 Elasticsearch will need to query all of those shards just in case a comment happened to have been routed there. We can take advantage of the support in Elasticsearch and in monstache to do some intelligent routing such that all comments for post 123 reside in the same shard. First we need to tell monstache that we would like to do custom routing for this collection by setting routing equal to true on a custom script for the namespace. Then we need to add some metadata to the document telling monstache how to route the document when indexing. In this case we want to route by the post_id field. [[script]] namespace = \"blog.comments\" routing = true script = \"\"\" module.exports = function(doc) { doc._meta_monstache = { routing: doc.post_id }; return doc; } \"\"\" Now when monstache indexes document for the collection blog.comments it will set the special _routing attribute for the document on the index request such that Elasticsearch routes comments based on their corresponding post. The _meta_monstache field is used only to inform monstache about routing and is not included in the source document when indexing to Elasticsearch. Now when we are searching for comments and we know the post id that the comment belongs to we can include that post id in the request and make a search that normally queries all shards query only 1 shard. $ curl -H \"Content-Type:application/json\" -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d ' { \"query\":{ \"match_all\":{} } }' You will notice in the response that only 1 shard was queried instead of all your shards. Custom routing is great way to reduce broadcast searches and thus get better performance. The catch with custom routing is that you need to include the routing parameter on all insert, update, and delete operations. Insert and update is not a problem for monstache because the routing information will come from your MongoDB document. Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the information in the oplog is limited to the id of the document that was deleted. But monstache needs to know where the document was originally routed in order to tell Elasticsearch where to look for it. Monstache has 3 available strategies for handling deletes in this situation. The default strategy is stateless and uses a term query into Elasticsearch based on the ID of the document deleted in MongoDB. If the search into Elasticsearch returns exactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires giving monstache the ability to write to the collection monstache.meta . In this collection monstache stores information about documents that were given custom indexing metadata. This stategy slows down indexing and takes up space in MongoDB. However, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and leaves document deletion to the user. If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains documents which have been deleted in MongoDB, this option is available. See Delete Strategy for more information. For more information see Customizing Document Routing In addition to letting your customize the shard routing for a specific document, you can also customize the Elasticsearch index and type using a script by putting the custom information in the meta attribute. [[script]] namespace = \"blog.comments\" routing = true script = \"\"\" module.exports = function(doc) { if (doc.score >= 100) { // NOTE: prefix dynamic index with namespace for proper cleanup on drops doc._meta_monstache = { index: \"blog.comments.highscore\", type: \"highScoreComment\", routing: doc.post_id }; } else { doc._meta_monstache = { routing: doc.post_id }; } return doc; } \"\"\" Joins \u00b6 Elasticsearch 6 introduces an updated approach to parent-child called joins. The following example shows how you can accomplish joins with Monstache. The example is based on the Elasticsearch documentation . This example assumes Monstache is syncing the test.test collection in MongoDB with the test.test index in Elasticsearch. First we will want to setup an index mapping in Elasticsearch describing the join field. curl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d' { \"mappings\": { \"_doc\": { \"properties\": { \"my_join_field\": { \"type\": \"join\", \"relations\": { \"question\": \"answer\" } } } } } } ' Warning The above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new versions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater. Monstache defaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater. If you are using a previous version of Elasticsearch monstache defaults to using the MongoDB collection name as the Elasticsearch type. The type Monstache uses can be overriden but it is not recommended from Elasticsearch 6.2 on. Next will will configure Monstache with custom Javascript middleware that does transformation and routing. In a file called CONFIG.toml. [[script]] namespace = \"test.test\" routing = true script = \"\"\" module.exports = function(doc) { var routing; if (doc.type === \"question\") { routing = doc._id; doc.my_join_field = { name: \"question\" } } else if (doc.type === \"answer\") { routing = doc.question; doc.my_join_field = { name: \"answer\", parent: routing }; } if (routing) { doc._meta_monstache = { routing: routing }; } return doc; } \"\"\" The mapping function adds a my_join_field field to each document. The contents of the field are based on the type attribute in the MongoDB document. Also, the function ensures that the routing is always based on the _id of the question document. Now with this config in place we can start Monstache. We will use verbose to see the requests. monstache -verbose -f CONFIG.toml With Monstache running we are now ready to insert into MongoDB rs:PRIMARY> use test; switched to db test rs:PRIMARY> db.test.insert({type: \"question\", text: \"This is a question\"}); rs:PRIMARY> db.test.find() { \"_id\" : ObjectId(\"5a84a8b826993bde57c12893\"), \"type\" : \"question\", \"text\" : \"This is a question\" } rs:PRIMARY> db.test.insert({type: \"answer\", text: \"This is an answer\", question: ObjectId(\"5a84a8b826993bde57c12893\") }); When we insert these documents we should see Monstache generate the following requests to Elasticsearch {\"index\":{\"_id\":\"5a84a8b826993bde57c12893\",\"_index\":\"test.test\",\"_type\":\"_doc\",\"routing\":\"5a84a8b826993bde57c12893\",\"version\":6522523668566769665,\"version_type\":\"external\"}} {\"my_join_field\":{\"name\":\"question\"},\"text\":\"This is a question\",\"type\":\"question\"} {\"index\":{\"_id\":\"5a84a92b26993bde57c12894\",\"_index\":\"test.test\",\"_type\":\"_doc\",\"routing\":\"5a84a8b826993bde57c12893\",\"version\":6522524162488008705,\"version_type\":\"external\"}} {\"my_join_field\":{\"name\":\"answer\",\"parent\":\"5a84a8b826993bde57c12893\"},\"question\":\"5a84a8b826993bde57c12893\",\"text\":\"This is an answer\",\"type\":\"answer\"} This looks good. We should now have a parent/child relationship between these documents in Elasticsearch. If we do a search on the test.test index we see the following results: \"hits\" : { \"total\" : 2, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"test.test\", \"_type\" : \"_doc\", \"_id\" : \"5a84a8b826993bde57c12893\", \"_score\" : 1.0, \"_routing\" : \"5a84a8b826993bde57c12893\", \"_source\" : { \"my_join_field\" : { \"name\" : \"question\" }, \"text\" : \"This is a question\", \"type\" : \"question\" } }, { \"_index\" : \"test.test\", \"_type\" : \"_doc\", \"_id\" : \"5a84a92b26993bde57c12894\", \"_score\" : 1.0, \"_routing\" : \"5a84a8b826993bde57c12893\", \"_source\" : { \"my_join_field\" : { \"name\" : \"answer\", \"parent\" : \"5a84a8b826993bde57c12893\" }, \"question\" : \"5a84a8b826993bde57c12893\", \"text\" : \"This is an answer\", \"type\" : \"answer\" } } ] } To clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by updating our mapping function. This information needs not be at the top-level since it is duplicated in my_join_field . return _.omit(doc, \"type\", \"question\"); If your parent and child documents are in separate MongoDB collections then you would set up a script for each collection. You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same index. doc._meta_monstache = { routing: routing, index: \"parentsAndChildren\" }; Warning You must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index that the document _ids across the MongoDB collections do not collide for any 2 docs because they will be used as the _id in the target index. Time Machines \u00b6 If you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use time machine namespaces . For example, you've inserted and later updated a document with id 123 in the test.test collection in MongoDB. If test.test is a time machine namespace you will have 2 documents representing those changes in the log.test.test.2018-02-20 index (timestamp will change) in Elasticsearch. If you later want all the changes made to that document in MongoDB you can issue a query like this: $ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d ' { \"query\":{ \"sort\" : [ { \"_oplog_ts\" : {\"order\" : \"desc\"}} ], \"filtered\":{ \"query\":{ \"match_all\":{} }, \"filter\":{ \"term\":{ \"_source_id\":\"123\" } } } } }' That query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123. It filters the documents on that shard by _source_id , or id from MongoDB, to only give us the changes to that document. Finally, it sorts by the _oplog_ts which gives us the most recent change docs first. The index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the test.test namespace. Merge Patches \u00b6 A unique feature of monstache is support for JSON Merge Patches rfc-7396 . If merge patches are enabled monstache will add an additional field to documents indexed into Elasticsearch. The name of this field is configurable but it defaults to json-merge-patches . Consider the following example with merge patches enabled... db.test.insert({name: \"Joe\", age: 16, friends: [1, 2, 3]}) At this point you would have the following document source in Elasticsearch. \"_source\" : { \"age\" : 16, \"friends\" : [ 1, 2, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 } ], \"name\" : \"Joe\" } As you can see we have a single timestamped merge patch in the json-merge-patches array. Now let's update the document to remove a friend and update the age. db.test.update({name: \"Joe\"}, {$set: {age: 21, friends: [1, 3]}}) If we now look at the document in Elasticsearch we see the following: \"_source\" : { \"age\" : 21, \"friends\" : [ 1, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 }, { \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\", \"ts\" : 1487263746, \"v\" : 2 } ], \"name\" : \"Joe\" } You can see that the document was updated as expected and an additional merge patch was added. Each time the document is updated in MongoDB the corresponding document in Elasticsearch gains a timestamped merge patch. Using this information we can time travel is the document's history. There is a merge patch for each version of the document. To recreate a specific version we simply need to apply the merge patches in order up to the version that we want. To get version 1 of the document above we start with {} and apply the 1st merge patch. To get version 2 of the document above we start with {} apply the 1st merge patch to get v1 apply the 2nd merge patch to v1 to get v2 The timestamps associated with these merge patches are in seconds since the epoch, taken from the timestamp recorded in the oplog when the insert or update occured. To enable the merge patches feature in monstache you need to add the following to you TOML config: enable-patches = true patch-namespaces = [\"test.test\"] You need you add each namespace that you would like to see patches for in the patch-namespaces array. Optionally, you can change the key under which the patches are stored in the source document as follows: merge-patch-attribute = \"custom-merge-attr\" Merge patches will only be recorded for data read from the MongoDB oplog. Data read using the direct read feature will not be enhanced with merge patches. Most likely, you will want to turn off indexing for the merge patch attribute. You can do this by creating an index template for each patch namespace before running monstache... PUT /_template/test.test { \"template\" : \"test.test\", \"mappings\" : { \"test\" : { \"json-merge-patches\" : { \"index\" : false } } } } Systemd \u00b6 Monstache has support built in for integrating with systemd. The following monstache.service is an example systemd configuration. [Unit] Description=monstache sync service [Service] Type=notify ExecStart=/usr/local/bin/monstache -f /etc/monstache/config.toml WatchdogSec=30s Restart=always [Install] WantedBy=multi-user.target Systemd unit files are normally saved to /lib/systemd/system . Verify same with your OS documentation. After saving the monstache.service file you can run systemctl daemon-reload to tell systemd to reload all unit files. You can enable the service to start on boot with systemctl enable monstache.service and start the service with systemctl start monstache.service . With the configuration above monstache will notify systemd when it has started successfully and then notify systemd repeatedly at half the WatchDog interval to signal liveness. The configuration above causes systemd to restart monstache if it does not start or respond within the WatchdDog interval. Docker \u00b6 There are Docker images available for Monstache on Docker Hub You can pull and run the latest images with docker run rwynn/monstache:rel6 -v docker run rwynn/monstache:rel5 -v You can pull and run release images with docker run rwynn/monstache:6.7.4 -v docker run rwynn/monstache:5.7.4 -v For example, to run monstache via Docker with a golang plugin that resides at ~/plugin/plugin.so on the host you can use a bind mount docker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:6.7.4 -mapper-plugin-path /tmp/plugin/plugin.so HTTP Server \u00b6 Monstache has a built in HTTP server that you can enable with --enable-http-server. It listens on :8080 by default but you can change this with --http-server-addr. When using monstache with kubernetes this server can be used to detect liveness and act accordingly The following GET endpoints are available /started \u00b6 Returns the uptime of the server /healthz \u00b6 Returns at 200 status code with the text \"ok\" when monstache is running /stats \u00b6 Returns the current indexing statistics in JSON format. Only available if stats are enabled /instance \u00b6 Returns information about the running monstache process including whether or not it is currently enabled (a cluster will have one enabled process) and the most recent change event timestamp read from MongoDB. /debug (if pprof is enabled) \u00b6 If the pprof setting is enabled the following endpoints are also made available: /debug/pprof/ /debug/pprof/cmdline /debug/pprof/profile /debug/pprof/symbol /debug/pprof/trace MongoDB Authentication \u00b6 Check the following link for all available options that you can specify in the MongoDB connection string related to authentication. For more information on how the MongoDB driver processes authentication configuration see the driver docs . AWS Signature Version 4 \u00b6 Monstache has included AWS Signature Version 4 request signing. To enable the AWS Signature Version 4 support add the following to your config file: elasticsearch-urls = [\"https://<endpoint_from_aws_overview_screen>:443\"] [aws-connect] access-key = \"XXX\" secret-key = \"YYY\" region = \"ZZZ\" See the docs for aws-connect for the different stategies available for configuring a credential provider. Notice how the elasticsearch-url references the port number 443 in the connection string. This is because AWS makes your cluster available on the standard https port and not the default Elasticsearch port of 9200 . If you have connection problems make sure you are using the correct port. You cannot omit the port because the driver will default to 9200 if a port is not specified. You can read more about Signature Version 4 and Amazon Elasticsearch Service . For information on how to obtain the access-key and secret-key required to connect you can read this blog post . In short, you will need to create or use an existing IAM user in your AWS account. You will then need to give this user access to your Elasticsearch domain. The access-key and secret-key you put in your configuration file are those associated with the IAM user. { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:user/<iam-user-name>\" }, \"Action\": \"es:*\", \"Resource\": \"arn:aws:es:us-east-1:<account-id>:domain/<elasticsearch-domain-name>/*\" } Watching changes on specific fields only \u00b6 If you are using MongoDB 3.6+ you can use a change stream pipeline to only listen for change events on specific fields. For example, if you wanted to listen for create , delete , and update events on the namespace test.test , but you only wanted to sync changes when the foo or bar field changed on the doc, you could use the following configuration. If, for example, a field named count changed on the document, then this change would be ignored by monstache. change-stream-namespaces = [\"test.test\"] [[pipeline]] namespace = \"test.test\" script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { $or: [ { \"updateDescription\": {$exists: false} }, { \"updateDescription.updatedFields.foo\": {$exists: true}}, { \"updateDescription.updatedFields.bar\": {$exists: true}} ] } } ]; } else { return []; } } \"\"\" To build complicated change stream pipelines see Change Events for information on the structure of change events. This information will shape your pipeline. MongoDB view replication \u00b6 You may have a situation where you want to replicate a MongoDB view in Elasticsearch. Or you have a collection that should trigger sync of another collection. You can use the relate config to do this. Consider you have a collections thing and state . A thing has an associated state and a thing is linked to a state via a field s which points to the _id of the associated state in the state collection. You can create a view in MongoDB that uses a $lookup to pull the state information in and present a view of things with the state information included. use thingdb; db.createView(\"thingview\", \"thing\", [ {$lookup: {from: \"state\", localField: \"s\", foreignField: \"_id\", as: \"s\"}}]) Given this view you can use the following config to keep things up to date in a things index in Elasticsearch. direct-read-namespaces = [\"thingdb.thingview\"] # read direct from the view of the collection to seed index change-stream-namespaces = [\"thingdb.thing\", \"thingdb.state\"] # change events happen on the underlying collections not views [[mapping]] namespace = \"thingdb.thing\" # map change events on the thing collection to the things index index = \"things\" [[mapping]] namespace = \"thingdb.thingview\" # map direct reads of the thingview to the same things index index = \"things\" [[relate]] namespace = \"thingdb.thing\" # when a thing changes look it up in the assoicated view by _id and index that with-namespace = \"thingdb.thingview\" keep-src = false # ignore the original thing that changed and instead just use the lookup of that thing via the view [[relate]] namespace = \"thingdb.state\" # when a state changes trigger a thing change event since thing is associated to a state with-namespace = \"thingdb.thing\" src-field = \"_id\" # use the _id field of the state that changed to lookup associated things match-field = \"s\" # only trigger change events for the things where thing.s (match-field) = state._id (src-field). keep-src = false Warning Be careful of the expense of using relate with a view. In the example above, if there were many things associated to a single state then a change to that state would trigger n+1 queries to MongoDB when n is the number of things related to the state. 1 query would be used to find all associated things and n queries would be used to lookup each thing in the view. Amazon DocumentDB (with MongoDB compatibility) \u00b6 Monstache support for Amazon DocumentDB is currently experimental. Support for the change streams API in MongoDB was recently added to Amazon DocumentDB. Consult the DocumentDB documentation for instructions on enabling change streams for your collections. Since Amazon DocumentDB only supports compatibility with MongoDB API 3.6 you will want to ensure that your change stream configuration targets collections and that your resume strategy is set to use tokens and not the default of timestamps. Ensure that your MongoDB connection URI is set with a primary read preference: e.g. ?readPreference=primary . # ensure you target collections in your change stream namespaces change-stream-namespaces = [\"db1.col1\", \"db2.col2\"] # ensure that resuming, if enabled, is done based on tokens and not timestamps resume = true resume-strategy = 1","title":"Advanced"},{"location":"advanced/#advanced","text":"","title":"Advanced"},{"location":"advanced/#versions","text":"Monstache version Git branch (used to build plugin) Docker tag Description Elasticsearch MongoDB Status 6 rel6 rel6, latest MongoDB, Inc. go driver Version 7+ Version 2.6+ Supported 5 rel5 rel5 MongoDB, Inc. go driver Version 6 Version 2.6+ Supported 4 master rel4 mgo community go driver Version 6 Version 3 Deprecated 3 rel3 rel3 mgo community go driver Versions 2 and 5 Version 3 Deprecated Note You can use monstache rel5 and rel6 with MongoDB versions back to 2.6 with the following caveats. If you have MongoDB 3.6 then you must explicitly enumerate collections in your change-stream-namespaces setting because change streams against databases and entire deployments was not introduced until MongoDB version 4.0. Alternatively, you can disable change events entirely with disable-change-events . You must also set resume-strategy to 1 to use a token-based resume strategy compatibile with MongoDB API 3.6. If you have MongoDB 2.6 - 3.5 then you must omit any mention of change-stream-namespaces in your config file because change streams were first introduced in 3.6. To emulate change events you must turn on the option enable-oplog . Alternatively, you can disable change events entirely with disable-change-events . Warning Your MongoDB binary version does not always mean that the feature compatibility is at that same level. Check your feature compatibility version from the MongoDB console to ensure that MongoDB is not operating in a lesser capability mode. This sometimes happens when MongoDB is upgraded in place or MongoDB is started with a data directory of a previous installation. Sometimes there are reasons to stay at a lower feature compatibility so check before you upgrade it.","title":"Versions"},{"location":"advanced/#gridfs-support","text":"Monstache supports indexing the raw content of files stored in GridFS into Elasticsearch for full text search. This feature requires that you install an Elasticsearch plugin which enables the field type attachment . For versions of Elasticsearch prior to version 5 you should install the mapper-attachments plugin. For version 5 or later of Elasticsearch you should instead install the ingest-attachment plugin. Once you have installed the appropriate plugin for Elasticsearch, getting file content from GridFS into Elasticsearch is as simple as configuring monstache. You will want to enable the index-files option and also tell monstache the namespace of all collections which will hold GridFS files. For example in your TOML config file, index-files = true direct-read-namespaces = [\"users.fs.files\", \"posts.fs.files\"] file-namespaces = [\"users.fs.files\", \"posts.fs.files\"] file-highlighting = true The above configuration tells monstache that you wish to index the raw content of GridFS files in the users and posts MongoDB databases. By default, MongoDB uses a bucket named fs , so if you just use the defaults your collection name will be fs.files . However, if you have customized the bucket name, then your file collection would be something like mybucket.files and the entire namespace would be users.mybucket.files . When you configure monstache this way it will perform an additional operation at startup to ensure the destination indexes in Elasticsearch have a field named file with a type mapping of attachment . For the example TOML configuration above, monstache would initialize 2 indices in preparation for indexing into Elasticsearch by issuing the following REST commands: For Elasticsearch versions prior to version 5... POST /users.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} POST /posts.fs.files { \"mappings\": { \"fs.files\": { \"properties\": { \"file\": { \"type\": \"attachment\" } }}}} For Elasticsearch version 5 and above... PUT /_ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\" } } ] } When a file is inserted into MongoDB via GridFS, monstache will detect the new file, use the MongoDB api to retrieve the raw content, and index a document into Elasticsearch with the raw content stored in a file field as a base64 encoded string. The Elasticsearch plugin will then extract text content from the raw content using Apache Tika , tokenize the text content, and allow you to query on the content of the file. To test this feature of monstache you can simply use the mongofiles command to quickly add a file to MongoDB via GridFS. Continuing the example above one could issue the following command to put a file named resume.docx into GridFS and after a short time this file should be searchable in Elasticsearch in the index users.fs.files . mongofiles -d users put resume.docx After a short time you should be able to query the contents of resume.docx in the users index in Elasticsearch curl -XGET \"http://localhost:9200/users.fs.files/_search?q=golang\" If you would like to see the text extracted by Apache Tika you can project the appropriate sub-field For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [ \"file.content\" ], \"query\": { \"match\": { \"file.content\": \"golang\" } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [ \"attachment.content\" ], \"query\": { \"match\": { \"attachment.content\": \"golang\" } } }' When file-highlighting is enabled you can add a highlight clause to your query For Elasticsearch versions prior to version 5... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"fields\": [\"file.content\"], \"query\": { \"match\": { \"file.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"file.content\": { } } } }' For Elasticsearch version 5 and above... curl -H \"Content-Type:application/json\" localhost:9200/users.fs.files/_search?pretty -d '{ \"_source\": [\"attachment.content\"], \"query\": { \"match\": { \"attachment.content\": \"golang\" } }, \"highlight\": { \"fields\": { \"attachment.content\": { } } } }' The highlight response will contain emphasis on the matching terms For Elasticsearch versions prior to version 5... \"hits\" : [ { \"highlight\" : { \"file.content\" : [ \"I like to program in <em>golang</em>.\\n\\n\" ] } } ] For Elasticsearch version 5 and above... \"hits\" : [{ \"highlight\" : { \"attachment.content\" : [ \"I like to program in <em>golang</em>.\" ] } }]","title":"GridFS Support"},{"location":"advanced/#workers","text":"You can run multiple monstache processes and distribute the work between them. First configure the names of all the workers in a shared config.toml file. workers = [\"Tom\", \"Dick\", \"Harry\"] In this case we have 3 workers. Now we can start 3 monstache processes and give each one of the worker names. monstache -f config.toml -worker Tom monstache -f config.toml -worker Dick monstache -f config.toml -worker Harry monstache will hash the id of each document using consistent hashing so that each id is handled by only one of the available workers.","title":"Workers"},{"location":"advanced/#high-availability","text":"You can run monstache in high availability mode by starting multiple processes with the same value for cluster-name . Each process will join a cluster which works together to ensure that a monstache process is always syncing to Elasticsearch. High availability works by ensuring one active process in the monstache.cluster collection in MongoDB at any given time. Only the process in this collection will be syncing for the cluster. Processes not present in this collection will be paused. Documents in the monstache.cluster collection have a TTL assigned to them. When a document in this collection times out it will be removed from the collection by MongoDB and another process in the monstache cluster will have a chance to write to the collection and become the new active process. When cluster-name is supplied the resume feature is automatically turned on and the resume-name becomes the name of the cluster. This is to ensure that each of the processes is able to pick up syncing where the last one left off. You can combine the HA feature with the workers feature. For 3 cluster nodes with 3 workers per node you would have something like the following: // config.toml workers = [\"Tom\", \"Dick\", \"Harry\"] // on host A monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host B monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml // on host C monstache -cluster-name HA -worker Tom -f config.toml monstache -cluster-name HA -worker Dick -f config.toml monstache -cluster-name HA -worker Harry -f config.toml When the clustering feature is combined with workers then the resume-name becomes the cluster name concatenated with the worker name.","title":"High Availability"},{"location":"advanced/#index-mapping","text":"When indexing documents from MongoDB into Elasticsearch the default mapping is as follows: For Elasticsearch prior to 6.2 Elasticsearch index name <= MongoDB database name . MongoDB collection name Elasticsearch type <= MongoDB collection name Elasticsearch document _id <= MongoDB document _id For Elasticsearch 6.2+ Elasticsearch index name <= MongoDB database name . MongoDB collection name Elasticsearch type <= _doc Elasticsearch document _id <= MongoDB document _id If these default won't work for some reason you can override the index and type mapping on a per collection basis by adding the following to your TOML config file: [[mapping]] namespace = \"test.test\" index = \"index1\" type = \"type1\" [[mapping]] namespace = \"test.test2\" index = \"index2\" type = \"type2\" With the configuration above documents in the test.test namespace in MongoDB are indexed into the index1 index in Elasticsearch with the type1 type. If you need your index and type mapping to be more dynamic, such as based on values inside the MongoDB document, then see the sections Middleware and Routing . Warning It is not recommended to override the default type of _doc if using Elasticsearch 6.2+ since this will be the supported path going forward. Also, using _doc as the type will not work with Elasticsearch prior to 6.2. Make sure that automatic index creation is not disabled in elasticsearch.yml or create your target indexes before using Monstache. If automatic index creation must be controlled, whitelist any indexes in elasticsearch.yml that monstache will create.","title":"Index Mapping"},{"location":"advanced/#namespaces","text":"When a document is inserted, updated, or deleted in MongoDB a document is appended to the oplog representing the event. This document has a field ns which is the namespace. For inserts, updates, and deletes the namespace is the database name and collection name of the document changed joined by a dot. E.g. for use test; db.foo.insert({hello: \"world\"}); the namespace for the event in the oplog would be test.foo . In addition to inserts, updates, and deletes monstache also supports database and collection drops. When a database or collection is dropped in MongoDB an event is appended to the oplog. Like the other types of changes this event has a field ns representing the namespace. However, for drops the namespace is the database name and the string $cmd joined by a dot. E.g. for use test; db.foo.drop() the namespace for the event in the oplog would be test.$cmd .","title":"Namespaces"},{"location":"advanced/#middleware","text":"monstache supports embedding user defined middleware between MongoDB and Elasticsearch. Middleware is able to transform documents, drop documents, or define indexing metadata. Middleware may be written in either Javascript or in Golang as a plugin. Warning It is HIGHLY recommended to use a golang plugin in production over a javascript plugin due to performance differences. Currently, golang plugins are orders of magnitude faster than javascript plugins. This is due to concurrency and the need to perform locking on the javascript environment. Javascript plugins are very useful for quickly prototyping a solution, however at some point it is recommended to convert them to golang plugins. If you enable a Golang plugin then monstache will ignore an javascript middleware in your configuration. This may change in the future but for now the choice of middleware language is mutually exclusive.","title":"Middleware"},{"location":"advanced/#golang","text":"monstache supports golang plugins. You should have golang version 1.11 or greater installed and will need to perform the build on the Linux or OSX platform. Golang plugins are not currently supported on the Windows platform due to limits in golang. To implement a plugin for monstache you need to implement specific function signatures, use the go command to build a .so file for your plugin, and finally pass the path to your plugin .so file when running monstache. See this wiki page for an example using Docker. Warning Golang plugins must be built with the exact same source code (including dependencies) of the loading program. If you don't build your plugin this way then monstache may fail to load it at runtime due to source code mismatches. To create a golang plugin for monstache git clone monstache somewhere outside your $GOPATH git checkout a specific monstache version tag (e.g. v6.7.4 ). See Versions above. in the monstache root directory run go install to build the monstache binary. It should now be in $GOPATH/bin create a .go source file for your plugin in the monstache root directory with the package name main implement one or more of the following functions: Map , Filter , Pipeline , Process func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) func Filter(input *monstachemap.MapperPluginInput) (keep bool, err error) func Pipeline(ns string, changeStream bool) (stages []interface, err error) func Process(input*monstachemap.ProcessPluginInput) error Compile your plugin to a .so with go build -buildmode=plugin -o myplugin.so myplugin.go Run the binary, the one you built above with go install (not a release binary), with the following arguments $GOPATH/bin/monstache -mapper-plugin-path /path/to/myplugin.so The following example plugin simply converts top-level string values to uppercase package main import ( \"github.com/rwynn/monstache/monstachemap\" \"strings\" ) // a plugin to convert document values to uppercase func Map(input *monstachemap.MapperPluginInput) (output *monstachemap.MapperPluginOutput, err error) { doc := input.Document for k, v := range doc { switch v.(type) { case string: doc[k] = strings.ToUpper(v.(string)) } } output = &monstachemap.MapperPluginOutput{Document: doc} return } The input parameter will contain information about the document's origin database and collection: field meaning Document MongoDB document updated or inserted UpdateDescription If available, the update description Namespace Operation namespace as described above Database MongoDB database from where the event came Collection MongoDB collection where the document was inserted, deleted or updated Operation Which kind of operation triggered this event, see gtm.mapOperation() . \"i\" for insert, \"u\" for update, \"d\" for delete and \"c\" for invalidate. The Map function will only receive inserts and updates. To handle deletes or invalidates implement the Process function described below. Session *mgo.Session . You need not Close the session as monstache will do this automatically when the function exits The output parameter will contain information about how the document should be treated by monstache: field meaning Document an updated document to index into Elasticsearch Index the name of the index to use Type the document type ID override the document ID Routing the routing value to use Drop set to true to indicate that the document should not be indexed but removed Passthrough set to true to indicate the original document should be indexed unchanged Parent the parent id to use Version the version of the document VersionType the version type of the document (internal, external, external_gte) Pipeline the pipeline to index with RetryOnConflict how many times to retry updates before failing Skip set to true to indicate the the document should be ignored For detailed information see monstachemap/plugin.go Few examples are: To skip the document (direct monstache to ignore it) set output.Skip = true . To drop the document (direct monstache not to index it but remove it) set output.Drop = true . To simply pass the original document through to Elasticsearch, set output.Passthrough = true To set custom indexing metadata on the document use output.Index , output.Type , output.Parent and output.Routing . Note If you override output.Index , output.Type , output.Parent or output.Routing for any MongoDB namespaces in a golang plugin you should also add those namespaces to the routing-namespaces array in your config file. This instructs Monstache to query the document metadata so that deletes of the document work correctly. If would like to embed other MongoDB documents (possibly from a different collection) within the current document before indexing, you can access the *mgo.Session pointer as input.Session . With the mgo session you can use the mgo API to find documents in MongoDB and embed them in the Document set on output. When you implement a Filter function the function is called immediately after reading inserts and updates from the oplog. You can return false from this function to completely ignore a document. This is different than setting output.Drop from the mapping function because when you set output.Drop to true, a delete request is issued to Elasticsearch in case the document had previously been indexed. By contrast, returning false from the Filter function causes the operation to be completely ignored and there is no corresponding delete request issued to Elasticsearch. When you implement a Pipeline function the function will be called to setup an aggregation pipeline for both direct reads and any change streams that you have configured. The aggregation pipeline stages that you return may be different depending if applied to a direct read or to a change stream. For direct reads the root document will be the document in the collection. For change streams the root document will be a change event with a fullDocument field inside it. Use the boolean parameter changeStream to alter the stages that you return from this function accordingly. When you implement a Process function the function will be called after monstache processes each event. This function has full access to the MongoDB and Elasticsearch clients (including the Elasticsearch bulk processor) in the input and allows you to handle complex event processing scenarios. The input parameter for the Process function will have all the same fields as the input to a Map function described above plus the following: field meaning Document MongoDB document updated, inserted, or deleted ElasticClient A full featured Elasticsearch client ElasticBulkProcessor The same bulk processor monstache uses to index documents. You need only Add requests to the processor and they will be flushed in bulk automatically. Note you must delete the _id field from any argument to the bulk processor Add function Timestamp The MongoDB timestamp of the change event from the oplog. In the case of direct reads the timestamp is the time at which the document was read from MongoDB. Note Under the docker/plugin folder there is a build.sh script to help you build a plugin. There is a README file in that directory with instructions.","title":"Golang"},{"location":"advanced/#javascript","text":"Monstache supports plugins written in Javascript. You may find that Javascript plugins give you much less performance than golang plugins. You also may reach some limits of what can be done in the Javascript. This is due to the implementation of the Javascript environment and the locking required under high load. Javascript plugins are still very useful for quick prototypes and small data sets.","title":"Javascript"},{"location":"advanced/#transformation","text":"Monstache uses the amazing otto library to provide transformation at the document field level in Javascript. You can associate one javascript mapping function per MongoDB collection. You can also associate a function at the global level by not specifying a namespace. These javascript functions are added to your TOML config file, for example: [[script]] namespace = \"mydb.mycollection\" script = \"\"\" var counter = 1; module.exports = function(doc) { doc.foo += \"test\" + counter; counter++; return doc; } \"\"\" [[script]] namespace = \"anotherdb.anothercollection\" path = \"path/to/transform.js\" routing = true [[script]] # this script does not declare a namespace # it is global to all collections script = \"\"\" module.exports = function(doc, ns, updateDesc) { // the doc namespace e.g. test.test is passed as the 2nd arg // if available, an object containing the update description is passed as the 3rd arg return _.omit(doc, \"password\", \"secret\"); } \"\"\" The example TOML above configures 3 scripts. The first is applied to mycollection in mydb while the second is applied to anothercollection in anotherdb . The first script is inlined while the second is loaded from a file path. The path can be absolute or relative to the directory monstache is executed from. The last script does not specify a namespace, so documents from all collections pass through it. Global scripts are run before scripts which are linked to a specific namespace. You will notice that the multi-line string feature of TOML is used to assign a javascript snippet to the variable named script . The javascript assigned to script must assign a function to the exports property of the module object. This function will be passed the document from MongoDB just before it is indexed in Elasticsearch. Inside the function you can manipulate the document to drop fields, add fields, or augment the existing fields. The this reference in the mapping function is assigned to the document from MongoDB. When the return value from the mapping function is an object then that mapped object is what actually gets indexed in Elasticsearch. For these purposes an object is a javascript non-primitive, excluding Function , Array , String , Number , Boolean , Date , Error and RegExp .","title":"Transformation"},{"location":"advanced/#filtering","text":"You can completely ignore documents by adding filter configurations to your TOML config file. The filter functions are executing immediately after inserts or updates are read from the oplog. The correspding document is passed into the function and you can return true or false to include or ignore the document. [[filter]] namespace = \"db.collection\" script = \"\"\" module.exports = function(doc, ns, updateDesc) { return !!doc.interesting; } \"\"\" [[filter]] namespace = \"db2.collection2\" path = \"path/to/script.js\"","title":"Filtering"},{"location":"advanced/#aggregation-pipelines","text":"You can alter or filter direct reads and change streams by using a pipeline definition. Note, when building a pipeline for a change stream the root of the document will be the change event and the associated document will be under a field named fullDocument . For more information on the properties of the root document for change streams see Change Events . You can scope a pipeline to a particular namespace using the namespace attribute or leave it off to have the pipeline applied to all namespaces. [[pipeline]] script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: {\"fullDocument.foo\": 1} } ]; } else { return [ { $match: {\"foo\": 1} } ]; } } \"\"\" Warning You should not replace the root using $replaceRoot for a change stream since monstache needs this information. You should only make modifications to the fullDocument field in a pipeline.","title":"Aggregation Pipelines"},{"location":"advanced/#dropping","text":"If the return value from the mapping function is not an object per the definition above then the result is converted into a boolean and if the boolean value is false then that indicates to monstache that you would not like to index the document. If the boolean value is true then the original document from MongoDB gets indexed in Elasticsearch. This allows you to return false or null if you have implemented soft deletes in MongoDB. [[script]] namespace = \"db.collection\" script = \"\"\" module.exports = function(doc) { if (!!doc.deletedAt) { return false; } return true; } \"\"\" In the above example monstache will index any document except the ones with a deletedAt property. If the document is first inserted without a deletedAt property, but later updated to include the deletedAt property then monstache will remove, or drop, the previously indexed document from the Elasticsearch index. Note Dropping a document is different that filtering a document. A filtered document is completely ignored. A dropped document results in a delete request being issued to Elasticsearch in case the document had previously been indexed.","title":"Dropping"},{"location":"advanced/#scripting-features","text":"You may have noticed that in the first example above the exported mapping function closes over a var named counter . You can use closures to maintain state between invocations of your mapping function. Finally, since Otto makes it so easy, the venerable Underscore library is included for you at no extra charge. Feel free to abuse the power of the _ .","title":"Scripting Features"},{"location":"advanced/#embedding-documents","text":"In your javascript function you have access to the following global functions to retreive documents from MongoDB for embedding in the current document before indexing. Using this approach you can pull in related data. function findId(documentId, [options]) { // convenience method for findOne({_id: documentId}) // returns 1 document or null } function findOne(query, [options]) { // returns 1 document or null } function find(query, [options]) { // returns an array of documents or null } function pipe(stages, [options]) { // returns an array of documents or null } Each function takes a query type object parameter and an optional options object parameter. The options object takes the following keys and values: var options = { database: \"test\", collection: \"test\", // to omit _id set the _id key to 0 in select select: { age: 1 }, // only applicable to find... sort: [\"name\"], limit: 2 } If the database or collection keys are omitted from the options object, the values for database and/or collection are set to the database and collection of the document being processed. Here are some examples: This example sorts the documents in the same collection as the document being processed by name and returns the first 2 documents projecting only the age field. The result is set on the current document before being indexed. [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc) { doc.twoAgesSortedByName = find({}, { sort: [\"name\"], limit: 2, select: { age: 1 } }); return doc; } \"\"\" This example grabs a reference id from a document and replaces it with the corresponding document with that id. [[script]] namespace = \"test.posts\" script = \"\"\" module.exports = function(post) { if (post.author) { // author is a an object id reference post.author = findId(post.author, { database: \"test\", collection: \"users\" }); } return post; } \"\"\" This example runs an aggregation pipeline and stores the results in an extra field in the document [[script]] namespace = \"test.test\" script = \"\"\" module.exports = function(doc, ns) { doc.extra = pipe([ { $match: {foo: 1} }, { $limit: 1 }, { $project: { _id: 0, foo: 1}} ] // optional , { database: \"foo\", collection: \"bar\"} // defaults to same namespace ); return doc; } \"\"\"","title":"Embedding Documents"},{"location":"advanced/#indexing-metadata","text":"You can override the indexing metadata for an individual document by setting a special field named _meta_monstache on the document you return from your Javascript function. The _meta_monstache object supports the following properties. prop meaning routing the routing value index the name of the index to use type the document type parent the document parent version the document version versionType the document version type pipeline the name of a pipeline to apply to the document retryOnConflict control how retry works on conflicts skip set this boolean to true to skip indexing id override the ID used to index the document Assume there is a collection in MongoDB named company in the test database. The documents in this collection look like either { \"_id\": \"london\", \"type\": \"branch\", \"name\": \"London Westminster\", \"city\": \"London\", \"country\": \"UK\" } or { \"_id\": \"alice\", \"type\": \"employee\", \"name\": \"Alice Smith\", \"branch\": \"london\" } Given the above the following snippet sets up a parent-child relationship in Elasticsearch based on the incoming documents from MongoDB and updates the ns (namespace) from test.company to company in Elasticsearch [[script]] namespace = \"test.company\" routing = true script = \"\"\" module.exports = function(doc, ns) { // var meta = { type: doc.type, index: 'company' }; var meta = { type: doc.type, index: ns.split(\".\")[1] }; if (doc.type === \"employee\") { meta.parent = doc.branch; } doc._meta_monstache = meta; return _.omit(doc, \"branch\", \"type\"); } \"\"\" The snippet above will route these documents to the company index in Elasticsearch instead of the default of test.company , if you didn't specify a namespace, it'll route all documents to indexes named as the collection only without the database db . collection (MongoDB) => collection (Elasticsearch). Also, instead of using company as the Elasticsearch type, the type attribute from the document will be used as the Elasticsearch type. Finally, if the type is employee then the document will be indexed as a child of the branch the person belongs to. We can throw away the type and branch information by deleting it from the document before returning since the type information will be stored in Elasticsearch under _type and the branch information will be stored under _parent . The example is based on the Elasticsearch docs for parent-child For more on updating the namespace name, check the Delete Strategy","title":"Indexing Metadata"},{"location":"advanced/#routing","text":"Routing is the process by which Elasticsearch determines which shard a document will reside in. Monstache supports user defined, or custom, routing of your MongoDB documents into Elasticsearch. Consider an example where you have a comments collection in MongoDB which stores a comment and its associated post identifier. use blog; db.comments.insert({title: \"Did you read this?\", post_id: \"123\"}); db.comments.insert({title: \"Yeah, it's good\", post_id: \"123\"}); In this case monstache will index those 2 documents in an index named blog.comments under the id created by MongoDB. When Elasticsearch routes a document to a shard, by default, it does so by hashing the id of the document. This means that as the number of comments on post 123 grows, each of the comments will be distributed somewhat evenly between the available shards in the cluster. Thus, when a query is performed searching among the comments for post 123 Elasticsearch will need to query all of those shards just in case a comment happened to have been routed there. We can take advantage of the support in Elasticsearch and in monstache to do some intelligent routing such that all comments for post 123 reside in the same shard. First we need to tell monstache that we would like to do custom routing for this collection by setting routing equal to true on a custom script for the namespace. Then we need to add some metadata to the document telling monstache how to route the document when indexing. In this case we want to route by the post_id field. [[script]] namespace = \"blog.comments\" routing = true script = \"\"\" module.exports = function(doc) { doc._meta_monstache = { routing: doc.post_id }; return doc; } \"\"\" Now when monstache indexes document for the collection blog.comments it will set the special _routing attribute for the document on the index request such that Elasticsearch routes comments based on their corresponding post. The _meta_monstache field is used only to inform monstache about routing and is not included in the source document when indexing to Elasticsearch. Now when we are searching for comments and we know the post id that the comment belongs to we can include that post id in the request and make a search that normally queries all shards query only 1 shard. $ curl -H \"Content-Type:application/json\" -XGET 'http://localhost:9200/blog.comments/_search?routing=123' -d ' { \"query\":{ \"match_all\":{} } }' You will notice in the response that only 1 shard was queried instead of all your shards. Custom routing is great way to reduce broadcast searches and thus get better performance. The catch with custom routing is that you need to include the routing parameter on all insert, update, and delete operations. Insert and update is not a problem for monstache because the routing information will come from your MongoDB document. Deletes, however, pose a problem for monstache because when a delete occurs in MongoDB the information in the oplog is limited to the id of the document that was deleted. But monstache needs to know where the document was originally routed in order to tell Elasticsearch where to look for it. Monstache has 3 available strategies for handling deletes in this situation. The default strategy is stateless and uses a term query into Elasticsearch based on the ID of the document deleted in MongoDB. If the search into Elasticsearch returns exactly 1 document then monstache will schedule that document for deletion. The 2nd stategy monstache uses is stateful and requires giving monstache the ability to write to the collection monstache.meta . In this collection monstache stores information about documents that were given custom indexing metadata. This stategy slows down indexing and takes up space in MongoDB. However, it is precise because it records exactly how each document was indexed. The final stategy simply punts on deletes and leaves document deletion to the user. If you don't generally delete documents in MongoDB or don't care if Elasticsearch contains documents which have been deleted in MongoDB, this option is available. See Delete Strategy for more information. For more information see Customizing Document Routing In addition to letting your customize the shard routing for a specific document, you can also customize the Elasticsearch index and type using a script by putting the custom information in the meta attribute. [[script]] namespace = \"blog.comments\" routing = true script = \"\"\" module.exports = function(doc) { if (doc.score >= 100) { // NOTE: prefix dynamic index with namespace for proper cleanup on drops doc._meta_monstache = { index: \"blog.comments.highscore\", type: \"highScoreComment\", routing: doc.post_id }; } else { doc._meta_monstache = { routing: doc.post_id }; } return doc; } \"\"\"","title":"Routing"},{"location":"advanced/#joins","text":"Elasticsearch 6 introduces an updated approach to parent-child called joins. The following example shows how you can accomplish joins with Monstache. The example is based on the Elasticsearch documentation . This example assumes Monstache is syncing the test.test collection in MongoDB with the test.test index in Elasticsearch. First we will want to setup an index mapping in Elasticsearch describing the join field. curl -XPUT 'localhost:9200/test.test?pretty' -H 'Content-Type: application/json' -d' { \"mappings\": { \"_doc\": { \"properties\": { \"my_join_field\": { \"type\": \"join\", \"relations\": { \"question\": \"answer\" } } } } } } ' Warning The above mapping uses _doc as the Elasticsearch type. _doc is the recommended type for new versions Elasticsearch but it only works with Elasticsearch versions 6.2 and greater. Monstache defaults to using _doc as the type when it detects Elasticsearch version 6.2 or greater. If you are using a previous version of Elasticsearch monstache defaults to using the MongoDB collection name as the Elasticsearch type. The type Monstache uses can be overriden but it is not recommended from Elasticsearch 6.2 on. Next will will configure Monstache with custom Javascript middleware that does transformation and routing. In a file called CONFIG.toml. [[script]] namespace = \"test.test\" routing = true script = \"\"\" module.exports = function(doc) { var routing; if (doc.type === \"question\") { routing = doc._id; doc.my_join_field = { name: \"question\" } } else if (doc.type === \"answer\") { routing = doc.question; doc.my_join_field = { name: \"answer\", parent: routing }; } if (routing) { doc._meta_monstache = { routing: routing }; } return doc; } \"\"\" The mapping function adds a my_join_field field to each document. The contents of the field are based on the type attribute in the MongoDB document. Also, the function ensures that the routing is always based on the _id of the question document. Now with this config in place we can start Monstache. We will use verbose to see the requests. monstache -verbose -f CONFIG.toml With Monstache running we are now ready to insert into MongoDB rs:PRIMARY> use test; switched to db test rs:PRIMARY> db.test.insert({type: \"question\", text: \"This is a question\"}); rs:PRIMARY> db.test.find() { \"_id\" : ObjectId(\"5a84a8b826993bde57c12893\"), \"type\" : \"question\", \"text\" : \"This is a question\" } rs:PRIMARY> db.test.insert({type: \"answer\", text: \"This is an answer\", question: ObjectId(\"5a84a8b826993bde57c12893\") }); When we insert these documents we should see Monstache generate the following requests to Elasticsearch {\"index\":{\"_id\":\"5a84a8b826993bde57c12893\",\"_index\":\"test.test\",\"_type\":\"_doc\",\"routing\":\"5a84a8b826993bde57c12893\",\"version\":6522523668566769665,\"version_type\":\"external\"}} {\"my_join_field\":{\"name\":\"question\"},\"text\":\"This is a question\",\"type\":\"question\"} {\"index\":{\"_id\":\"5a84a92b26993bde57c12894\",\"_index\":\"test.test\",\"_type\":\"_doc\",\"routing\":\"5a84a8b826993bde57c12893\",\"version\":6522524162488008705,\"version_type\":\"external\"}} {\"my_join_field\":{\"name\":\"answer\",\"parent\":\"5a84a8b826993bde57c12893\"},\"question\":\"5a84a8b826993bde57c12893\",\"text\":\"This is an answer\",\"type\":\"answer\"} This looks good. We should now have a parent/child relationship between these documents in Elasticsearch. If we do a search on the test.test index we see the following results: \"hits\" : { \"total\" : 2, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"test.test\", \"_type\" : \"_doc\", \"_id\" : \"5a84a8b826993bde57c12893\", \"_score\" : 1.0, \"_routing\" : \"5a84a8b826993bde57c12893\", \"_source\" : { \"my_join_field\" : { \"name\" : \"question\" }, \"text\" : \"This is a question\", \"type\" : \"question\" } }, { \"_index\" : \"test.test\", \"_type\" : \"_doc\", \"_id\" : \"5a84a92b26993bde57c12894\", \"_score\" : 1.0, \"_routing\" : \"5a84a8b826993bde57c12893\", \"_source\" : { \"my_join_field\" : { \"name\" : \"answer\", \"parent\" : \"5a84a8b826993bde57c12893\" }, \"question\" : \"5a84a8b826993bde57c12893\", \"text\" : \"This is an answer\", \"type\" : \"answer\" } } ] } To clean up our documents in Elasticsearch a bit we can omit the information that we don't really need in the source docs by updating our mapping function. This information needs not be at the top-level since it is duplicated in my_join_field . return _.omit(doc, \"type\", \"question\"); If your parent and child documents are in separate MongoDB collections then you would set up a script for each collection. You can tell if the doc is a parent or child by the collection it comes from. The only other difference would be that you would need to override the index dynamically in addition to the routing such that documents from both MongoDB collections target the same index. doc._meta_monstache = { routing: routing, index: \"parentsAndChildren\" }; Warning You must be careful when you route 2 or more MongoDB collections to the same Elasticsearch index that the document _ids across the MongoDB collections do not collide for any 2 docs because they will be used as the _id in the target index.","title":"Joins"},{"location":"advanced/#time-machines","text":"If you are not just interested in what the current value of a document in MongoDB is, but also would like to see how it has changed over time use time machine namespaces . For example, you've inserted and later updated a document with id 123 in the test.test collection in MongoDB. If test.test is a time machine namespace you will have 2 documents representing those changes in the log.test.test.2018-02-20 index (timestamp will change) in Elasticsearch. If you later want all the changes made to that document in MongoDB you can issue a query like this: $ curl -XGET 'http://localhost:9200/log.test.test.*/_search?routing=123' -d ' { \"query\":{ \"sort\" : [ { \"_oplog_ts\" : {\"order\" : \"desc\"}} ], \"filtered\":{ \"query\":{ \"match_all\":{} }, \"filter\":{ \"term\":{ \"_source_id\":\"123\" } } } } }' That query will be very efficient because it only queries the shard that all the change docs went to for MongoDB document id 123. It filters the documents on that shard by _source_id , or id from MongoDB, to only give us the changes to that document. Finally, it sorts by the _oplog_ts which gives us the most recent change docs first. The index pattern in the query is a wildcard to pick up all the timestamped indexes that we've acculated for the test.test namespace.","title":"Time Machines"},{"location":"advanced/#merge-patches","text":"A unique feature of monstache is support for JSON Merge Patches rfc-7396 . If merge patches are enabled monstache will add an additional field to documents indexed into Elasticsearch. The name of this field is configurable but it defaults to json-merge-patches . Consider the following example with merge patches enabled... db.test.insert({name: \"Joe\", age: 16, friends: [1, 2, 3]}) At this point you would have the following document source in Elasticsearch. \"_source\" : { \"age\" : 16, \"friends\" : [ 1, 2, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 } ], \"name\" : \"Joe\" } As you can see we have a single timestamped merge patch in the json-merge-patches array. Now let's update the document to remove a friend and update the age. db.test.update({name: \"Joe\"}, {$set: {age: 21, friends: [1, 3]}}) If we now look at the document in Elasticsearch we see the following: \"_source\" : { \"age\" : 21, \"friends\" : [ 1, 3 ], \"json-merge-patches\" : [ { \"p\" : \"{\\\"age\\\":16,\\\"friends\\\":[1,2,3],\\\"name\\\":\\\"Joe\\\"}\", \"ts\" : 1487263414, \"v\" : 1 }, { \"p\" : \"{\\\"age\\\":21,\\\"friends\\\":[1,3]}\", \"ts\" : 1487263746, \"v\" : 2 } ], \"name\" : \"Joe\" } You can see that the document was updated as expected and an additional merge patch was added. Each time the document is updated in MongoDB the corresponding document in Elasticsearch gains a timestamped merge patch. Using this information we can time travel is the document's history. There is a merge patch for each version of the document. To recreate a specific version we simply need to apply the merge patches in order up to the version that we want. To get version 1 of the document above we start with {} and apply the 1st merge patch. To get version 2 of the document above we start with {} apply the 1st merge patch to get v1 apply the 2nd merge patch to v1 to get v2 The timestamps associated with these merge patches are in seconds since the epoch, taken from the timestamp recorded in the oplog when the insert or update occured. To enable the merge patches feature in monstache you need to add the following to you TOML config: enable-patches = true patch-namespaces = [\"test.test\"] You need you add each namespace that you would like to see patches for in the patch-namespaces array. Optionally, you can change the key under which the patches are stored in the source document as follows: merge-patch-attribute = \"custom-merge-attr\" Merge patches will only be recorded for data read from the MongoDB oplog. Data read using the direct read feature will not be enhanced with merge patches. Most likely, you will want to turn off indexing for the merge patch attribute. You can do this by creating an index template for each patch namespace before running monstache... PUT /_template/test.test { \"template\" : \"test.test\", \"mappings\" : { \"test\" : { \"json-merge-patches\" : { \"index\" : false } } } }","title":"Merge Patches"},{"location":"advanced/#systemd","text":"Monstache has support built in for integrating with systemd. The following monstache.service is an example systemd configuration. [Unit] Description=monstache sync service [Service] Type=notify ExecStart=/usr/local/bin/monstache -f /etc/monstache/config.toml WatchdogSec=30s Restart=always [Install] WantedBy=multi-user.target Systemd unit files are normally saved to /lib/systemd/system . Verify same with your OS documentation. After saving the monstache.service file you can run systemctl daemon-reload to tell systemd to reload all unit files. You can enable the service to start on boot with systemctl enable monstache.service and start the service with systemctl start monstache.service . With the configuration above monstache will notify systemd when it has started successfully and then notify systemd repeatedly at half the WatchDog interval to signal liveness. The configuration above causes systemd to restart monstache if it does not start or respond within the WatchdDog interval.","title":"Systemd"},{"location":"advanced/#docker","text":"There are Docker images available for Monstache on Docker Hub You can pull and run the latest images with docker run rwynn/monstache:rel6 -v docker run rwynn/monstache:rel5 -v You can pull and run release images with docker run rwynn/monstache:6.7.4 -v docker run rwynn/monstache:5.7.4 -v For example, to run monstache via Docker with a golang plugin that resides at ~/plugin/plugin.so on the host you can use a bind mount docker run --rm --net=host -v ~/plugin:/tmp/plugin rwynn/monstache:6.7.4 -mapper-plugin-path /tmp/plugin/plugin.so","title":"Docker"},{"location":"advanced/#http-server","text":"Monstache has a built in HTTP server that you can enable with --enable-http-server. It listens on :8080 by default but you can change this with --http-server-addr. When using monstache with kubernetes this server can be used to detect liveness and act accordingly The following GET endpoints are available","title":"HTTP Server"},{"location":"advanced/#started","text":"Returns the uptime of the server","title":"/started"},{"location":"advanced/#healthz","text":"Returns at 200 status code with the text \"ok\" when monstache is running","title":"/healthz"},{"location":"advanced/#stats","text":"Returns the current indexing statistics in JSON format. Only available if stats are enabled","title":"/stats"},{"location":"advanced/#instance","text":"Returns information about the running monstache process including whether or not it is currently enabled (a cluster will have one enabled process) and the most recent change event timestamp read from MongoDB.","title":"/instance"},{"location":"advanced/#debug-if-pprof-is-enabled","text":"If the pprof setting is enabled the following endpoints are also made available: /debug/pprof/ /debug/pprof/cmdline /debug/pprof/profile /debug/pprof/symbol /debug/pprof/trace","title":"/debug (if pprof is enabled)"},{"location":"advanced/#mongodb-authentication","text":"Check the following link for all available options that you can specify in the MongoDB connection string related to authentication. For more information on how the MongoDB driver processes authentication configuration see the driver docs .","title":"MongoDB Authentication"},{"location":"advanced/#aws-signature-version-4","text":"Monstache has included AWS Signature Version 4 request signing. To enable the AWS Signature Version 4 support add the following to your config file: elasticsearch-urls = [\"https://<endpoint_from_aws_overview_screen>:443\"] [aws-connect] access-key = \"XXX\" secret-key = \"YYY\" region = \"ZZZ\" See the docs for aws-connect for the different stategies available for configuring a credential provider. Notice how the elasticsearch-url references the port number 443 in the connection string. This is because AWS makes your cluster available on the standard https port and not the default Elasticsearch port of 9200 . If you have connection problems make sure you are using the correct port. You cannot omit the port because the driver will default to 9200 if a port is not specified. You can read more about Signature Version 4 and Amazon Elasticsearch Service . For information on how to obtain the access-key and secret-key required to connect you can read this blog post . In short, you will need to create or use an existing IAM user in your AWS account. You will then need to give this user access to your Elasticsearch domain. The access-key and secret-key you put in your configuration file are those associated with the IAM user. { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::<account-id>:user/<iam-user-name>\" }, \"Action\": \"es:*\", \"Resource\": \"arn:aws:es:us-east-1:<account-id>:domain/<elasticsearch-domain-name>/*\" }","title":"AWS Signature Version 4"},{"location":"advanced/#watching-changes-on-specific-fields-only","text":"If you are using MongoDB 3.6+ you can use a change stream pipeline to only listen for change events on specific fields. For example, if you wanted to listen for create , delete , and update events on the namespace test.test , but you only wanted to sync changes when the foo or bar field changed on the doc, you could use the following configuration. If, for example, a field named count changed on the document, then this change would be ignored by monstache. change-stream-namespaces = [\"test.test\"] [[pipeline]] namespace = \"test.test\" script = \"\"\" module.exports = function(ns, changeStream) { if (changeStream) { return [ { $match: { $or: [ { \"updateDescription\": {$exists: false} }, { \"updateDescription.updatedFields.foo\": {$exists: true}}, { \"updateDescription.updatedFields.bar\": {$exists: true}} ] } } ]; } else { return []; } } \"\"\" To build complicated change stream pipelines see Change Events for information on the structure of change events. This information will shape your pipeline.","title":"Watching changes on specific fields only"},{"location":"advanced/#mongodb-view-replication","text":"You may have a situation where you want to replicate a MongoDB view in Elasticsearch. Or you have a collection that should trigger sync of another collection. You can use the relate config to do this. Consider you have a collections thing and state . A thing has an associated state and a thing is linked to a state via a field s which points to the _id of the associated state in the state collection. You can create a view in MongoDB that uses a $lookup to pull the state information in and present a view of things with the state information included. use thingdb; db.createView(\"thingview\", \"thing\", [ {$lookup: {from: \"state\", localField: \"s\", foreignField: \"_id\", as: \"s\"}}]) Given this view you can use the following config to keep things up to date in a things index in Elasticsearch. direct-read-namespaces = [\"thingdb.thingview\"] # read direct from the view of the collection to seed index change-stream-namespaces = [\"thingdb.thing\", \"thingdb.state\"] # change events happen on the underlying collections not views [[mapping]] namespace = \"thingdb.thing\" # map change events on the thing collection to the things index index = \"things\" [[mapping]] namespace = \"thingdb.thingview\" # map direct reads of the thingview to the same things index index = \"things\" [[relate]] namespace = \"thingdb.thing\" # when a thing changes look it up in the assoicated view by _id and index that with-namespace = \"thingdb.thingview\" keep-src = false # ignore the original thing that changed and instead just use the lookup of that thing via the view [[relate]] namespace = \"thingdb.state\" # when a state changes trigger a thing change event since thing is associated to a state with-namespace = \"thingdb.thing\" src-field = \"_id\" # use the _id field of the state that changed to lookup associated things match-field = \"s\" # only trigger change events for the things where thing.s (match-field) = state._id (src-field). keep-src = false Warning Be careful of the expense of using relate with a view. In the example above, if there were many things associated to a single state then a change to that state would trigger n+1 queries to MongoDB when n is the number of things related to the state. 1 query would be used to find all associated things and n queries would be used to lookup each thing in the view.","title":"MongoDB view replication"},{"location":"advanced/#amazon-documentdb-with-mongodb-compatibility","text":"Monstache support for Amazon DocumentDB is currently experimental. Support for the change streams API in MongoDB was recently added to Amazon DocumentDB. Consult the DocumentDB documentation for instructions on enabling change streams for your collections. Since Amazon DocumentDB only supports compatibility with MongoDB API 3.6 you will want to ensure that your change stream configuration targets collections and that your resume strategy is set to use tokens and not the default of timestamps. Ensure that your MongoDB connection URI is set with a primary read preference: e.g. ?readPreference=primary . # ensure you target collections in your change stream namespaces change-stream-namespaces = [\"db1.col1\", \"db2.col2\"] # ensure that resuming, if enabled, is done based on tokens and not timestamps resume = true resume-strategy = 1","title":"Amazon DocumentDB (with MongoDB compatibility)"},{"location":"config/","text":"Configuration \u00b6 Configuration can be specified in environment variables (a limited set of options), in a TOML config file or passed into monstache as program arguments on the command line. Environment variables names can be suffixed with __FILE. In this case the value of the environment variable will be interpreted as a file path. Monstache will attempt to read the file at that path and use the contents of the file as the value of the variable. Note Command line arguments take precedance over environment variables which in turn take precedance over the TOML config file. You can verify the final configuration used by Monstache by running monstache with -print-config . Warning Keep simple one-line configs above any TOML table definitions in your config file. A TOML table is only ended by another TOML table or the end of the file. Anything below a TOML table will be interpreted to be part of the table by the parser unless it is ended. See the following Issue 58 for more information. aws-connect \u00b6 TOML table (default nil ) Enable support for using a connection to Elasticsearch that uses AWS Signature Version 4 strategy \u00b6 int (default 0) The stategy used to configure the AWS credential provider. The 0 strategy is static and uses the values of access-key and secret-key . The 1 strategy is file based and loads the credentials from the credentials-file setting or from value of the standard AWS_SHARED_CREDENTIALS_FILE , or ~/.aws/credentials . The 2 strategy loads the credentials from the standard AWS environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . The 3 strategy loads the credentials from the default remote endpoints such as EC2 or ECS roles. The 4 strategy chains together strategies 1-3 and uses the first strategy that returns a credential. credentials-file \u00b6 string (default \"~/.aws/credentials\") The credentials file to use. Normally, you need not set this as it will come from either AWS_SHARED_CREDENTIALS_FILE or default to ~/.aws/credentials . profile \u00b6 string (default \"\") The AWS profile to use from the credentials file. If not provided a profile named default will be used. watch-credentials \u00b6 bool (default false) Set to true to put a watch on the credentials-watch-dir . When a file in the watch dir is changed monstache will invalidate the credentials such that they will be re-established on the next request to Elasticsearch. credentials-watch-dir \u00b6 string (default \"~/.aws\") The path to a directory to watch for changes if watch-credentials is enabled. force-expire \u00b6 string (default \"\") A golang duration string, e.g. 5m. If given, monstache will force expire the credentials on this interval. access-key \u00b6 string (default \"\") (env var name MONSTACHE_AWS_ACCESS_KEY ) AWS Access Key secret-key \u00b6 string (default \"\") (env var name MONSTACHE_AWS_SECRET_KEY ) AWS Secret Key region \u00b6 string (default \"\") (env var name MONSTACHE_AWS_REGION ) AWS Region change-stream-namespaces \u00b6 []string (default nil ) (env var name MONSTACHE_CHANGE_STREAM_NS ) This option requires MongoDB 3.6 or above This option allows you to opt in to using MongoDB change streams . The namespaces included will be tailed using watch API. When this option is enabled the direct tailing of the oplog is disabled, therefore you do not need to specify additional regular expressions to filter the set of collections to watch. If you are using MongoDB 4 or greater you can open a change stream against entire databases or even the entire deployment. To tail a database set the value of the namespace to the database name. For example, instead of db.collection the value would simply be db . To tail the entire deployment use an empty string as the namespace value. For example, change-stream-namespaces = [ '' ] . This option may be passed on the command line as ./monstache --change-stream-namespace test.foo If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_CHANGE_STREAM_NS=test.foo,test.bar config-database-name \u00b6 string (default monstache ) The name of the MongoDB database that monstache will store metadata under. This metadata includes information to support resuming from a specific point in the oplog and managing cluster mode. This database is only written to for some configurations. Namely, if you specify cluster-name , enable resume or set direct-read-stateful . WARNING: If you are listening to changes via change-stream-namespaces , you cannot set the same database to both listen to changes & store the configs in. cluster-name \u00b6 string (default \"\" ) (env var name MONSTACHE_CLUSTER ) When cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate. Only one of the processes in a cluster will sync changes. The other processes will be in a paused state. If the process which is syncing changes goes down for some reason one of the processes in paused state will take control and start syncing. See the section high availability for more information. delete-index-pattern \u00b6 string (default * ) When using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete will consider. If monstache only indexes to index a, b, and c then you can set this to a,b,c . If monstache only indexes to indexes starting with mydb then you can set this to mydb* . delete-strategy \u00b6 int (default 0 ) The strategy to use for handling document deletes when custom indexing is done in scripts. Strategy 0 -default- will do a term query by document id across all Elasticsearch indexes in delete-index-pattern . Will only perform the delete if one single document is returned by the query. Stategy 1 -deprecated- will store indexing metadata in MongoDB in the monstache.meta collection and use this metadata to locate and delete the document. Stategy 2 will completely ignore document deletes in MongoDB. direct-read-bounded \u00b6 boolean (default false ) When this option is enabled monstache will ensure that all direct read queries have a min and max set on the query. This ensures that direct reads will complete and not chase new data that is being inserted while the cursor is being exhausted. direct-read-concur \u00b6 int (default 0 ) This option allows you to control the number of namespaces in direct-read-namespaces which will be syncing concurrently. By default monstache starts reading and syncing all namespaces concurrently. If this places too much stress on MongoDB then you can set this option to an integer greater than 0. If you set it to 1 , for example, then monstache will sync the collections serially. Numbers greater than 1 allow you to sync collections in batches of that size. direct-read-dynamic-exclude-regex \u00b6 string (default \"\" ) (env var name MONSTACHE_DIRECT_READ_NS_DYNAMIC_EXCLUDE_REGEX ) This option is only available in monstache v5 and v6. This options allows you to exclude any collections that match the given regex when monstache is directed to dynamically register direct-read-namespaces . When direct read namespaces are explicit it is not used. direct-read-dynamic-include-regex \u00b6 string (default \"\" ) (env var name MONSTACHE_DIRECT_READ_NS_DYNAMIC_INCLUDE_REGEX ) This option is only available in monstache v5 and v6. This options allows you to only include collections that match the given regex when monstache is directed to dynamically register direct-read-namespaces . When direct read namespaces are explicit it is not used. direct-read-namespaces \u00b6 []string (default nil ) (env var name MONSTACHE_DIRECT_READ_NS ) This option allows you to directly copy collections from MongoDB to Elasticsearch. Monstache allows filtering the data that is actually indexed to Elasticsearch, so you need not necessarily copy the entire collection. Note In monstache v5 and v6 you can use an array with a single empty string to direct monstache to dynamically discover your collections and perform direct reads on them. When direct-read-dynamic-exclude-regex is configured you can prune from the list that is discovered. System collections will not be considered for inclusion in the discovery. Since the oplog is a capped collection it may only contain a subset of all your data. In this case you can perform a direct sync of Mongodb to Elasticsearch. To do this, set direct-read-namespaces to an array of namespaces that you would like to copy. Monstache will perform reads directly from the given set of db.collection and sync them to Elasticsearch. Note This option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_DIRECT_READ_NS=test.foo,test.bar Warning When direct reads are enabled Monstache still processes change events while the direct reads are being performed. It does not wait until direct reads are completed to start listening for changes. This is to ensure that any changes that occur during the direct read process get synchronized. By default, Monstache maps a MongoDB collection named foo in a database named test to the test.foo index in Elasticsearch. For maximum indexing performance when doing alot of a direct reads you might want to adjust the refresh interval during indexing on the destination Elasticsearch indices. The refresh interval can be set at a global level in elasticsearch.yml or on a per index basis by using the Index Settings or Index Template APIs. For more information see Update Indices Settings . By default, Elasticsearch refreshes every second. You will want to increase this value or turn off refresh completely during the indexing phase by setting the refresh_interval to -1. Remember to reset the refresh_interval to a positive value and do a force merge after the indexing phase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries. Another way to speed up bulk indexing is to set the number_of_replicas to 0 while indexing and then later increase the number of replicas. The following index template shows how one might configure a target index for better indexing throughput by controlling replicas and the refresh interval. The index template needs to be installed before running monstache. { \"index_patterns\": [\"test.*\"], \"settings\": { \"number_of_shards\": 5, \"number_of_replicas\": 0, \"refresh_interval\": \"30s\" } } direct-read-no-timeout \u00b6 boolean (default false ) When direct-read-no-timeout is true monstache will set the no cursor timeout flag on cursors opened for direct reads. The default is not to do this since having cursors without timeouts is not generally a good practice. However, for reading very large collections you may find it necessary to avoid cursor timeout errors. An alternative to enabling this setting is to increase the cursor timeout on your MongoDB server or look into using the direct-read-split-max and direct-read-concur options to limit the number of cursors opened for direct reads. direct-read-split-max \u00b6 int (default 9 ) The maximum number of times to split a collection for direct reads. This setting greatly impacts the memory consumption of Monstache. When direct reads are performed, the collection is first broken up into ranges which are then read concurrently is separate go routines. If you increase this value you will notice the connection count increase in mongostat when direct reads are performed. You will also notice the memory consumption of Monstache grow. Increasing this value can increase the throughput for reading large collections, but you need to have enough memory available to Monstache to do so. You can decrease this value for a memory constrained Monstache process. To disable collection splitting altogether, set this option to -1 . In this case monstache will not try to segment the collection, but rather use a single cursor for the entire read. direct-read-stateful \u00b6 boolean (default false ) When this setting is set to true monstache will mark direct read namespaces as complete after they have been fully read in a collection named directreads in the monstache config database. On subsequent restarts monstache will check this collection and only start direct reads for the namespaces not in the completed list. This allow you to keep the list the direct read namespaces in the configuration but manage the list that has completed and should not be run again externally in MongoDB. Deleting the directreads collection and restarting monstache will force a full sync. disable-change-events \u00b6 boolean (default false ) When disable-change-events is true monstache will not listen to change events from the oplog or call watch on any collections. This option is only useful if you are using direct-read-namespaces to copy collections and would prefer not to sync change events. disable-file-pipeline-put \u00b6 boolean (default false ) This setting only applies to monstache versions 5 and 6. When this option is true monstache will not attempt to auto create an ingest pipeline named attachment with a file field at startup when index-files is enabled. In this case the user must create the pipeline before running monstache. For example, the user must issue a command against Elasticsearch as follows prior to running monstache in order to index GridFS files: PUT _ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\", \"indexed_chars\" : -1 } } ] } dropped-databases \u00b6 boolean (default true ) When dropped-databases is false monstache will not delete the mapped indexes in Elasticsearch if a MongoDB database is dropped dropped-collections \u00b6 boolean (default true ) When dropped-collections is false monstache will not delete the mapped index in Elasticsearch if a MongoDB collection is dropped elasticsearch-user \u00b6 string (default \"\" ) (env var name MONSTACHE_ES_USER ) Optional Elasticsearch username for basic auth elasticsearch-password \u00b6 string (default \"\" ) (env var name MONSTACHE_ES_PASS ) Optional Elasticsearch password for basic auth elasticsearch-urls \u00b6 []string (default [ \"http://localhost:9200\" ] ) (env var name MONSTACHE_ES_URLS ) An array of URLs to connect to the Elasticsearch REST Interface Note This option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2 If specified as an environment variable the value should be URLs separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_ES_URLS=http://es1:9200,http://es2:9200 elasticsearch-healthcheck-timeout-startup \u00b6 int (default 15 ) The number of seconds to wait on the initial health check to Elasticsearch to responed before giving up and exiting. elasticsearch-healthcheck-timeout \u00b6 int (default 5 ) The number of seconds to wait for a post-initial health check to Elasticsearch to respond elasticsearch-version \u00b6 string (by default determined by connecting to the server ) When elasticsearch-version is provided monstache will parse the given server version to determine how to interact with the Elasticsearch API. This is normally not recommended because monstache will connect to Elasticsearch to find out which version is being used. This option is provided for cases where connecting to the base URL of the Elasticsearch REST API to get the version is not possible or desired. elasticsearch-max-conns \u00b6 int (default 4 ) The size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch. If you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded. To increase the size of the bulk indexing queue you can update the Elasticsearch config file: thread_pool: bulk: queue_size: 200 For more information see Thread Pool . You will want to tune this variable in sync with the elasticsearch-max-bytes option. elasticsearch-retry \u00b6 boolean (default false ) When elasticseach-retry is true a failed request to Elasticsearch will be retried with an exponential backoff policy. The policy is set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how this works see Back Off Strategy elasticsearch-client-timeout \u00b6 int (default 0 ) The number of seconds before a request to Elasticsearch times out. A setting of 0, the default, disables the timeout. elasticsearch-max-docs \u00b6 int (default -1 ) When elasticsearch-max-docs is given a bulk index request to Elasticsearch will be forced when the buffer reaches the given number of documents. Warning It is not recommended to change this option but rather use elasticsearch-max-bytes instead since the document count is not a good gauge of when to flush. The default value of -1 means to not use the number of docs as a flush indicator. elasticsearch-max-bytes \u00b6 int (default 8MB as bytes) When elasticsearch-max-bytes is given a bulk index request to Elasticsearch will be forced when a connection buffer reaches the given number of bytes. This setting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory. Each connection in elasticsearch-max-conns will flush when its queue gets filled to this size. elasticsearch-max-seconds \u00b6 int (default 1 ) When elasticsearch-max-seconds is given a bulk index request to Elasticsearch will be forced when a request has not been made in the given number of seconds. The default value is automatically increased to 5 when direct read namespaces are detected. This is to ensure that flushes do not happen too often in this case which would cut performance. elasticsearch-pem-file \u00b6 string (default \"\" ) (env var name MONSTACHE_ES_PEM ) When elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to Elasticsearch. This should only be used when Elasticsearch is configured with SSL enabled. elasticsearch-pki-auth \u00b6 TOML table (default nil ) Used to configure client to use PKI user auth for Elasticsearch cert-file \u00b6 string (default \"\") (env var name MONSTACHE_ES_PKI_CERT ) Path to the cert file e.g. the --cert argument to curl key-file \u00b6 string (default \"\") (env var name MONSTACHE_ES_PKI_KEY ) Path to the key file e.g. the --key argument to curl elasticsearch-validate-pem-file \u00b6 boolean (default true ) (env var name MONSTACHE_ES_VALIDATE_PEM ) When elasticsearch-validate-pem-file is false TLS will be configured to skip verification enable-easy-json \u00b6 boolean (default false ) When enable-easy-json is true monstache will the easy-json library to serialize requests to Elasticsearch enable-http-server \u00b6 boolean (default false ) Add this flag to enable an embedded HTTP server at localhost:8080 enable-oplog \u00b6 boolean (default false ) This option only applies to monstache v5 and v6. Enabling it turns on change event emulation feature that tails the MongoDB oplog directly. It should only be turned on when pairing monstache v5 or v6 with a MongoDB server at a server compatibility version less than 3.6. enable-patches \u00b6 boolean (default false ) Set to true to enable storing rfc7396 patches in your Elasticsearch documents env-delimiter \u00b6 string (default , ) This option is only supported on the command line. The value for this delimiter will be used to split environment variable values when the environment variable is used in conjunction with an option of array type. E.g. with export MONSTACHE_DIRECT_READ_NS=test.test,foo.bar . exit-after-direct-reads \u00b6 boolean (default false ) The direct-read-namespaces option gives you a way to do a full sync on multiple collections. At times you may want to perform a full sync via the direct-read-namespaces option and then quit monstache. Set this option to true and monstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful if you would like to run monstache to run a full sync on a set of collections via a cron job. fail-fast \u00b6 boolean (default false ) When fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache will log the request that produced the response as an ERROR and then exit immediately with an error status. Normally, monstache just logs the error and continues processing events. If monstache has been configured with elasticsearch-retry true, a failed request will be retried before being considered a failure. file-downloaders \u00b6 int (default 10 ) Number of go routines concurrently processing GridFS files when file index-files is turned on. file-highlighting \u00b6 boolean (default false ) When file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files for queries on files which were indexed in Elasticsearch from gridfs. file-namespaces \u00b6 []string (default nil ) (env var name MONSTACHE_FILE_NS ) The file-namespaces config must be set when index-files is enabled. file-namespaces must be set to an array of MongoDB namespace strings. Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their raw content indexed into Elasticsearch via either the mapper-attachments or ingest-attachment plugin. Note This option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_FILE_NS=test.foo,test.bar filter \u00b6 [] array of TOML table (default nil ) When filter is given monstache will pass the MongoDB document from an insert or update operation into the filter function immediately after it is read from the oplog. Return true from the function to continue processing the document or false to completely ignore the document. See the section Middleware for more information. namespace \u00b6 string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit namespace the filter function will be applied to all documents. script \u00b6 string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return true/false to include or filter the document. path \u00b6 string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. graylog-addr \u00b6 string (default \"\") (env var name MONSTACHE_GRAYLOG_ADDR ) The address of a graylog server to redirect logs to in GELF gtm-settings \u00b6 TOML table (default nil ) The following gtm configuration properties are available. See gtm for details channel-size \u00b6 int (default 512) Controls the size of the go channels created for processing events. When many events are processed at once a larger channel size may prevent blocking in gtm. buffer-size \u00b6 int (default 32) Determines how many documents are buffered by a gtm worker go routine before they are batch fetched from MongoDB. When many documents are inserted or updated at once it is better to fetch them together. buffer-duration \u00b6 string (default 75ms) A string representation of a golang duration. Determines the maximum time a buffer is held before it is fetched in batch from MongoDB and flushed for indexing. max-await-time \u00b6 string (default \"\") A string represetation of a golang duration, e.g. \"10s\". If set, will be converted and passed at the maxAwaitTimeMS option for change streams. This determines the maximum amount of time in milliseconds the server waits for new data changes to report to the change stream cursor before returning an empty batch. gzip \u00b6 boolean (default false ) When gzip is true, monstache will compress requests to Elasticsearch. If you enable gzip in monstache and are using Elasticsearch prior to version 5 you will also need to update the Elasticsearch config file to set http.compression: true. In Elasticsearch version 5 and above http.compression is enabled by default. Enabling gzip compression is recommended if you enable the index-files setting. http-server-addr \u00b6 string (default :8080 ) (env var name MONSTACHE_HTTP_ADDR ) The address to bind the embedded HTTP server on if enabled index-as-update \u00b6 boolean (default false ) When index-as-update is set to true monstache will sync create and update operations in MongoDB as updates to Elasticsearch. This does not change the fact that Monstache always sends an entire copy of the data in MongoDB. It just means that any existing non-overlapping fields in Elasticsearch will be maintained. By default, monstache will overwrite the entire document in Elasticsearch. This setting may be useful if you make updates to Elasticsearch to the documents monstache has previously synced out of band and would like to retain these updates when the document changes in MongoDB. You will only be able to retain fields in Elasticsearch that do not overlap with fields in MongoDB. When this setting is turned on some guarantees about the order of operations applied in Elasticsearch are lost. The reason for this is that the version field cannot be set with this enabled. The version field by default is set to the timestamp of the event in MongoDB. Elasticsearch will only apply changes if the version number is greater or equal to the last value indexed maintaining serialization. If you enable this setting and do not see serialized updates in MongoDB being indexed correctly then you can mitigate this problem with the following settings: elasticsearch-max-conns = 1 [gtm-settings] buffer-size = 2048 buffer-duration = 4s index-files \u00b6 boolean (default false ) When index-files is true monstache will index the raw content of files stored in GridFS into Elasticsearch as an attachment type. By default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS. In order for index-files to index the raw content of files stored in GridFS you must install a plugin for Elasticsearch. For versions of Elasticsearch prior to version 5, you should install the mapper-attachments plugin. In version 5 or greater of Elasticsearch the mapper-attachment plugin is deprecated and you should install the ingest-attachment plugin instead. For further information on how to configure monstache to index content from GridFS, see the section GridFS support . index-oplog-time \u00b6 boolean (default false ) If this option is set to true monstache will include 2 automatic fields in the source document indexed into Elasticsearch. The first is oplog_ts which is the timestamp for the event copied directly from the MongoDB oplog. The second is oplog_date which is an Elasticsearch date field corresponding to the time of the same event. This information is generally useful in Elasticsearch giving the notion of last updated. However, it's also valuable information to have for failed indexing requests since it gives one the information to replay from a failure point. See the option resume-from-timestamp for information on how to replay oplog events since a given event occurred. For data read via the direct read feature the oplog time will only be available if the id of the MongoDB document is an ObjectID. If the id of the MongoDB document is not an ObjectID and the document source is a direct read query then the oplog time will not be available. index-stats \u00b6 boolean (default false ) When both stats and index-stats are true monstache will write statistics about its indexing progress in Elasticsearch instead of standard out. The indexes used to store the statistics are time stamped by day and prefixed monstache.stats. . E.g. monstache.stats.2017-07-01 and so on. As these indexes will accrue over time your can use a tool like curator to prune them with a Delete Indices action and an age filter. logs \u00b6 TOML table (default nil ) (env var name MONSTACHE_LOG_DIR ) Allows writing logs to a file using a rolling appender instead of stdout. Supply a file path for each type of log you would like to send to a file. When the MONSTACHE_LOG_DIR environment variable is used then a log file for each log level will be generated in the given directory. info \u00b6 string (default \"\") The file path to write info level logs to warn \u00b6 string (default \"\") The file path to write warning level logs to error \u00b6 string (default \"\") The file path to write error level logs to trace \u00b6 string (default \"\") The file path to write trace level logs to. Trace logs are enabled via the verbose option. stats \u00b6 string (default \"\") The file path to write indexing statistics to. Stats logs are enabled via the stats option. log-rotate \u00b6 TOML table (default nil ) Use to configure how log files are rotated/managed when logging to files. These options are passed through to the lumberjack logger. max-size \u00b6 int (default 500) (env var name MONSTACHE_LOG_MAX_SIZE ) MaxSize is the maximum size in megabytes of the log file before it gets rotated. max-age \u00b6 int (default 28) (env var name MONSTACHE_LOG_MAX_AGE ) MaxAge is the maximum number of days to retain old log files based on the timestamp encoded in their filename. Note that a day is defined as 24 hours and may not exactly correspond to calendar days due to daylight savings, leap seconds, etc. Use a value of zero to ignore the age of files. max-backups \u00b6 int (default 5) (env var name MONSTACHE_LOG_MAX_BACKUPS ) MaxBackups is the maximum number of old log files to retain. Use a value of zero to retain all old log files (though MaxAge may still cause them to get deleted.) localtime \u00b6 boolean (default false) LocalTime determines if the time used for formatting the timestamps in backup files is the computer's local time. The default is to use UTC time. compress \u00b6 boolean (default false) Compress determines if the rotated log files should be compressed using gzip. The default is not to perform compression. mapper-plugin-path \u00b6 string (default \"\" ) The path to an .so file golang plugin. mapping \u00b6 [] array of TOML table (default nil ) When mapping is given monstache will be directed to override the default index and type assigned to documents in Elasticsearch. See the section Index Mapping for more information. namespace \u00b6 string (default \"\") The MongoDB namespace, db.collection, to apply the mapping to. index \u00b6 string (default \"same as namespace including the dot. e.g. test.test\") Allows you to override the default index that monstache will send documents to. By default, the index is the same as the MongoDB namespace. type \u00b6 string (default \"_doc for ES 6.2+ and the name of the MongoDB collection otherwise\") Allows you to override the default type that monstache will index documents with. Overriding the type is not recommended for Elasticsearch version 6.2+. pipeline \u00b6 string (default \"\") The name of an existing Elasticsearch pipeline to index the data with. A pipeline is a series of Elasticsearch processors to be executed. An Elasticsearch pipeline is one way to transform MongoDB data before indexing. max-file-size \u00b6 int (default 0 ) When max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes. merge-patch-attribute \u00b6 string (default json-merge-patches ) Customize the name of the property under which merge patches are stored mongo-url \u00b6 string (default localhost ) (env var name MONSTACHE_MONGO_URL ) The URL to connect to MongoDB which must follow the Standard Connection String Format For sharded clusters this URL should point to the mongos router server and the mongo-config-url option must be set to point to the config server. mongo-config-url \u00b6 string (default \"\" ) (env var name MONSTACHE_MONGO_CONFIG_URL ) This config must only be set for sharded MongoDB clusters. Has the same syntax as mongo-url. This URL must point to the MongoDB config server. Monstache will read the list of shards using this connection and then setup a listener to react to new shards being added to the cluster at a later time. It will then setup a new direct connection to each shard to listen for events. Setting the mongo-config-url is not necessary if you are using change-stream-namespaces . mongo-pem-file \u00b6 string (default \"\" ) (env var name MONSTACHE_MONGO_PEM ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. When mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to MongoDB. This should only be used when MongoDB is configured with SSL enabled. mongo-validate-pem-file \u00b6 boolean (default true ) (env var name MONSTACHE_MONGO_VALIDATE_PEM ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. When mongo-validate-pem-file is false TLS will be configured to skip verification mongo-oplog-database-name \u00b6 string (default local ) (env var name MONSTACHE_MONGO_OPLOG_DB ) When mongo-oplog-database-name is given monstache will look for the MongoDB oplog in the supplied database mongo-oplog-collection-name \u00b6 string (default oplog.rs ) (env var name MONSTACHE_MONGO_OPLOG_COL ) When mongo-oplog-collection-name is given monstache will look for the MongoDB oplog in the supplied collection. The collection defaults to oplog.rs which is what will be produced when replica sets are enabled. If you are using an old version of MongoDB with master based replication instead of replica sets, then you will need to configure this setting to oplog.$main . Warning If this setting was not supplied monstache would previously search for the first collection prefixed oplog in the local database. However, starting in monstache v4.13.1 and v3.20.1 this behavior has changed. Now, monstache will not do dynamic resolution. Since master based replication in MongoDB is no longer supported, monstache now defaults to oplog.rs and will only use another collection (e.g. oplog.$main ) if you explicitly config it to do so. mongo-dial-settings \u00b6 TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. The following MongoDB dial properties are available. Timeout values of 0 disable the timeout. ssl \u00b6 bool (default false) \u00b6 Set to true to establish a connection using TLS. timeout \u00b6 int (default 15) \u00b6 Seconds to wait when establishing an initial connection to MongoDB before giving up read-timeout \u00b6 int (default 30) \u00b6 Seconds to wait when reading data from MongoDB before giving up. Must be greater than 0. This should be greater than 10 because Monstache waits 10s for new change events by retrying the query. write-timeout \u00b6 int (default 30) \u00b6 Seconds to wait when writing data to MongoDB before giving up. Must be greated than 0. This should be greater than 10 because Monstache waits 10s for new change events by retrying the query. mongo-session-settings \u00b6 TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. The following MongoDB session properties are available. Timeout values of 0 disable the timeout. socket-timeout \u00b6 int (default 0) Seconds to wait for a non-responding socket before it is forcefully closed sync-timeout \u00b6 int (default 30) Amount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established. Must be greater than 0. mongo-x509-settings \u00b6 TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. Allows one to configure x509 authentication with MongoDB. For more information see x509 auth . Note You must configure your mongo-url with the request parameter authMechanism=MONGODB-X509 . You must also supply both of the following file paths: client-cert-pem-file \u00b6 string (default \"\") The path to a PEM encoded file containing the client cert client-key-pem-file \u00b6 string (default \"\") The path to a PEM encoded file containing the client key namespace-drop-exclude-regex \u00b6 regex (default \"\" ) (env var name MONSTACHE_NS_DROP_EXCLUDE_REGEX ) When namespace-drop-exclude-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex does not match the namespace then the operation will by synced. namespace-drop-regex \u00b6 regexp (default \"\" ) (env var name MONSTACHE_NS_DROP_REGEX ) When namespace-drop-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex matches the namespace then the operation will by synced. namespace-exclude-regex \u00b6 regex (default \"\" ) (env var name MONSTACHE_NS_EXCLUDE_REGEX ) When namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces . namespace-regex \u00b6 regexp (default \"\" ) (env var name MONSTACHE_NS_REGEX ) When namespace-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces . oplog-date-field-format \u00b6 string (default 2006/01/02 15:04:05 ) Use this option to override the layout for formatting the oplog_date field. Refer to the Format function for the reference time values to use in the layout. oplog-date-field-name \u00b6 string (default oplog_date ) Use this option to override the name of the field used to store the oplog date string oplog-ts-field-name \u00b6 string (default oplog_ts ) Use this option to override the name of the field used to store the oplog timestamp patch-namespaces \u00b6 []string (default nil ) (env var name MONSTACHE_PATCH_NS ) An array of MongoDB namespaces that you would like to enable rfc7396 patches on Note This option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_PATCH_NS=test.foo,test.bar pipeline \u00b6 [] array of TOML table (default nil ) When pipeline is given monstache will call the function specified to determine an array of aggregation pipeline stages to run. See the section Middleware for more information. namespace \u00b6 string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the pipeline function with be applied to all namespaces. script \u00b6 string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a namespace and a boolean indicating whether or not the data is a change stream. The function should return an array of aggregation pipeline stages. Note, for change streams the root of the pipeline will be the change event with a field fullDocument representing the changed doc. You should alter your pipeline stages according to this boolean. Monstache needs the change event data so do not replace the root of the document in your pipeline for change streams. path \u00b6 string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. pipe-allow-disk \u00b6 boolean (default false ) Add this flag to allow MongoDB to use the disk as a temporary store for data during aggregation pipelines post-processors \u00b6 int (default 10 ) Number of go routines concurrently calling the Process method in any golang middleware plugins installed via mapper-plugin-path . pprof \u00b6 boolean (default false ) When pprof is true and the http server is enabled, monstache will make profiling information available. See Profiling for Go for more information. print-config \u00b6 boolean (default false ) When print-config is true monstache will print its configuration and then exit prune-invalid-json \u00b6 boolean (default false ) If your MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true. The Golang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When prune-invalid-json is set to true Monstache will drop those fields so that indexing errors do not occur. relate \u00b6 [] array of TOML table (default nil ) Allows one to relate 2 namespaces together such that a change to one causes a sync of the associated namespace namespace \u00b6 string (default \"\") The namespace of the collection that, when modified, triggers a sync of the with-namespace with-namespace \u00b6 string (default \"\") The namespace of the collection or view that will be synced when namespace changes src-field \u00b6 string (default \"_id\") The name of the field in namespace that will be extracted from the change doc and used as the value side of the query into with-namespace match-field \u00b6 string (default \"_id\") The name of the field in with-namespace that will be used as the field name to match side of the query into with-namespace match-field-type \u00b6 string (default \"\") Valid values for this property are objectId , string , int , long and decimal . If the property is given the value of the src-field will be converted into the type given (if possible) before being used to query against match-field . keep-src \u00b6 bool (default \"false\") Whether or not to sync the original change event in addition to the one looked up in with-namespace. By default the original change is ignored and only the document from with-namespace is synced. max-depth \u00b6 int (default 0) If max-depth is greater than 0 then the relationship will only fire if the number of relationships between this relate and the originating event is less than or equal to the value given. By default monstache will continue following relationships until none are left. dot-notation \u00b6 bool (default \"false\") If match-field is a nested field like foo.bar then setting dot-notation to true produces the query { \"foo.bar\": 1 } to MongoDB. If dot-notation is not enabled then the query will be an exact match query sent as { foo { bar : 1 } } . relate-buffer \u00b6 int (default 1000 ) Number of relate events allowed to queue up before skipping the event and reporting an error. This setting was introduced to prevent scenarios where relate queries get queued up and stall the pipeline. You can increase this value if you are hitting this limit, but monstache will take more memory to hold the events and you may find that MongoDB experiences high CPU due to Monstache performing many queries concurrently to try to clear the buffer. This limit is usually hit when you have a relate config and then do a mass insert or update against the relate namespace. relate-threads \u00b6 int (default 10 ) Number of go routines concurrently processing relationships when relate is enabled. This dictates the concurrency of queries trying to unload the relate queue. replay \u00b6 boolean (default false ) Warning Replay is currently deprecated in favor of direct-read-namespaces . Replay may be removed in a future release. When replay is true, monstache replays all events from the beginning of the MongoDB oplog and syncs them to Elasticsearch. If you've previously synced Monstache to Elasticsearch you may see many WARN statments in the log indicating that there was a version conflict. This is normal during a replay and it just means that you already have data in Elasticsearch that is newer than the point in time data from the oplog. When resume and replay are both true, monstache replays all events from the beginning of the MongoDB oplog, syncs them to Elasticsearch and also writes the timestamps of processed events to monstache.monstache. When neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events occurring after this timestamp (tails starting at the end). Timestamps are not written to monstache.monstache. This is the default behavior. resume \u00b6 boolean (default false ) When resume is true, monstache writes the timestamp of MongoDB operations it has successfully synced to Elasticsearch to the collection monstache.monstache. It also reads that timestamp from that collection when it starts in order to replay events which it might have missed because monstache was stopped. If monstache is started with the cluster-name option set then resume is automatically turned on. resume-from-timestamp \u00b6 int64 (default 0 ) This option only applies when the resume-strategy is 0 for timestamp based resume. When resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits represent an offset within a second) is given, monstache will sync events starting immediately after the timestamp. This is useful if you have a specific timestamp from the oplog and would like to start syncing from after this event. If you supply an integer that is greater than 0 but less than or equal to the max value of a 32-bit integer then monstache will interpret the value as seconds since the epoch and automatically shift the value 32 bits left. resume-name \u00b6 string (default default ) monstache uses the value of resume-name as an id when storing and retrieving timestamps to and from the MongoDB collection monstache.monstache . The default value for this option is the string default . However, there are some exceptions. If monstache is started with the cluster-name option set then the name of the cluster becomes the resume-name. This is to ensure that any process in the cluster is able to resume from the last timestamp successfully processed. Another exception occurs when worker is enabled. In that case the worker name becomes the resume-name. resume-strategy \u00b6 int (default 0 ) The strategy to use for resuming streams from previous runs of monstache. Only applies when resume is enabled. This strategy is also used when cluster-name is set to ensure streams are resumed when the active process in the cluster switches. Strategy 0 -default- Timestamp based resume of change streams. Compatible with MongoDB API 4.0+. Stategy 1 Token based resume of change streams. Compatible with MongoDB API 3.6+. Timestamps and tokens are written periodically to the database configured in config-database-name . Timestamps are written to the collection named monstache . Tokens are written to the collection named tokens . resume-write-unsafe \u00b6 boolean (default false ) When resume-write-unsafe is true monstache sets the safety mode of the MongoDB session such that writes are fire and forget. This speeds up writing of timestamps used to resume synching in a subsequent run of monstache. This speed up comes at the cost of no error checking on the write of the timestamp. Since errors writing the last synched timestamp are only logged by monstache and do not stop execution it's not unreasonable to set this to true to get a speedup. routing-namespaces \u00b6 []string (default nil ) You only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to Elasticsearch. script \u00b6 [] array of TOML table (default nil ) When script is given monstache will pass the MongoDB document into the script before indexing into Elasticsearch. See the section Middleware for more information. namespace \u00b6 string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the mapping function with be applied to all documents. routing \u00b6 boolean (default false) Set routing to true if you override the index, routing or parent metadata via _meta_monstache script \u00b6 string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return a modified doc. You can also return true to index the original document or false to ignore the document and schedule any previous documents with the same id for deletion. path \u00b6 string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path. stats \u00b6 boolean (default false ) When stats is true monstache will periodically print statistics accumulated by the indexer stats-duration \u00b6 string (default 30s ) Sets the duration after which statistics are printed if stats is enabled stats-index-format \u00b6 string (default monstache.stats.2006-01-02 ) The time.Time supported index name format for stats indices. By default, stats indexes are partitioned by day. To use less indices for stats you can shorten this format string (e.g monstache.stats.2006-01) or remove the time component completely to use a single index. time-machine-namespaces \u00b6 []string (default nil ) (env var name MONSTACHE_TIME_MACHINE_NS ) Monstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync. When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too. Same goes for deleting documents in MongoDB. But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan. That's what time-machine-namespaces are for. When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index. Say for example, you insert a document into the test.test collection in MongoDB. Monstache will index by default into the test.test index in Elasticsearch, but with time machines it will also index it into log.test.test.2018-02-19 . When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id. But, it stores the id from MongoDB in the source field _source_id . Also, it adds _oplog_ts and _oplog_date fields on the source document. These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc. This lets you do some cool things but mostly you'll want to sort by _oplog_date and filter by _source_id to see how documents have changed over time. Because the indexes are timestamped you can drop then after a period of time so they don't take up space. If you just want the last couple of days of changes, delete the indexes with the old timestamps. Elastic curator is your friend here. Note This option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_TIME_MACHINE_NS=test.foo,test.bar time-machine-index-prefix \u00b6 string (default log ) If you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting. time-machine-index-suffix \u00b6 string (default 2006-01-02 ) If you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting. Consult the golang docs for how date formats work. By default this suffixes the index name with the year, month, and day. time-machine-direct-reads \u00b6 boolean (default false ) This setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added. tpl \u00b6 boolean (default false ) This option is only supported on the command line. When the tpl mode is turned on then any config file passed via -f will be interpreted and executed as a golang template before being loaded. The template will have access to all environment variables. The environment variables will be passed as a map to the template. The env map can be accessed as the dot . symbol in the golang template and values from the map obtained using the index function. For example, the given an environment variable THRESHOLD, then with -tpl -f config.toml the config.toml might contain... [[script]] namespace = \"mydb.mycollection\" script = \"\"\" module.exports = function(doc) { if ( doc.score > {{index . \"THRESHOLD\"}} ) { doc.important = true; } return doc; } \"\"\" verbose \u00b6 boolean (default false ) When verbose is true monstache with enable debug logging including a trace of requests to Elasticsearch worker \u00b6 string (default \"\" ) (env var name MONSTACHE_WORKER ) When worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers. Use this mode to run multiple monstache processes and distribute the work between them. In this mode monstache will ensure that each MongoDB document id always goes to the same worker and none of the other workers. See the Workers section for more information. workers \u00b6 []string (default nil ) An array of worker names to be used in conjunction with the worker option. Note This option may be passed on the command line as ./monstache --workers w1 --workers w2","title":"Configuration"},{"location":"config/#configuration","text":"Configuration can be specified in environment variables (a limited set of options), in a TOML config file or passed into monstache as program arguments on the command line. Environment variables names can be suffixed with __FILE. In this case the value of the environment variable will be interpreted as a file path. Monstache will attempt to read the file at that path and use the contents of the file as the value of the variable. Note Command line arguments take precedance over environment variables which in turn take precedance over the TOML config file. You can verify the final configuration used by Monstache by running monstache with -print-config . Warning Keep simple one-line configs above any TOML table definitions in your config file. A TOML table is only ended by another TOML table or the end of the file. Anything below a TOML table will be interpreted to be part of the table by the parser unless it is ended. See the following Issue 58 for more information.","title":"Configuration"},{"location":"config/#aws-connect","text":"TOML table (default nil ) Enable support for using a connection to Elasticsearch that uses AWS Signature Version 4","title":"aws-connect"},{"location":"config/#strategy","text":"int (default 0) The stategy used to configure the AWS credential provider. The 0 strategy is static and uses the values of access-key and secret-key . The 1 strategy is file based and loads the credentials from the credentials-file setting or from value of the standard AWS_SHARED_CREDENTIALS_FILE , or ~/.aws/credentials . The 2 strategy loads the credentials from the standard AWS environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . The 3 strategy loads the credentials from the default remote endpoints such as EC2 or ECS roles. The 4 strategy chains together strategies 1-3 and uses the first strategy that returns a credential.","title":"strategy"},{"location":"config/#credentials-file","text":"string (default \"~/.aws/credentials\") The credentials file to use. Normally, you need not set this as it will come from either AWS_SHARED_CREDENTIALS_FILE or default to ~/.aws/credentials .","title":"credentials-file"},{"location":"config/#profile","text":"string (default \"\") The AWS profile to use from the credentials file. If not provided a profile named default will be used.","title":"profile"},{"location":"config/#watch-credentials","text":"bool (default false) Set to true to put a watch on the credentials-watch-dir . When a file in the watch dir is changed monstache will invalidate the credentials such that they will be re-established on the next request to Elasticsearch.","title":"watch-credentials"},{"location":"config/#credentials-watch-dir","text":"string (default \"~/.aws\") The path to a directory to watch for changes if watch-credentials is enabled.","title":"credentials-watch-dir"},{"location":"config/#force-expire","text":"string (default \"\") A golang duration string, e.g. 5m. If given, monstache will force expire the credentials on this interval.","title":"force-expire"},{"location":"config/#access-key","text":"string (default \"\") (env var name MONSTACHE_AWS_ACCESS_KEY ) AWS Access Key","title":"access-key"},{"location":"config/#secret-key","text":"string (default \"\") (env var name MONSTACHE_AWS_SECRET_KEY ) AWS Secret Key","title":"secret-key"},{"location":"config/#region","text":"string (default \"\") (env var name MONSTACHE_AWS_REGION ) AWS Region","title":"region"},{"location":"config/#change-stream-namespaces","text":"[]string (default nil ) (env var name MONSTACHE_CHANGE_STREAM_NS ) This option requires MongoDB 3.6 or above This option allows you to opt in to using MongoDB change streams . The namespaces included will be tailed using watch API. When this option is enabled the direct tailing of the oplog is disabled, therefore you do not need to specify additional regular expressions to filter the set of collections to watch. If you are using MongoDB 4 or greater you can open a change stream against entire databases or even the entire deployment. To tail a database set the value of the namespace to the database name. For example, instead of db.collection the value would simply be db . To tail the entire deployment use an empty string as the namespace value. For example, change-stream-namespaces = [ '' ] . This option may be passed on the command line as ./monstache --change-stream-namespace test.foo If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_CHANGE_STREAM_NS=test.foo,test.bar","title":"change-stream-namespaces"},{"location":"config/#config-database-name","text":"string (default monstache ) The name of the MongoDB database that monstache will store metadata under. This metadata includes information to support resuming from a specific point in the oplog and managing cluster mode. This database is only written to for some configurations. Namely, if you specify cluster-name , enable resume or set direct-read-stateful . WARNING: If you are listening to changes via change-stream-namespaces , you cannot set the same database to both listen to changes & store the configs in.","title":"config-database-name"},{"location":"config/#cluster-name","text":"string (default \"\" ) (env var name MONSTACHE_CLUSTER ) When cluster-name is given monstache will enter a high availablity mode. Processes with cluster name set to the same value will coordinate. Only one of the processes in a cluster will sync changes. The other processes will be in a paused state. If the process which is syncing changes goes down for some reason one of the processes in paused state will take control and start syncing. See the section high availability for more information.","title":"cluster-name"},{"location":"config/#delete-index-pattern","text":"string (default * ) When using a stateless delete strategy, set this to a valid Elasticsearch index pattern to restrict the scope of possible indexes that a stateless delete will consider. If monstache only indexes to index a, b, and c then you can set this to a,b,c . If monstache only indexes to indexes starting with mydb then you can set this to mydb* .","title":"delete-index-pattern"},{"location":"config/#delete-strategy","text":"int (default 0 ) The strategy to use for handling document deletes when custom indexing is done in scripts. Strategy 0 -default- will do a term query by document id across all Elasticsearch indexes in delete-index-pattern . Will only perform the delete if one single document is returned by the query. Stategy 1 -deprecated- will store indexing metadata in MongoDB in the monstache.meta collection and use this metadata to locate and delete the document. Stategy 2 will completely ignore document deletes in MongoDB.","title":"delete-strategy"},{"location":"config/#direct-read-bounded","text":"boolean (default false ) When this option is enabled monstache will ensure that all direct read queries have a min and max set on the query. This ensures that direct reads will complete and not chase new data that is being inserted while the cursor is being exhausted.","title":"direct-read-bounded"},{"location":"config/#direct-read-concur","text":"int (default 0 ) This option allows you to control the number of namespaces in direct-read-namespaces which will be syncing concurrently. By default monstache starts reading and syncing all namespaces concurrently. If this places too much stress on MongoDB then you can set this option to an integer greater than 0. If you set it to 1 , for example, then monstache will sync the collections serially. Numbers greater than 1 allow you to sync collections in batches of that size.","title":"direct-read-concur"},{"location":"config/#direct-read-dynamic-exclude-regex","text":"string (default \"\" ) (env var name MONSTACHE_DIRECT_READ_NS_DYNAMIC_EXCLUDE_REGEX ) This option is only available in monstache v5 and v6. This options allows you to exclude any collections that match the given regex when monstache is directed to dynamically register direct-read-namespaces . When direct read namespaces are explicit it is not used.","title":"direct-read-dynamic-exclude-regex"},{"location":"config/#direct-read-dynamic-include-regex","text":"string (default \"\" ) (env var name MONSTACHE_DIRECT_READ_NS_DYNAMIC_INCLUDE_REGEX ) This option is only available in monstache v5 and v6. This options allows you to only include collections that match the given regex when monstache is directed to dynamically register direct-read-namespaces . When direct read namespaces are explicit it is not used.","title":"direct-read-dynamic-include-regex"},{"location":"config/#direct-read-namespaces","text":"[]string (default nil ) (env var name MONSTACHE_DIRECT_READ_NS ) This option allows you to directly copy collections from MongoDB to Elasticsearch. Monstache allows filtering the data that is actually indexed to Elasticsearch, so you need not necessarily copy the entire collection. Note In monstache v5 and v6 you can use an array with a single empty string to direct monstache to dynamically discover your collections and perform direct reads on them. When direct-read-dynamic-exclude-regex is configured you can prune from the list that is discovered. System collections will not be considered for inclusion in the discovery. Since the oplog is a capped collection it may only contain a subset of all your data. In this case you can perform a direct sync of Mongodb to Elasticsearch. To do this, set direct-read-namespaces to an array of namespaces that you would like to copy. Monstache will perform reads directly from the given set of db.collection and sync them to Elasticsearch. Note This option may be passed on the command line as ./monstache --direct-read-namespace test.foo --direct-read-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_DIRECT_READ_NS=test.foo,test.bar Warning When direct reads are enabled Monstache still processes change events while the direct reads are being performed. It does not wait until direct reads are completed to start listening for changes. This is to ensure that any changes that occur during the direct read process get synchronized. By default, Monstache maps a MongoDB collection named foo in a database named test to the test.foo index in Elasticsearch. For maximum indexing performance when doing alot of a direct reads you might want to adjust the refresh interval during indexing on the destination Elasticsearch indices. The refresh interval can be set at a global level in elasticsearch.yml or on a per index basis by using the Index Settings or Index Template APIs. For more information see Update Indices Settings . By default, Elasticsearch refreshes every second. You will want to increase this value or turn off refresh completely during the indexing phase by setting the refresh_interval to -1. Remember to reset the refresh_interval to a positive value and do a force merge after the indexing phase has completed if you decide to temporarily turn off refresh, otherwise you will not be able to see the new documents in queries. Another way to speed up bulk indexing is to set the number_of_replicas to 0 while indexing and then later increase the number of replicas. The following index template shows how one might configure a target index for better indexing throughput by controlling replicas and the refresh interval. The index template needs to be installed before running monstache. { \"index_patterns\": [\"test.*\"], \"settings\": { \"number_of_shards\": 5, \"number_of_replicas\": 0, \"refresh_interval\": \"30s\" } }","title":"direct-read-namespaces"},{"location":"config/#direct-read-no-timeout","text":"boolean (default false ) When direct-read-no-timeout is true monstache will set the no cursor timeout flag on cursors opened for direct reads. The default is not to do this since having cursors without timeouts is not generally a good practice. However, for reading very large collections you may find it necessary to avoid cursor timeout errors. An alternative to enabling this setting is to increase the cursor timeout on your MongoDB server or look into using the direct-read-split-max and direct-read-concur options to limit the number of cursors opened for direct reads.","title":"direct-read-no-timeout"},{"location":"config/#direct-read-split-max","text":"int (default 9 ) The maximum number of times to split a collection for direct reads. This setting greatly impacts the memory consumption of Monstache. When direct reads are performed, the collection is first broken up into ranges which are then read concurrently is separate go routines. If you increase this value you will notice the connection count increase in mongostat when direct reads are performed. You will also notice the memory consumption of Monstache grow. Increasing this value can increase the throughput for reading large collections, but you need to have enough memory available to Monstache to do so. You can decrease this value for a memory constrained Monstache process. To disable collection splitting altogether, set this option to -1 . In this case monstache will not try to segment the collection, but rather use a single cursor for the entire read.","title":"direct-read-split-max"},{"location":"config/#direct-read-stateful","text":"boolean (default false ) When this setting is set to true monstache will mark direct read namespaces as complete after they have been fully read in a collection named directreads in the monstache config database. On subsequent restarts monstache will check this collection and only start direct reads for the namespaces not in the completed list. This allow you to keep the list the direct read namespaces in the configuration but manage the list that has completed and should not be run again externally in MongoDB. Deleting the directreads collection and restarting monstache will force a full sync.","title":"direct-read-stateful"},{"location":"config/#disable-change-events","text":"boolean (default false ) When disable-change-events is true monstache will not listen to change events from the oplog or call watch on any collections. This option is only useful if you are using direct-read-namespaces to copy collections and would prefer not to sync change events.","title":"disable-change-events"},{"location":"config/#disable-file-pipeline-put","text":"boolean (default false ) This setting only applies to monstache versions 5 and 6. When this option is true monstache will not attempt to auto create an ingest pipeline named attachment with a file field at startup when index-files is enabled. In this case the user must create the pipeline before running monstache. For example, the user must issue a command against Elasticsearch as follows prior to running monstache in order to index GridFS files: PUT _ingest/pipeline/attachment { \"description\" : \"Extract file information\", \"processors\" : [ { \"attachment\" : { \"field\" : \"file\", \"indexed_chars\" : -1 } } ] }","title":"disable-file-pipeline-put"},{"location":"config/#dropped-databases","text":"boolean (default true ) When dropped-databases is false monstache will not delete the mapped indexes in Elasticsearch if a MongoDB database is dropped","title":"dropped-databases"},{"location":"config/#dropped-collections","text":"boolean (default true ) When dropped-collections is false monstache will not delete the mapped index in Elasticsearch if a MongoDB collection is dropped","title":"dropped-collections"},{"location":"config/#elasticsearch-user","text":"string (default \"\" ) (env var name MONSTACHE_ES_USER ) Optional Elasticsearch username for basic auth","title":"elasticsearch-user"},{"location":"config/#elasticsearch-password","text":"string (default \"\" ) (env var name MONSTACHE_ES_PASS ) Optional Elasticsearch password for basic auth","title":"elasticsearch-password"},{"location":"config/#elasticsearch-urls","text":"[]string (default [ \"http://localhost:9200\" ] ) (env var name MONSTACHE_ES_URLS ) An array of URLs to connect to the Elasticsearch REST Interface Note This option may be passed on the command line as ./monstache --elasticsearch-url URL1 --elasticsearch-url URL2 If specified as an environment variable the value should be URLs separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_ES_URLS=http://es1:9200,http://es2:9200","title":"elasticsearch-urls"},{"location":"config/#elasticsearch-healthcheck-timeout-startup","text":"int (default 15 ) The number of seconds to wait on the initial health check to Elasticsearch to responed before giving up and exiting.","title":"elasticsearch-healthcheck-timeout-startup"},{"location":"config/#elasticsearch-healthcheck-timeout","text":"int (default 5 ) The number of seconds to wait for a post-initial health check to Elasticsearch to respond","title":"elasticsearch-healthcheck-timeout"},{"location":"config/#elasticsearch-version","text":"string (by default determined by connecting to the server ) When elasticsearch-version is provided monstache will parse the given server version to determine how to interact with the Elasticsearch API. This is normally not recommended because monstache will connect to Elasticsearch to find out which version is being used. This option is provided for cases where connecting to the base URL of the Elasticsearch REST API to get the version is not possible or desired.","title":"elasticsearch-version"},{"location":"config/#elasticsearch-max-conns","text":"int (default 4 ) The size of the Elasticsearch HTTP connection pool. This determines the concurrency of bulk indexing requests to Elasticsearch. If you increase this value too high you may begin to see bulk indexing failures if the bulk index queue gets overloaded. To increase the size of the bulk indexing queue you can update the Elasticsearch config file: thread_pool: bulk: queue_size: 200 For more information see Thread Pool . You will want to tune this variable in sync with the elasticsearch-max-bytes option.","title":"elasticsearch-max-conns"},{"location":"config/#elasticsearch-retry","text":"boolean (default false ) When elasticseach-retry is true a failed request to Elasticsearch will be retried with an exponential backoff policy. The policy is set with an initial timeout of 50 ms, an exponential factor of 2, and a max wait of 20 seconds. For more information on how this works see Back Off Strategy","title":"elasticsearch-retry"},{"location":"config/#elasticsearch-client-timeout","text":"int (default 0 ) The number of seconds before a request to Elasticsearch times out. A setting of 0, the default, disables the timeout.","title":"elasticsearch-client-timeout"},{"location":"config/#elasticsearch-max-docs","text":"int (default -1 ) When elasticsearch-max-docs is given a bulk index request to Elasticsearch will be forced when the buffer reaches the given number of documents. Warning It is not recommended to change this option but rather use elasticsearch-max-bytes instead since the document count is not a good gauge of when to flush. The default value of -1 means to not use the number of docs as a flush indicator.","title":"elasticsearch-max-docs"},{"location":"config/#elasticsearch-max-bytes","text":"int (default 8MB as bytes) When elasticsearch-max-bytes is given a bulk index request to Elasticsearch will be forced when a connection buffer reaches the given number of bytes. This setting greatly impacts performance. A high value for this setting will cause high memory monstache memory usage as the documents are buffered in memory. Each connection in elasticsearch-max-conns will flush when its queue gets filled to this size.","title":"elasticsearch-max-bytes"},{"location":"config/#elasticsearch-max-seconds","text":"int (default 1 ) When elasticsearch-max-seconds is given a bulk index request to Elasticsearch will be forced when a request has not been made in the given number of seconds. The default value is automatically increased to 5 when direct read namespaces are detected. This is to ensure that flushes do not happen too often in this case which would cut performance.","title":"elasticsearch-max-seconds"},{"location":"config/#elasticsearch-pem-file","text":"string (default \"\" ) (env var name MONSTACHE_ES_PEM ) When elasticsearch-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to Elasticsearch. This should only be used when Elasticsearch is configured with SSL enabled.","title":"elasticsearch-pem-file"},{"location":"config/#elasticsearch-pki-auth","text":"TOML table (default nil ) Used to configure client to use PKI user auth for Elasticsearch","title":"elasticsearch-pki-auth"},{"location":"config/#cert-file","text":"string (default \"\") (env var name MONSTACHE_ES_PKI_CERT ) Path to the cert file e.g. the --cert argument to curl","title":"cert-file"},{"location":"config/#key-file","text":"string (default \"\") (env var name MONSTACHE_ES_PKI_KEY ) Path to the key file e.g. the --key argument to curl","title":"key-file"},{"location":"config/#elasticsearch-validate-pem-file","text":"boolean (default true ) (env var name MONSTACHE_ES_VALIDATE_PEM ) When elasticsearch-validate-pem-file is false TLS will be configured to skip verification","title":"elasticsearch-validate-pem-file"},{"location":"config/#enable-easy-json","text":"boolean (default false ) When enable-easy-json is true monstache will the easy-json library to serialize requests to Elasticsearch","title":"enable-easy-json"},{"location":"config/#enable-http-server","text":"boolean (default false ) Add this flag to enable an embedded HTTP server at localhost:8080","title":"enable-http-server"},{"location":"config/#enable-oplog","text":"boolean (default false ) This option only applies to monstache v5 and v6. Enabling it turns on change event emulation feature that tails the MongoDB oplog directly. It should only be turned on when pairing monstache v5 or v6 with a MongoDB server at a server compatibility version less than 3.6.","title":"enable-oplog"},{"location":"config/#enable-patches","text":"boolean (default false ) Set to true to enable storing rfc7396 patches in your Elasticsearch documents","title":"enable-patches"},{"location":"config/#env-delimiter","text":"string (default , ) This option is only supported on the command line. The value for this delimiter will be used to split environment variable values when the environment variable is used in conjunction with an option of array type. E.g. with export MONSTACHE_DIRECT_READ_NS=test.test,foo.bar .","title":"env-delimiter"},{"location":"config/#exit-after-direct-reads","text":"boolean (default false ) The direct-read-namespaces option gives you a way to do a full sync on multiple collections. At times you may want to perform a full sync via the direct-read-namespaces option and then quit monstache. Set this option to true and monstache will exit after syncing the direct read collections instead of continuing to tail the oplog. This is useful if you would like to run monstache to run a full sync on a set of collections via a cron job.","title":"exit-after-direct-reads"},{"location":"config/#fail-fast","text":"boolean (default false ) When fail-fast is true, if monstache receives a failed bulk indexing response from Elasticsearch, monstache will log the request that produced the response as an ERROR and then exit immediately with an error status. Normally, monstache just logs the error and continues processing events. If monstache has been configured with elasticsearch-retry true, a failed request will be retried before being considered a failure.","title":"fail-fast"},{"location":"config/#file-downloaders","text":"int (default 10 ) Number of go routines concurrently processing GridFS files when file index-files is turned on.","title":"file-downloaders"},{"location":"config/#file-highlighting","text":"boolean (default false ) When file-highlighting is true monstache will enable the ability to return highlighted keywords in the extracted text of files for queries on files which were indexed in Elasticsearch from gridfs.","title":"file-highlighting"},{"location":"config/#file-namespaces","text":"[]string (default nil ) (env var name MONSTACHE_FILE_NS ) The file-namespaces config must be set when index-files is enabled. file-namespaces must be set to an array of MongoDB namespace strings. Files uploaded through gridfs to any of the namespaces in file-namespaces will be retrieved and their raw content indexed into Elasticsearch via either the mapper-attachments or ingest-attachment plugin. Note This option may be passed on the command line as ./monstache --file-namespace test.foo --file-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_FILE_NS=test.foo,test.bar","title":"file-namespaces"},{"location":"config/#filter","text":"[] array of TOML table (default nil ) When filter is given monstache will pass the MongoDB document from an insert or update operation into the filter function immediately after it is read from the oplog. Return true from the function to continue processing the document or false to completely ignore the document. See the section Middleware for more information.","title":"filter"},{"location":"config/#namespace","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit namespace the filter function will be applied to all documents.","title":"namespace"},{"location":"config/#script","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return true/false to include or filter the document.","title":"script"},{"location":"config/#path","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#graylog-addr","text":"string (default \"\") (env var name MONSTACHE_GRAYLOG_ADDR ) The address of a graylog server to redirect logs to in GELF","title":"graylog-addr"},{"location":"config/#gtm-settings","text":"TOML table (default nil ) The following gtm configuration properties are available. See gtm for details","title":"gtm-settings"},{"location":"config/#channel-size","text":"int (default 512) Controls the size of the go channels created for processing events. When many events are processed at once a larger channel size may prevent blocking in gtm.","title":"channel-size"},{"location":"config/#buffer-size","text":"int (default 32) Determines how many documents are buffered by a gtm worker go routine before they are batch fetched from MongoDB. When many documents are inserted or updated at once it is better to fetch them together.","title":"buffer-size"},{"location":"config/#buffer-duration","text":"string (default 75ms) A string representation of a golang duration. Determines the maximum time a buffer is held before it is fetched in batch from MongoDB and flushed for indexing.","title":"buffer-duration"},{"location":"config/#max-await-time","text":"string (default \"\") A string represetation of a golang duration, e.g. \"10s\". If set, will be converted and passed at the maxAwaitTimeMS option for change streams. This determines the maximum amount of time in milliseconds the server waits for new data changes to report to the change stream cursor before returning an empty batch.","title":"max-await-time"},{"location":"config/#gzip","text":"boolean (default false ) When gzip is true, monstache will compress requests to Elasticsearch. If you enable gzip in monstache and are using Elasticsearch prior to version 5 you will also need to update the Elasticsearch config file to set http.compression: true. In Elasticsearch version 5 and above http.compression is enabled by default. Enabling gzip compression is recommended if you enable the index-files setting.","title":"gzip"},{"location":"config/#http-server-addr","text":"string (default :8080 ) (env var name MONSTACHE_HTTP_ADDR ) The address to bind the embedded HTTP server on if enabled","title":"http-server-addr"},{"location":"config/#index-as-update","text":"boolean (default false ) When index-as-update is set to true monstache will sync create and update operations in MongoDB as updates to Elasticsearch. This does not change the fact that Monstache always sends an entire copy of the data in MongoDB. It just means that any existing non-overlapping fields in Elasticsearch will be maintained. By default, monstache will overwrite the entire document in Elasticsearch. This setting may be useful if you make updates to Elasticsearch to the documents monstache has previously synced out of band and would like to retain these updates when the document changes in MongoDB. You will only be able to retain fields in Elasticsearch that do not overlap with fields in MongoDB. When this setting is turned on some guarantees about the order of operations applied in Elasticsearch are lost. The reason for this is that the version field cannot be set with this enabled. The version field by default is set to the timestamp of the event in MongoDB. Elasticsearch will only apply changes if the version number is greater or equal to the last value indexed maintaining serialization. If you enable this setting and do not see serialized updates in MongoDB being indexed correctly then you can mitigate this problem with the following settings: elasticsearch-max-conns = 1 [gtm-settings] buffer-size = 2048 buffer-duration = 4s","title":"index-as-update"},{"location":"config/#index-files","text":"boolean (default false ) When index-files is true monstache will index the raw content of files stored in GridFS into Elasticsearch as an attachment type. By default index-files is false meaning that monstache will only index metadata associated with files stored in GridFS. In order for index-files to index the raw content of files stored in GridFS you must install a plugin for Elasticsearch. For versions of Elasticsearch prior to version 5, you should install the mapper-attachments plugin. In version 5 or greater of Elasticsearch the mapper-attachment plugin is deprecated and you should install the ingest-attachment plugin instead. For further information on how to configure monstache to index content from GridFS, see the section GridFS support .","title":"index-files"},{"location":"config/#index-oplog-time","text":"boolean (default false ) If this option is set to true monstache will include 2 automatic fields in the source document indexed into Elasticsearch. The first is oplog_ts which is the timestamp for the event copied directly from the MongoDB oplog. The second is oplog_date which is an Elasticsearch date field corresponding to the time of the same event. This information is generally useful in Elasticsearch giving the notion of last updated. However, it's also valuable information to have for failed indexing requests since it gives one the information to replay from a failure point. See the option resume-from-timestamp for information on how to replay oplog events since a given event occurred. For data read via the direct read feature the oplog time will only be available if the id of the MongoDB document is an ObjectID. If the id of the MongoDB document is not an ObjectID and the document source is a direct read query then the oplog time will not be available.","title":"index-oplog-time"},{"location":"config/#index-stats","text":"boolean (default false ) When both stats and index-stats are true monstache will write statistics about its indexing progress in Elasticsearch instead of standard out. The indexes used to store the statistics are time stamped by day and prefixed monstache.stats. . E.g. monstache.stats.2017-07-01 and so on. As these indexes will accrue over time your can use a tool like curator to prune them with a Delete Indices action and an age filter.","title":"index-stats"},{"location":"config/#logs","text":"TOML table (default nil ) (env var name MONSTACHE_LOG_DIR ) Allows writing logs to a file using a rolling appender instead of stdout. Supply a file path for each type of log you would like to send to a file. When the MONSTACHE_LOG_DIR environment variable is used then a log file for each log level will be generated in the given directory.","title":"logs"},{"location":"config/#info","text":"string (default \"\") The file path to write info level logs to","title":"info"},{"location":"config/#warn","text":"string (default \"\") The file path to write warning level logs to","title":"warn"},{"location":"config/#error","text":"string (default \"\") The file path to write error level logs to","title":"error"},{"location":"config/#trace","text":"string (default \"\") The file path to write trace level logs to. Trace logs are enabled via the verbose option.","title":"trace"},{"location":"config/#stats","text":"string (default \"\") The file path to write indexing statistics to. Stats logs are enabled via the stats option.","title":"stats"},{"location":"config/#log-rotate","text":"TOML table (default nil ) Use to configure how log files are rotated/managed when logging to files. These options are passed through to the lumberjack logger.","title":"log-rotate"},{"location":"config/#max-size","text":"int (default 500) (env var name MONSTACHE_LOG_MAX_SIZE ) MaxSize is the maximum size in megabytes of the log file before it gets rotated.","title":"max-size"},{"location":"config/#max-age","text":"int (default 28) (env var name MONSTACHE_LOG_MAX_AGE ) MaxAge is the maximum number of days to retain old log files based on the timestamp encoded in their filename. Note that a day is defined as 24 hours and may not exactly correspond to calendar days due to daylight savings, leap seconds, etc. Use a value of zero to ignore the age of files.","title":"max-age"},{"location":"config/#max-backups","text":"int (default 5) (env var name MONSTACHE_LOG_MAX_BACKUPS ) MaxBackups is the maximum number of old log files to retain. Use a value of zero to retain all old log files (though MaxAge may still cause them to get deleted.)","title":"max-backups"},{"location":"config/#localtime","text":"boolean (default false) LocalTime determines if the time used for formatting the timestamps in backup files is the computer's local time. The default is to use UTC time.","title":"localtime"},{"location":"config/#compress","text":"boolean (default false) Compress determines if the rotated log files should be compressed using gzip. The default is not to perform compression.","title":"compress"},{"location":"config/#mapper-plugin-path","text":"string (default \"\" ) The path to an .so file golang plugin.","title":"mapper-plugin-path"},{"location":"config/#mapping","text":"[] array of TOML table (default nil ) When mapping is given monstache will be directed to override the default index and type assigned to documents in Elasticsearch. See the section Index Mapping for more information.","title":"mapping"},{"location":"config/#namespace_1","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the mapping to.","title":"namespace"},{"location":"config/#index","text":"string (default \"same as namespace including the dot. e.g. test.test\") Allows you to override the default index that monstache will send documents to. By default, the index is the same as the MongoDB namespace.","title":"index"},{"location":"config/#type","text":"string (default \"_doc for ES 6.2+ and the name of the MongoDB collection otherwise\") Allows you to override the default type that monstache will index documents with. Overriding the type is not recommended for Elasticsearch version 6.2+.","title":"type"},{"location":"config/#pipeline","text":"string (default \"\") The name of an existing Elasticsearch pipeline to index the data with. A pipeline is a series of Elasticsearch processors to be executed. An Elasticsearch pipeline is one way to transform MongoDB data before indexing.","title":"pipeline"},{"location":"config/#max-file-size","text":"int (default 0 ) When max-file-size is greater than 0 monstache will not index the content of GridFS files that exceed this limit in bytes.","title":"max-file-size"},{"location":"config/#merge-patch-attribute","text":"string (default json-merge-patches ) Customize the name of the property under which merge patches are stored","title":"merge-patch-attribute"},{"location":"config/#mongo-url","text":"string (default localhost ) (env var name MONSTACHE_MONGO_URL ) The URL to connect to MongoDB which must follow the Standard Connection String Format For sharded clusters this URL should point to the mongos router server and the mongo-config-url option must be set to point to the config server.","title":"mongo-url"},{"location":"config/#mongo-config-url","text":"string (default \"\" ) (env var name MONSTACHE_MONGO_CONFIG_URL ) This config must only be set for sharded MongoDB clusters. Has the same syntax as mongo-url. This URL must point to the MongoDB config server. Monstache will read the list of shards using this connection and then setup a listener to react to new shards being added to the cluster at a later time. It will then setup a new direct connection to each shard to listen for events. Setting the mongo-config-url is not necessary if you are using change-stream-namespaces .","title":"mongo-config-url"},{"location":"config/#mongo-pem-file","text":"string (default \"\" ) (env var name MONSTACHE_MONGO_PEM ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. When mongo-pem-file is given monstache will use the given file path to add a local certificate to x509 cert pool when connecting to MongoDB. This should only be used when MongoDB is configured with SSL enabled.","title":"mongo-pem-file"},{"location":"config/#mongo-validate-pem-file","text":"boolean (default true ) (env var name MONSTACHE_MONGO_VALIDATE_PEM ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. When mongo-validate-pem-file is false TLS will be configured to skip verification","title":"mongo-validate-pem-file"},{"location":"config/#mongo-oplog-database-name","text":"string (default local ) (env var name MONSTACHE_MONGO_OPLOG_DB ) When mongo-oplog-database-name is given monstache will look for the MongoDB oplog in the supplied database","title":"mongo-oplog-database-name"},{"location":"config/#mongo-oplog-collection-name","text":"string (default oplog.rs ) (env var name MONSTACHE_MONGO_OPLOG_COL ) When mongo-oplog-collection-name is given monstache will look for the MongoDB oplog in the supplied collection. The collection defaults to oplog.rs which is what will be produced when replica sets are enabled. If you are using an old version of MongoDB with master based replication instead of replica sets, then you will need to configure this setting to oplog.$main . Warning If this setting was not supplied monstache would previously search for the first collection prefixed oplog in the local database. However, starting in monstache v4.13.1 and v3.20.1 this behavior has changed. Now, monstache will not do dynamic resolution. Since master based replication in MongoDB is no longer supported, monstache now defaults to oplog.rs and will only use another collection (e.g. oplog.$main ) if you explicitly config it to do so.","title":"mongo-oplog-collection-name"},{"location":"config/#mongo-dial-settings","text":"TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. The following MongoDB dial properties are available. Timeout values of 0 disable the timeout.","title":"mongo-dial-settings"},{"location":"config/#ssl","text":"","title":"ssl"},{"location":"config/#bool-default-false","text":"Set to true to establish a connection using TLS.","title":"bool (default false)"},{"location":"config/#timeout","text":"","title":"timeout"},{"location":"config/#int-default-15","text":"Seconds to wait when establishing an initial connection to MongoDB before giving up","title":"int (default 15)"},{"location":"config/#read-timeout","text":"","title":"read-timeout"},{"location":"config/#int-default-30","text":"Seconds to wait when reading data from MongoDB before giving up. Must be greater than 0. This should be greater than 10 because Monstache waits 10s for new change events by retrying the query.","title":"int (default 30)"},{"location":"config/#write-timeout","text":"","title":"write-timeout"},{"location":"config/#int-default-30_1","text":"Seconds to wait when writing data to MongoDB before giving up. Must be greated than 0. This should be greater than 10 because Monstache waits 10s for new change events by retrying the query.","title":"int (default 30)"},{"location":"config/#mongo-session-settings","text":"TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. The following MongoDB session properties are available. Timeout values of 0 disable the timeout.","title":"mongo-session-settings"},{"location":"config/#socket-timeout","text":"int (default 0) Seconds to wait for a non-responding socket before it is forcefully closed","title":"socket-timeout"},{"location":"config/#sync-timeout","text":"int (default 30) Amount of time in seconds an operation will wait before returning an error in case a connection to a usable server can't be established. Must be greater than 0.","title":"sync-timeout"},{"location":"config/#mongo-x509-settings","text":"TOML table (default nil ) This setting only applies to the mgo driver in monstache versions 3 and 4. The driver in monstache 5 and 6 uses the connection string for all settings. Allows one to configure x509 authentication with MongoDB. For more information see x509 auth . Note You must configure your mongo-url with the request parameter authMechanism=MONGODB-X509 . You must also supply both of the following file paths:","title":"mongo-x509-settings"},{"location":"config/#client-cert-pem-file","text":"string (default \"\") The path to a PEM encoded file containing the client cert","title":"client-cert-pem-file"},{"location":"config/#client-key-pem-file","text":"string (default \"\") The path to a PEM encoded file containing the client key","title":"client-key-pem-file"},{"location":"config/#namespace-drop-exclude-regex","text":"regex (default \"\" ) (env var name MONSTACHE_NS_DROP_EXCLUDE_REGEX ) When namespace-drop-exclude-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex does not match the namespace then the operation will by synced.","title":"namespace-drop-exclude-regex"},{"location":"config/#namespace-drop-regex","text":"regexp (default \"\" ) (env var name MONSTACHE_NS_DROP_REGEX ) When namespace-drop-regex is given this regex is tested against the namespace, database.collection, of drops in MongoDB. For database drops the namespace will be database-name.$cmd. For collections drops the namespace will be database-name.collection-name. If the regex matches the namespace then the operation will by synced.","title":"namespace-drop-regex"},{"location":"config/#namespace-exclude-regex","text":"regex (default \"\" ) (env var name MONSTACHE_NS_EXCLUDE_REGEX ) When namespace-exclude-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache ignores the event, otherwise it continues processing event filters. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces .","title":"namespace-exclude-regex"},{"location":"config/#namespace-regex","text":"regexp (default \"\" ) (env var name MONSTACHE_NS_REGEX ) When namespace-regex is given this regex is tested against the namespace, database.collection, of any insert, update, delete in MongoDB. If the regex matches monstache continues processing event filters, otherwise it drops the event. By default monstache processes events in all databases and all collections with the exception of the reserved database monstache, any collections suffixed with .chunks, and the system collections. For more information see the section Namespaces .","title":"namespace-regex"},{"location":"config/#oplog-date-field-format","text":"string (default 2006/01/02 15:04:05 ) Use this option to override the layout for formatting the oplog_date field. Refer to the Format function for the reference time values to use in the layout.","title":"oplog-date-field-format"},{"location":"config/#oplog-date-field-name","text":"string (default oplog_date ) Use this option to override the name of the field used to store the oplog date string","title":"oplog-date-field-name"},{"location":"config/#oplog-ts-field-name","text":"string (default oplog_ts ) Use this option to override the name of the field used to store the oplog timestamp","title":"oplog-ts-field-name"},{"location":"config/#patch-namespaces","text":"[]string (default nil ) (env var name MONSTACHE_PATCH_NS ) An array of MongoDB namespaces that you would like to enable rfc7396 patches on Note This option may be passed on the command line as ./monstache --patch-namespace test.foo --patch-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_PATCH_NS=test.foo,test.bar","title":"patch-namespaces"},{"location":"config/#pipeline_1","text":"[] array of TOML table (default nil ) When pipeline is given monstache will call the function specified to determine an array of aggregation pipeline stages to run. See the section Middleware for more information.","title":"pipeline"},{"location":"config/#namespace_2","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the pipeline function with be applied to all namespaces.","title":"namespace"},{"location":"config/#script_1","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a namespace and a boolean indicating whether or not the data is a change stream. The function should return an array of aggregation pipeline stages. Note, for change streams the root of the pipeline will be the change event with a field fullDocument representing the changed doc. You should alter your pipeline stages according to this boolean. Monstache needs the change event data so do not replace the root of the document in your pipeline for change streams.","title":"script"},{"location":"config/#path_1","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#pipe-allow-disk","text":"boolean (default false ) Add this flag to allow MongoDB to use the disk as a temporary store for data during aggregation pipelines","title":"pipe-allow-disk"},{"location":"config/#post-processors","text":"int (default 10 ) Number of go routines concurrently calling the Process method in any golang middleware plugins installed via mapper-plugin-path .","title":"post-processors"},{"location":"config/#pprof","text":"boolean (default false ) When pprof is true and the http server is enabled, monstache will make profiling information available. See Profiling for Go for more information.","title":"pprof"},{"location":"config/#print-config","text":"boolean (default false ) When print-config is true monstache will print its configuration and then exit","title":"print-config"},{"location":"config/#prune-invalid-json","text":"boolean (default false ) If your MongoDB data contains values like +Infinity, -Infinity, NaN, or invalid dates you will want to set this option to true. The Golang json serializer is not able to handle these values and the indexer will get stuck in an infinite loop. When prune-invalid-json is set to true Monstache will drop those fields so that indexing errors do not occur.","title":"prune-invalid-json"},{"location":"config/#relate","text":"[] array of TOML table (default nil ) Allows one to relate 2 namespaces together such that a change to one causes a sync of the associated namespace","title":"relate"},{"location":"config/#namespace_3","text":"string (default \"\") The namespace of the collection that, when modified, triggers a sync of the with-namespace","title":"namespace"},{"location":"config/#with-namespace","text":"string (default \"\") The namespace of the collection or view that will be synced when namespace changes","title":"with-namespace"},{"location":"config/#src-field","text":"string (default \"_id\") The name of the field in namespace that will be extracted from the change doc and used as the value side of the query into with-namespace","title":"src-field"},{"location":"config/#match-field","text":"string (default \"_id\") The name of the field in with-namespace that will be used as the field name to match side of the query into with-namespace","title":"match-field"},{"location":"config/#match-field-type","text":"string (default \"\") Valid values for this property are objectId , string , int , long and decimal . If the property is given the value of the src-field will be converted into the type given (if possible) before being used to query against match-field .","title":"match-field-type"},{"location":"config/#keep-src","text":"bool (default \"false\") Whether or not to sync the original change event in addition to the one looked up in with-namespace. By default the original change is ignored and only the document from with-namespace is synced.","title":"keep-src"},{"location":"config/#max-depth","text":"int (default 0) If max-depth is greater than 0 then the relationship will only fire if the number of relationships between this relate and the originating event is less than or equal to the value given. By default monstache will continue following relationships until none are left.","title":"max-depth"},{"location":"config/#dot-notation","text":"bool (default \"false\") If match-field is a nested field like foo.bar then setting dot-notation to true produces the query { \"foo.bar\": 1 } to MongoDB. If dot-notation is not enabled then the query will be an exact match query sent as { foo { bar : 1 } } .","title":"dot-notation"},{"location":"config/#relate-buffer","text":"int (default 1000 ) Number of relate events allowed to queue up before skipping the event and reporting an error. This setting was introduced to prevent scenarios where relate queries get queued up and stall the pipeline. You can increase this value if you are hitting this limit, but monstache will take more memory to hold the events and you may find that MongoDB experiences high CPU due to Monstache performing many queries concurrently to try to clear the buffer. This limit is usually hit when you have a relate config and then do a mass insert or update against the relate namespace.","title":"relate-buffer"},{"location":"config/#relate-threads","text":"int (default 10 ) Number of go routines concurrently processing relationships when relate is enabled. This dictates the concurrency of queries trying to unload the relate queue.","title":"relate-threads"},{"location":"config/#replay","text":"boolean (default false ) Warning Replay is currently deprecated in favor of direct-read-namespaces . Replay may be removed in a future release. When replay is true, monstache replays all events from the beginning of the MongoDB oplog and syncs them to Elasticsearch. If you've previously synced Monstache to Elasticsearch you may see many WARN statments in the log indicating that there was a version conflict. This is normal during a replay and it just means that you already have data in Elasticsearch that is newer than the point in time data from the oplog. When resume and replay are both true, monstache replays all events from the beginning of the MongoDB oplog, syncs them to Elasticsearch and also writes the timestamps of processed events to monstache.monstache. When neither resume nor replay are true, monstache reads the last timestamp in the oplog and starts listening for events occurring after this timestamp (tails starting at the end). Timestamps are not written to monstache.monstache. This is the default behavior.","title":"replay"},{"location":"config/#resume","text":"boolean (default false ) When resume is true, monstache writes the timestamp of MongoDB operations it has successfully synced to Elasticsearch to the collection monstache.monstache. It also reads that timestamp from that collection when it starts in order to replay events which it might have missed because monstache was stopped. If monstache is started with the cluster-name option set then resume is automatically turned on.","title":"resume"},{"location":"config/#resume-from-timestamp","text":"int64 (default 0 ) This option only applies when the resume-strategy is 0 for timestamp based resume. When resume-from-timestamp (a 64 bit timestamp where the high 32 bytes represent the number of seconds since epoch and the low 32 bits represent an offset within a second) is given, monstache will sync events starting immediately after the timestamp. This is useful if you have a specific timestamp from the oplog and would like to start syncing from after this event. If you supply an integer that is greater than 0 but less than or equal to the max value of a 32-bit integer then monstache will interpret the value as seconds since the epoch and automatically shift the value 32 bits left.","title":"resume-from-timestamp"},{"location":"config/#resume-name","text":"string (default default ) monstache uses the value of resume-name as an id when storing and retrieving timestamps to and from the MongoDB collection monstache.monstache . The default value for this option is the string default . However, there are some exceptions. If monstache is started with the cluster-name option set then the name of the cluster becomes the resume-name. This is to ensure that any process in the cluster is able to resume from the last timestamp successfully processed. Another exception occurs when worker is enabled. In that case the worker name becomes the resume-name.","title":"resume-name"},{"location":"config/#resume-strategy","text":"int (default 0 ) The strategy to use for resuming streams from previous runs of monstache. Only applies when resume is enabled. This strategy is also used when cluster-name is set to ensure streams are resumed when the active process in the cluster switches. Strategy 0 -default- Timestamp based resume of change streams. Compatible with MongoDB API 4.0+. Stategy 1 Token based resume of change streams. Compatible with MongoDB API 3.6+. Timestamps and tokens are written periodically to the database configured in config-database-name . Timestamps are written to the collection named monstache . Tokens are written to the collection named tokens .","title":"resume-strategy"},{"location":"config/#resume-write-unsafe","text":"boolean (default false ) When resume-write-unsafe is true monstache sets the safety mode of the MongoDB session such that writes are fire and forget. This speeds up writing of timestamps used to resume synching in a subsequent run of monstache. This speed up comes at the cost of no error checking on the write of the timestamp. Since errors writing the last synched timestamp are only logged by monstache and do not stop execution it's not unreasonable to set this to true to get a speedup.","title":"resume-write-unsafe"},{"location":"config/#routing-namespaces","text":"[]string (default nil ) You only need to set this configuration option if you use golang and javascript plugins are do custom routing: override parent or routing attributes. This array should be set to a list of all the namespaces that custom routing is done on. This ensures that deletes in MongoDB are routed correctly to Elasticsearch.","title":"routing-namespaces"},{"location":"config/#script_2","text":"[] array of TOML table (default nil ) When script is given monstache will pass the MongoDB document into the script before indexing into Elasticsearch. See the section Middleware for more information.","title":"script"},{"location":"config/#namespace_4","text":"string (default \"\") The MongoDB namespace, db.collection, to apply the script to. If you omit the namespace the mapping function with be applied to all documents.","title":"namespace"},{"location":"config/#routing","text":"boolean (default false) Set routing to true if you override the index, routing or parent metadata via _meta_monstache","title":"routing"},{"location":"config/#script_3","text":"string (default \"\") An inline script. You can use TOML multiline syntax here. The function should take 2 arguments, a doc and a namespace, and return a modified doc. You can also return true to index the original document or false to ignore the document and schedule any previous documents with the same id for deletion.","title":"script"},{"location":"config/#path_2","text":"string (default \"\") The file path to load a script from. Use this or an inline script but not both. Can be a path relative to the directory monstache is executed from or an absolute path.","title":"path"},{"location":"config/#stats_1","text":"boolean (default false ) When stats is true monstache will periodically print statistics accumulated by the indexer","title":"stats"},{"location":"config/#stats-duration","text":"string (default 30s ) Sets the duration after which statistics are printed if stats is enabled","title":"stats-duration"},{"location":"config/#stats-index-format","text":"string (default monstache.stats.2006-01-02 ) The time.Time supported index name format for stats indices. By default, stats indexes are partitioned by day. To use less indices for stats you can shorten this format string (e.g monstache.stats.2006-01) or remove the time component completely to use a single index.","title":"stats-index-format"},{"location":"config/#time-machine-namespaces","text":"[]string (default nil ) (env var name MONSTACHE_TIME_MACHINE_NS ) Monstache is good at keeping your MongoDB collections and Elasticsearch indexes in sync. When a document is updated in MongoDB the corresponding document in Elasticsearch is updated too. Same goes for deleting documents in MongoDB. But what if you also wanted to keep a log of all the changes to a MongoDB document over its lifespan. That's what time-machine-namespaces are for. When you configure a list of namespaces in MongoDB to add to the time machine, in addition to keeping documents in sync, Monstache will index of copy of your MongoDB document at the time it changes in a separate timestamped index. Say for example, you insert a document into the test.test collection in MongoDB. Monstache will index by default into the test.test index in Elasticsearch, but with time machines it will also index it into log.test.test.2018-02-19 . When it indexes it into the time machine index it does so without the id from MongoDB and lets Elasticsearch generate a unique id. But, it stores the id from MongoDB in the source field _source_id . Also, it adds _oplog_ts and _oplog_date fields on the source document. These correspond to the timestamp from the oplog when the data changed in MongoDB. Finally, it routes the document by the MongoDB id so that you can speed up queries later to find changes to a doc. This lets you do some cool things but mostly you'll want to sort by _oplog_date and filter by _source_id to see how documents have changed over time. Because the indexes are timestamped you can drop then after a period of time so they don't take up space. If you just want the last couple of days of changes, delete the indexes with the old timestamps. Elastic curator is your friend here. Note This option may be passed on the command line as ./monstache --time-machine-namespace test.foo --time-machine-namespace test.bar If specified as an environment variable the value should be namespaces separated only by the env-delimiter which defaults to a comma. E.g. MONSTACHE_TIME_MACHINE_NS=test.foo,test.bar","title":"time-machine-namespaces"},{"location":"config/#time-machine-index-prefix","text":"string (default log ) If you have enabled time machine namespaces and want to change the prefix assigned to the index names use this setting.","title":"time-machine-index-prefix"},{"location":"config/#time-machine-index-suffix","text":"string (default 2006-01-02 ) If you have enabled time machine namespaces and want to suffix the index names using a different date format use this setting. Consult the golang docs for how date formats work. By default this suffixes the index name with the year, month, and day.","title":"time-machine-index-suffix"},{"location":"config/#time-machine-direct-reads","text":"boolean (default false ) This setting controls whether or not direct reads are added to the time machine log index. This is false by default so only changes read from the oplog are added.","title":"time-machine-direct-reads"},{"location":"config/#tpl","text":"boolean (default false ) This option is only supported on the command line. When the tpl mode is turned on then any config file passed via -f will be interpreted and executed as a golang template before being loaded. The template will have access to all environment variables. The environment variables will be passed as a map to the template. The env map can be accessed as the dot . symbol in the golang template and values from the map obtained using the index function. For example, the given an environment variable THRESHOLD, then with -tpl -f config.toml the config.toml might contain... [[script]] namespace = \"mydb.mycollection\" script = \"\"\" module.exports = function(doc) { if ( doc.score > {{index . \"THRESHOLD\"}} ) { doc.important = true; } return doc; } \"\"\"","title":"tpl"},{"location":"config/#verbose","text":"boolean (default false ) When verbose is true monstache with enable debug logging including a trace of requests to Elasticsearch","title":"verbose"},{"location":"config/#worker","text":"string (default \"\" ) (env var name MONSTACHE_WORKER ) When worker is given monstache will enter multi-worker mode and will require you to also provide the config option workers. Use this mode to run multiple monstache processes and distribute the work between them. In this mode monstache will ensure that each MongoDB document id always goes to the same worker and none of the other workers. See the Workers section for more information.","title":"worker"},{"location":"config/#workers","text":"[]string (default nil ) An array of worker names to be used in conjunction with the worker option. Note This option may be passed on the command line as ./monstache --workers w1 --workers w2","title":"workers"},{"location":"start/","text":"Getting Started \u00b6 Installation \u00b6 Monstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP. Monstache is written in Go but you don't need to install the Go language unless you decide to write your own Go plugins. If you simply want to run Monstache you just need to download the latest version . Which version should I use? \u00b6 Monstache version Git branch (used to build plugin) Docker tag Description Elasticsearch MongoDB Status 6 rel6 rel6, latest MongoDB, Inc. go driver Version 7+ Version 2.6+ Supported 5 rel5 rel5 MongoDB, Inc. go driver Version 6 Version 2.6+ Supported 4 master rel4 mgo community go driver Version 6 Version 3 Deprecated 3 rel3 rel3 mgo community go driver Versions 2 and 5 Version 3 Deprecated Unzip the download and adjust your PATH variable to include the path to the folder for your platform. Let's make sure Monstache is set up as expected. You should see a similar version number in your terminal: monstache -v # 6.7.4 You can also build monstache from source. Monstache uses vgo . cd ~/build # somewhere outside your $GOPATH git clone https://github.com/rwynn/monstache.git cd monstache git checkout <branch-or-tag-to-build> go install # monstache binary should now be in $GOPATH/bin Usage \u00b6 Monstache uses the MongoDB oplog as an event source. You will need to ensure that MongoDB is configured to produce an oplog by deploying a replica set . If you haven't already done so, follow the 5 step procedure to initiate and validate your replica set. For local testing your replica set may contain a single member . Note If you have enabled security in MongoDB you will need to give the user in your connection string certain privileges: For MongoDB versions prior to 3.6 the user in the connection string will need to be able read the local database (to read from the oplog) and any user databases that you wish to synch data from. When using the resume or clustering features the user will need to be able to write to and create indexes for the monstache database, or more generally, whatever you configure the option config-database-name to be. When using change streams you will need to implement the changes in the documentation for access control . Monstache defaults to opening the change stream against the entire deployment. Without any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost on the default ports and begin tailing the MongoDB oplog. Any changes to MongoDB while Monstache is running will be reflected in Elasticsearch. To see the indexes created by Monstache you may want to issue the following command which will show the indices in Elasticsearch. By default, the index names will match the db.collection name in MongoDB. curl localhost:9200/_cat/indices?v Monstache uses the TOML format for its configuration. You can run monstache with an explicit configuration by passing the -f flag. monstache -f /path/to/config.toml The following shows how to specify options in a TOML config file. Note It is highly recommended that you start with only your MongoDB and Elasticsearch connection settings and only specify additional options as needed. # connection settings # connect to MongoDB using the following URL mongo-url = \"mongodb://someuser:password@localhost:40001\" # connect to the Elasticsearch REST API at the following node URLs elasticsearch-urls = [\"https://es1:9200\", \"https://es2:9200\"] # frequently required settings # if you need to seed an index from a collection and not just listen and sync changes events # you can copy entire collections or views from MongoDB to Elasticsearch direct-read-namespaces = [\"mydb.mycollection\", \"db.collection\", \"test.test\", \"db2.myview\"] # if you want to use MongoDB change streams instead of legacy oplog tailing use change-stream-namespaces # change streams require at least MongoDB API 3.6+ # if you have MongoDB 4+ you can listen for changes to an entire database or entire deployment # in this case you usually don't need regexes in your config to filter collections unless you target the deployment. # to listen to an entire db use only the database name. For a deployment use an empty string. change-stream-namespaces = [\"mydb.mycollection\", \"db.collection\", \"test.test\"] # additional settings # if you don't want to listen for changes to all collections in MongoDB but only a few # e.g. only listen for inserts, updates, deletes, and drops from mydb.mycollection # this setting does not initiate a copy, it is only a filter on the change event listener namespace-regex = '^mydb\\.mycollection$' # compress requests to Elasticsearch gzip = true # generate indexing statistics stats = true # index statistics into Elasticsearch index-stats = true # use the following user name for Elasticsearch basic auth elasticsearch-user = \"someuser\" # use the following password for Elasticsearch basic auth elasticsearch-password = \"somepassword\" # use 4 go routines concurrently pushing documents to Elasticsearch elasticsearch-max-conns = 4 # use the following PEM file to connections to Elasticsearch elasticsearch-pem-file = \"/path/to/elasticCert.pem\" # validate connections to Elasticsearch elastic-validate-pem-file = true # propogate dropped collections in MongoDB as index deletes in Elasticsearch dropped-collections = true # propogate dropped databases in MongoDB as index deletes in Elasticsearch dropped-databases = true # do not start processing at the beginning of the MongoDB oplog # if you set the replay to true you may see version conflict messages # in the log if you had synced previously. This just means that you are replaying old docs which are already # in Elasticsearch with a newer version. Elasticsearch is preventing the old docs from overwriting new ones. replay = false # resume processing from a timestamp saved in a previous run resume = true # do not validate that progress timestamps have been saved resume-write-unsafe = false # override the name under which resume state is saved resume-name = \"default\" # use a custom resume strategy (tokens) instead of the default strategy (timestamps) # tokens work with MongoDB API 3.6+ while timestamps work only with MongoDB API 4.0+ resume-strategy = 1 # exclude documents whose namespace matches the following pattern namespace-exclude-regex = '^mydb\\.ignorecollection$' # turn on indexing of GridFS file content index-files = true # turn on search result highlighting of GridFS content file-highlighting = true # index GridFS files inserted into the following collections file-namespaces = [\"users.fs.files\"] # print detailed information including request traces verbose = true # enable clustering mode cluster-name = 'apollo' # do not exit after full-sync, rather continue tailing the oplog exit-after-direct-reads = false See Configuration for details about each configuration option.","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"Monstache is just a single binary without dependencies on runtimes like Ruby, Python or PHP. Monstache is written in Go but you don't need to install the Go language unless you decide to write your own Go plugins. If you simply want to run Monstache you just need to download the latest version .","title":"Installation"},{"location":"start/#which-version-should-i-use","text":"Monstache version Git branch (used to build plugin) Docker tag Description Elasticsearch MongoDB Status 6 rel6 rel6, latest MongoDB, Inc. go driver Version 7+ Version 2.6+ Supported 5 rel5 rel5 MongoDB, Inc. go driver Version 6 Version 2.6+ Supported 4 master rel4 mgo community go driver Version 6 Version 3 Deprecated 3 rel3 rel3 mgo community go driver Versions 2 and 5 Version 3 Deprecated Unzip the download and adjust your PATH variable to include the path to the folder for your platform. Let's make sure Monstache is set up as expected. You should see a similar version number in your terminal: monstache -v # 6.7.4 You can also build monstache from source. Monstache uses vgo . cd ~/build # somewhere outside your $GOPATH git clone https://github.com/rwynn/monstache.git cd monstache git checkout <branch-or-tag-to-build> go install # monstache binary should now be in $GOPATH/bin","title":"Which version should I use?"},{"location":"start/#usage","text":"Monstache uses the MongoDB oplog as an event source. You will need to ensure that MongoDB is configured to produce an oplog by deploying a replica set . If you haven't already done so, follow the 5 step procedure to initiate and validate your replica set. For local testing your replica set may contain a single member . Note If you have enabled security in MongoDB you will need to give the user in your connection string certain privileges: For MongoDB versions prior to 3.6 the user in the connection string will need to be able read the local database (to read from the oplog) and any user databases that you wish to synch data from. When using the resume or clustering features the user will need to be able to write to and create indexes for the monstache database, or more generally, whatever you configure the option config-database-name to be. When using change streams you will need to implement the changes in the documentation for access control . Monstache defaults to opening the change stream against the entire deployment. Without any explicit configuration monstache will connect to Elasticsearch and MongoDB on localhost on the default ports and begin tailing the MongoDB oplog. Any changes to MongoDB while Monstache is running will be reflected in Elasticsearch. To see the indexes created by Monstache you may want to issue the following command which will show the indices in Elasticsearch. By default, the index names will match the db.collection name in MongoDB. curl localhost:9200/_cat/indices?v Monstache uses the TOML format for its configuration. You can run monstache with an explicit configuration by passing the -f flag. monstache -f /path/to/config.toml The following shows how to specify options in a TOML config file. Note It is highly recommended that you start with only your MongoDB and Elasticsearch connection settings and only specify additional options as needed. # connection settings # connect to MongoDB using the following URL mongo-url = \"mongodb://someuser:password@localhost:40001\" # connect to the Elasticsearch REST API at the following node URLs elasticsearch-urls = [\"https://es1:9200\", \"https://es2:9200\"] # frequently required settings # if you need to seed an index from a collection and not just listen and sync changes events # you can copy entire collections or views from MongoDB to Elasticsearch direct-read-namespaces = [\"mydb.mycollection\", \"db.collection\", \"test.test\", \"db2.myview\"] # if you want to use MongoDB change streams instead of legacy oplog tailing use change-stream-namespaces # change streams require at least MongoDB API 3.6+ # if you have MongoDB 4+ you can listen for changes to an entire database or entire deployment # in this case you usually don't need regexes in your config to filter collections unless you target the deployment. # to listen to an entire db use only the database name. For a deployment use an empty string. change-stream-namespaces = [\"mydb.mycollection\", \"db.collection\", \"test.test\"] # additional settings # if you don't want to listen for changes to all collections in MongoDB but only a few # e.g. only listen for inserts, updates, deletes, and drops from mydb.mycollection # this setting does not initiate a copy, it is only a filter on the change event listener namespace-regex = '^mydb\\.mycollection$' # compress requests to Elasticsearch gzip = true # generate indexing statistics stats = true # index statistics into Elasticsearch index-stats = true # use the following user name for Elasticsearch basic auth elasticsearch-user = \"someuser\" # use the following password for Elasticsearch basic auth elasticsearch-password = \"somepassword\" # use 4 go routines concurrently pushing documents to Elasticsearch elasticsearch-max-conns = 4 # use the following PEM file to connections to Elasticsearch elasticsearch-pem-file = \"/path/to/elasticCert.pem\" # validate connections to Elasticsearch elastic-validate-pem-file = true # propogate dropped collections in MongoDB as index deletes in Elasticsearch dropped-collections = true # propogate dropped databases in MongoDB as index deletes in Elasticsearch dropped-databases = true # do not start processing at the beginning of the MongoDB oplog # if you set the replay to true you may see version conflict messages # in the log if you had synced previously. This just means that you are replaying old docs which are already # in Elasticsearch with a newer version. Elasticsearch is preventing the old docs from overwriting new ones. replay = false # resume processing from a timestamp saved in a previous run resume = true # do not validate that progress timestamps have been saved resume-write-unsafe = false # override the name under which resume state is saved resume-name = \"default\" # use a custom resume strategy (tokens) instead of the default strategy (timestamps) # tokens work with MongoDB API 3.6+ while timestamps work only with MongoDB API 4.0+ resume-strategy = 1 # exclude documents whose namespace matches the following pattern namespace-exclude-regex = '^mydb\\.ignorecollection$' # turn on indexing of GridFS file content index-files = true # turn on search result highlighting of GridFS content file-highlighting = true # index GridFS files inserted into the following collections file-namespaces = [\"users.fs.files\"] # print detailed information including request traces verbose = true # enable clustering mode cluster-name = 'apollo' # do not exit after full-sync, rather continue tailing the oplog exit-after-direct-reads = false See Configuration for details about each configuration option.","title":"Usage"}]}